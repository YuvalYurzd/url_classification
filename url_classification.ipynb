{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "009c0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\User\\\\Desktop\\\\url_dataset_updated.csv')\n",
    "\n",
    "# Drop all duplicates from df\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Reset the index after dropping duplicates\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "url_column_name = 'URL'  # Replace with your actual column name\n",
    "\n",
    "# Remove 'http://' and 'https://' from all URLs\n",
    "df[url_column_name] = df[url_column_name].str.replace('http://', '', regex=False)\n",
    "df[url_column_name] = df[url_column_name].str.replace('https://', '', regex=False)\n",
    "\n",
    "\n",
    "# Separate the dataset into malicious and benign\n",
    "malicious_df = df[df['Label'] == 1]\n",
    "benign_df = df[df['Label'] == 0]\n",
    "\n",
    "# Randomly sample 150,000 entries from each\n",
    "malicious_sampled_df = resample(malicious_df, n_samples=150000, random_state=42)\n",
    "benign_sampled_df = resample(benign_df, n_samples=150000, random_state=42)\n",
    "\n",
    "# Combine the sampled data\n",
    "balanced_df = pd.concat([malicious_sampled_df, benign_sampled_df])\n",
    "\n",
    "# Shuffle the combined dataset to mix malicious and benign URLs\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Now, balanced_df contains the balanced dataset ready for further processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5a02635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-whois\n",
      "  Downloading python-whois-0.8.0.tar.gz (109 kB)\n",
      "     ---------------------------------------- 0.0/109.6 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/109.6 kB ? eta -:--:--\n",
      "     ---------- -------------------------- 30.7/109.6 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------- ---------------- 61.4/109.6 kB 409.6 kB/s eta 0:00:01\n",
      "     ------------------------------------ 109.6/109.6 kB 577.9 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: future in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-whois) (0.18.3)\n",
      "Building wheels for collected packages: python-whois\n",
      "  Building wheel for python-whois (setup.py): started\n",
      "  Building wheel for python-whois (setup.py): finished with status 'done'\n",
      "  Created wheel for python-whois: filename=python_whois-0.8.0-py3-none-any.whl size=103273 sha256=3e2e949e2cb8cb4f94ea1142528916a862d06ce05f001a3e13086bf1f1a8a66a\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\8a\\d4\\1d\\bab4b44ad52eadf1b10c5c1ec7cb18a936f24b58bfb95b427e\n",
      "Successfully built python-whois\n",
      "Installing collected packages: python-whois\n",
      "Successfully installed python-whois-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-whois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "71c5b570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>etransfers.interac.ca-ssl.net/sh/2o05I9/bdesj/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>betterhealthsmoothies.com/Adobe/adobe-3D6/inde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lloydsbank.deregister-payee-secure-auth.com/Lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>archive.md</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pkg00-atx.netgate.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>infomation-fb-service.e82443.repl.co</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>img-1000736.ad-score.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>sosyalsat.com/help/home.html</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>storageapi.fleek.co/12678f8a-04f9-4b69-a70f-49...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>trepievirealestate.com/paintdesk/auth/1/inner_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      URL  Label\n",
       "0       etransfers.interac.ca-ssl.net/sh/2o05I9/bdesj/...      1\n",
       "1       betterhealthsmoothies.com/Adobe/adobe-3D6/inde...      1\n",
       "2       lloydsbank.deregister-payee-secure-auth.com/Lo...      1\n",
       "3                                              archive.md      0\n",
       "4                                   pkg00-atx.netgate.com      0\n",
       "...                                                   ...    ...\n",
       "299995               infomation-fb-service.e82443.repl.co      1\n",
       "299996                           img-1000736.ad-score.com      0\n",
       "299997                       sosyalsat.com/help/home.html      1\n",
       "299998  storageapi.fleek.co/12678f8a-04f9-4b69-a70f-49...      1\n",
       "299999  trepievirealestate.com/paintdesk/auth/1/inner_...      1\n",
       "\n",
       "[300000 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1903987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Error trying to connect to socket: closing socket - [Errno 11002] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [Errno 11002] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import tldextract \n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "import whois\n",
    "from datetime import datetime\n",
    "\n",
    "def ensure_scheme(url):\n",
    "    if not urlparse(url).scheme:\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "# Feature extraction functions\n",
    "def get_url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "def get_dot_count(url):\n",
    "    return url.count('.')\n",
    "\n",
    "def contains_security_sensitive_words(url):\n",
    "    security_sensitive_words = ['login', 'signin', 'auth', 'bank', 'update', 'account', 'verification', 'authenticate','authentication','verify','user']\n",
    "    return int(any(word in url for word in security_sensitive_words))\n",
    "\n",
    "def get_directory_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return len(path)\n",
    "\n",
    "def get_sub_directory_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return path.count('/') - 1\n",
    "\n",
    "def get_token_count_in_path(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    return len(tokens) - 1\n",
    "\n",
    "def get_largest_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    if tokens:\n",
    "        return max(len(token) for token in tokens)\n",
    "    return 0\n",
    "\n",
    "def get_average_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = [token for token in path.split('/') if token]\n",
    "    if tokens:\n",
    "        return np.mean([len(token) for token in tokens])\n",
    "    return 0\n",
    "\n",
    "def get_file_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return len(filename)\n",
    "\n",
    "def get_dot_count_in_file(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return filename.count('.')\n",
    "\n",
    "def get_delimiter_count_in_file(url):\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    delimiters = ['.', '_', '-']\n",
    "    return sum(filename.count(delimiter) for delimiter in delimiters)\n",
    "\n",
    "def get_arguments_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(query)\n",
    "\n",
    "def get_number_of_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(parse_qs(query))\n",
    "\n",
    "def get_length_of_largest_argument_value(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    if params:\n",
    "        return max(len(max(values, key=len)) for values in params.values())\n",
    "    return 0\n",
    "\n",
    "def get_max_delimiters_in_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    delimiters = ['&', '=', '-', '_']\n",
    "    if params:\n",
    "        return max(sum(value.count(delimiter) for delimiter in delimiters) for values in params.values() for value in values)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_hyphen_count_in_domain(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return domain.count('-')\n",
    "\n",
    "def contains_ip(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    try:\n",
    "        socket.inet_aton(domain)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def get_domain_features(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(ensure_scheme(url)).netloc\n",
    "    tokens = domain.split('.')\n",
    "    \n",
    "    # Domain Length\n",
    "    domain_length = len(domain)\n",
    "    \n",
    "    # Count of Tokens in the Domain\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Length of Largest Token in the Domain\n",
    "    largest_token_length = max(len(token) for token in tokens) if tokens else 0\n",
    "    \n",
    "    # Average Domain Token Length\n",
    "    average_token_length = sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n",
    "    \n",
    "    return domain_length, token_count, largest_token_length, average_token_length\n",
    "\n",
    "# New feature extraction functions\n",
    "def get_special_character_count(url):\n",
    "    special_characters = ['@', '=', '+', '*', '?', '&', '%', '$', '#', '!']\n",
    "    return sum(url.count(char) for char in special_characters)\n",
    "\n",
    "def get_entropy(url):\n",
    "    # Count the frequency of each character in the string\n",
    "    freq = Counter(url)\n",
    "    # Calculate the probabilities\n",
    "    probs = [count / len(url) for count in freq.values()]\n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -sum(p * math.log(p, 2) for p in probs if p > 0)\n",
    "    return entropy\n",
    "\n",
    "def check_url_shortened(url):\n",
    "    shortened_services = ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co']\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return int(domain in shortened_services)\n",
    "\n",
    "\n",
    "def get_port_number(url):\n",
    "    url = ensure_scheme(url)\n",
    "    port = urlparse(url).port\n",
    "    return port if port else -1  # Return -1 if no port specified\n",
    "\n",
    "def get_subdomain_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain_parts = urlparse(url).netloc.split('.')\n",
    "    # Count as subdomains any parts beyond the second-level domain and TLD\n",
    "    return max(0, len(domain_parts) - 2)\n",
    "\n",
    "def get_suspicious_tld(url):\n",
    "    suspicious_tlds = ['xyz', 'top', 'loan', 'win', 'club']\n",
    "    url = ensure_scheme(url)\n",
    "    tld = urlparse(url).netloc.split('.')[-1]\n",
    "    return int(tld in suspicious_tlds)\n",
    "\n",
    "def get_numeric_ratio(url):\n",
    "    numeric_chars = sum(c.isdigit() for c in url)\n",
    "    return numeric_chars / len(url) if len(url) > 0 else 0\n",
    "\n",
    "def get_word_count(url):\n",
    "    words = re.findall(r'\\w+', url)\n",
    "    return len(words)\n",
    "\n",
    "def get_url_is_internationalized(url):\n",
    "    try:\n",
    "        url.encode('ascii')\n",
    "        return 0\n",
    "    except UnicodeEncodeError:\n",
    "        return 1\n",
    "    \n",
    "\n",
    "\n",
    "# Apply feature extraction\n",
    "features = balanced_df['URL'].apply(lambda x: pd.Series({\n",
    "    'url_length': get_url_length(x),\n",
    "    'dot_count': get_dot_count(x),\n",
    "    'hyphen_count_domain': get_hyphen_count_in_domain(x),\n",
    "    'security_sensitive_words': contains_security_sensitive_words(x),\n",
    "    'directory_length': get_directory_length(x),\n",
    "    'sub_directory_count': get_sub_directory_count(x),\n",
    "    'token_count_path': get_token_count_in_path(x),\n",
    "    'largest_token_length': get_largest_token_length(x),\n",
    "    'average_token_length': get_average_token_length(x),\n",
    "    'file_length': get_file_length(x),\n",
    "    'contains_ip': contains_ip(x),\n",
    "    'dot_count_in_file': get_dot_count_in_file(x),\n",
    "    'delimiter_count_in_file': get_delimiter_count_in_file(x),\n",
    "    'arguments_length': get_arguments_length(x),\n",
    "    'number_of_arguments': get_number_of_arguments(x),\n",
    "    'length_of_largest_argument_value': get_length_of_largest_argument_value(x),\n",
    "    'max_delimiters_in_arguments': get_max_delimiters_in_arguments(x),\n",
    "    'special_character_count': get_special_character_count(x),\n",
    "    'entropy': get_entropy(x),\n",
    "    'url_shortened': check_url_shortened(x),\n",
    "    'port_number': get_port_number(x),\n",
    "    'subdomain_count': get_subdomain_count(x),\n",
    "    'suspicious_tld': get_suspicious_tld(x),\n",
    "    'numeric_ratio': get_numeric_ratio(x),\n",
    "    'url_is_internationalized': get_url_is_internationalized(x),\n",
    "    'domain_length': get_domain_features(x)[0],\n",
    "    'domain_token_count': get_domain_features(x)[1],\n",
    "    'largest_domain_token_length': get_domain_features(x)[2],\n",
    "    'average_domain_token_length': get_domain_features(x)[3],\n",
    "    'word_count': get_word_count(x)\n",
    "}))\n",
    "\n",
    "\n",
    "# Concatenate original DF with features\n",
    "balanced_df = pd.concat([balanced_df, features], axis=1)\n",
    "\n",
    "\n",
    "# Define X and y correctly\n",
    "X = balanced_df.drop(['Label', 'URL'], axis=1)  # Features\n",
    "y = balanced_df['Label']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "# Since your features are already numerical, directly use RandomForestClassifier without TfidfVectorizer\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d20ee435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>url_length</th>\n",
       "      <th>dot_count</th>\n",
       "      <th>hyphen_count_domain</th>\n",
       "      <th>security_sensitive_words</th>\n",
       "      <th>directory_length</th>\n",
       "      <th>sub_directory_count</th>\n",
       "      <th>token_count_path</th>\n",
       "      <th>largest_token_length</th>\n",
       "      <th>...</th>\n",
       "      <th>port_number</th>\n",
       "      <th>subdomain_count</th>\n",
       "      <th>suspicious_tld</th>\n",
       "      <th>numeric_ratio</th>\n",
       "      <th>url_is_internationalized</th>\n",
       "      <th>domain_length</th>\n",
       "      <th>domain_token_count</th>\n",
       "      <th>largest_domain_token_length</th>\n",
       "      <th>average_domain_token_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>etransfers.interac.ca-ssl.net/sh/2o05I9/bdesj/...</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>betterhealthsmoothies.com/Adobe/adobe-3D6/inde...</td>\n",
       "      <td>1</td>\n",
       "      <td>51.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lloydsbank.deregister-payee-secure-auth.com/Lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>53.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.666667</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>archive.md</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pkg00-atx.netgate.com</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>infomation-fb-service.e82443.repl.co</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>img-1000736.ad-score.com</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>sosyalsat.com/help/home.html</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>storageapi.fleek.co/12678f8a-04f9-4b69-a70f-49...</td>\n",
       "      <td>1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>trepievirealestate.com/paintdesk/auth/1/inner_...</td>\n",
       "      <td>1</td>\n",
       "      <td>286.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.493007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      URL  Label  url_length  \\\n",
       "0       etransfers.interac.ca-ssl.net/sh/2o05I9/bdesj/...      1        58.0   \n",
       "1       betterhealthsmoothies.com/Adobe/adobe-3D6/inde...      1        51.0   \n",
       "2       lloydsbank.deregister-payee-secure-auth.com/Lo...      1        53.0   \n",
       "3                                              archive.md      0        10.0   \n",
       "4                                   pkg00-atx.netgate.com      0        21.0   \n",
       "...                                                   ...    ...         ...   \n",
       "299995               infomation-fb-service.e82443.repl.co      1        36.0   \n",
       "299996                           img-1000736.ad-score.com      0        24.0   \n",
       "299997                       sosyalsat.com/help/home.html      1        28.0   \n",
       "299998  storageapi.fleek.co/12678f8a-04f9-4b69-a70f-49...      1        75.0   \n",
       "299999  trepievirealestate.com/paintdesk/auth/1/inner_...      1       286.0   \n",
       "\n",
       "        dot_count  hyphen_count_domain  security_sensitive_words  \\\n",
       "0             4.0                  1.0                       0.0   \n",
       "1             2.0                  0.0                       0.0   \n",
       "2             3.0                  3.0                       1.0   \n",
       "3             1.0                  0.0                       0.0   \n",
       "4             2.0                  1.0                       0.0   \n",
       "...           ...                  ...                       ...   \n",
       "299995        3.0                  2.0                       0.0   \n",
       "299996        2.0                  2.0                       0.0   \n",
       "299997        2.0                  0.0                       0.0   \n",
       "299998        3.0                  0.0                       0.0   \n",
       "299999        2.0                  0.0                       0.0   \n",
       "\n",
       "        directory_length  sub_directory_count  token_count_path  \\\n",
       "0                   29.0                  3.0               4.0   \n",
       "1                   26.0                  2.0               3.0   \n",
       "2                   10.0                  0.0               1.0   \n",
       "3                    0.0                 -1.0               0.0   \n",
       "4                    0.0                 -1.0               0.0   \n",
       "...                  ...                  ...               ...   \n",
       "299995               0.0                 -1.0               0.0   \n",
       "299996               0.0                 -1.0               0.0   \n",
       "299997              15.0                  1.0               2.0   \n",
       "299998              56.0                  1.0               2.0   \n",
       "299999              33.0                  3.0               4.0   \n",
       "\n",
       "        largest_token_length  ...  port_number  subdomain_count  \\\n",
       "0                       12.0  ...         -1.0              2.0   \n",
       "1                        9.0  ...         -1.0              0.0   \n",
       "2                        9.0  ...         -1.0              1.0   \n",
       "3                        0.0  ...         -1.0              0.0   \n",
       "4                        0.0  ...         -1.0              1.0   \n",
       "...                      ...  ...          ...              ...   \n",
       "299995                   0.0  ...         -1.0              2.0   \n",
       "299996                   0.0  ...         -1.0              1.0   \n",
       "299997                   9.0  ...         -1.0              0.0   \n",
       "299998                  43.0  ...         -1.0              1.0   \n",
       "299999                  15.0  ...         -1.0              0.0   \n",
       "\n",
       "        suspicious_tld  numeric_ratio  url_is_internationalized  \\\n",
       "0                  0.0       0.068966                       0.0   \n",
       "1                  0.0       0.039216                       0.0   \n",
       "2                  0.0       0.000000                       0.0   \n",
       "3                  0.0       0.000000                       0.0   \n",
       "4                  0.0       0.095238                       0.0   \n",
       "...                ...            ...                       ...   \n",
       "299995             0.0       0.138889                       0.0   \n",
       "299996             0.0       0.291667                       0.0   \n",
       "299997             0.0       0.000000                       0.0   \n",
       "299998             0.0       0.320000                       0.0   \n",
       "299999             0.0       0.493007                       0.0   \n",
       "\n",
       "        domain_length  domain_token_count  largest_domain_token_length  \\\n",
       "0                29.0                 4.0                         10.0   \n",
       "1                25.0                 2.0                         21.0   \n",
       "2                43.0                 3.0                         28.0   \n",
       "3                10.0                 2.0                          7.0   \n",
       "4                21.0                 3.0                          9.0   \n",
       "...               ...                 ...                          ...   \n",
       "299995           36.0                 4.0                         21.0   \n",
       "299996           24.0                 3.0                         11.0   \n",
       "299997           13.0                 2.0                          9.0   \n",
       "299998           19.0                 3.0                         10.0   \n",
       "299999           22.0                 2.0                         18.0   \n",
       "\n",
       "        average_domain_token_length  word_count  \n",
       "0                          6.500000        10.0  \n",
       "1                         12.000000         7.0  \n",
       "2                         13.666667         8.0  \n",
       "3                          4.500000         2.0  \n",
       "4                          6.333333         4.0  \n",
       "...                             ...         ...  \n",
       "299995                     8.250000         6.0  \n",
       "299996                     7.333333         5.0  \n",
       "299997                     6.000000         5.0  \n",
       "299998                     5.666667        11.0  \n",
       "299999                    10.500000        14.0  \n",
       "\n",
       "[300000 rows x 32 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9d995cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with \"entropy\" larger than 0: 3269\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'balanced_df' contains your feature \"entropy\"\n",
    "num_rows_entropy_greater_than_zero = (balanced_df['contains_ip'] > 0).sum()\n",
    "\n",
    "print(f'Number of rows with \"entropy\" larger than 0: {num_rows_entropy_greater_than_zero}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0db0b83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 162s 106ms/step - loss: 0.2499 - acc: 0.9051 - val_loss: 0.1960 - val_acc: 0.9235\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 158s 105ms/step - loss: 0.1877 - acc: 0.9290 - val_loss: 0.1700 - val_acc: 0.9320\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 157s 105ms/step - loss: 0.1630 - acc: 0.9385 - val_loss: 0.1625 - val_acc: 0.9383\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 161s 107ms/step - loss: 0.1488 - acc: 0.9443 - val_loss: 0.1441 - val_acc: 0.9444\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 159s 106ms/step - loss: 0.1397 - acc: 0.9478 - val_loss: 0.1331 - val_acc: 0.9492\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 161s 107ms/step - loss: 0.1344 - acc: 0.9504 - val_loss: 0.1287 - val_acc: 0.9506\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 161s 107ms/step - loss: 0.1275 - acc: 0.9526 - val_loss: 0.1267 - val_acc: 0.9519\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 160s 107ms/step - loss: 0.1228 - acc: 0.9547 - val_loss: 0.1205 - val_acc: 0.9535\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 161s 108ms/step - loss: 0.1179 - acc: 0.9564 - val_loss: 0.1157 - val_acc: 0.9564\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 161s 108ms/step - loss: 0.1135 - acc: 0.9579 - val_loss: 0.1194 - val_acc: 0.9546\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 178s 119ms/step - loss: 0.1103 - acc: 0.9590 - val_loss: 0.1129 - val_acc: 0.9579\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 181s 121ms/step - loss: 0.1061 - acc: 0.9608 - val_loss: 0.1078 - val_acc: 0.9595\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 181s 121ms/step - loss: 0.1016 - acc: 0.9626 - val_loss: 0.1094 - val_acc: 0.9598\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 179s 119ms/step - loss: 0.0982 - acc: 0.9641 - val_loss: 0.1083 - val_acc: 0.9601\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 182s 121ms/step - loss: 0.0953 - acc: 0.9650 - val_loss: 0.1066 - val_acc: 0.9603\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 180s 120ms/step - loss: 0.0916 - acc: 0.9664 - val_loss: 0.1027 - val_acc: 0.9622\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 179s 120ms/step - loss: 0.0888 - acc: 0.9672 - val_loss: 0.1022 - val_acc: 0.9629\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 179s 120ms/step - loss: 0.0861 - acc: 0.9688 - val_loss: 0.1010 - val_acc: 0.9633\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 180s 120ms/step - loss: 0.0840 - acc: 0.9695 - val_loss: 0.1016 - val_acc: 0.9642\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 179s 119ms/step - loss: 0.0808 - acc: 0.9706 - val_loss: 0.1008 - val_acc: 0.9634\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 181s 121ms/step - loss: 0.0787 - acc: 0.9716 - val_loss: 0.0973 - val_acc: 0.9650\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 179s 119ms/step - loss: 0.0761 - acc: 0.9725 - val_loss: 0.1054 - val_acc: 0.9642\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 180s 120ms/step - loss: 0.0740 - acc: 0.9734 - val_loss: 0.1022 - val_acc: 0.9649\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 168s 112ms/step - loss: 0.0709 - acc: 0.9743 - val_loss: 0.1027 - val_acc: 0.9651\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 165s 110ms/step - loss: 0.0700 - acc: 0.9749 - val_loss: 0.1050 - val_acc: 0.9646\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 169s 112ms/step - loss: 0.0682 - acc: 0.9752 - val_loss: 0.1057 - val_acc: 0.9642\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 167s 112ms/step - loss: 0.0659 - acc: 0.9762 - val_loss: 0.1031 - val_acc: 0.9654\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 170s 113ms/step - loss: 0.0644 - acc: 0.9768 - val_loss: 0.1018 - val_acc: 0.9657\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 166s 110ms/step - loss: 0.0629 - acc: 0.9773 - val_loss: 0.1077 - val_acc: 0.9645\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 166s 111ms/step - loss: 0.0616 - acc: 0.9779 - val_loss: 0.1051 - val_acc: 0.9660\n",
      "1875/1875 [==============================] - 25s 13ms/step\n",
      "Accuracy: 0.9651333333333333\n",
      "Confusion Matrix:\n",
      " [[29643   419]\n",
      " [ 1673 28265]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     30062\n",
      "           1       0.99      0.94      0.96     29938\n",
      "\n",
      "    accuracy                           0.97     60000\n",
      "   macro avg       0.97      0.97      0.97     60000\n",
      "weighted avg       0.97      0.97      0.97     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Assuming balanced_df is your DataFrame with URLs and labels\n",
    "\n",
    "# Tokenization and sequence padding parameters\n",
    "max_len = 100  # Adjust based on the length of the longest URL in your dataset\n",
    "max_words = 60000  # Adjust based on the size of your vocabulary\n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = Tokenizer(num_words=max_words, char_level=True)\n",
    "tokenizer.fit_on_texts(balanced_df['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df['Label'])\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 32, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))  # Add return_sequences if stacking LSTM layers\n",
    "model.add(Dropout(0.5))  # Adjust dropout rate as needed\n",
    "model.add(Bidirectional(LSTM(32)))  # Second LSTM layer, without return_sequences\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))  # Add L2 regularization\n",
    "model.add(Dropout(0.5))  # Adjust dropout rate as needed\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c5fdf8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 162/1500 [==>...........................] - ETA: 4:47 - loss: 0.3578 - acc: 0.8515"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[0;32m     55\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    869\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    870\u001b[0m   )\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Tokenization and sequence padding parameters\n",
    "max_len = 200  # Adjust based on the length of the longest URL in your dataset\n",
    "max_words = 60000  # Adjust based on the size of your vocabulary\n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = Tokenizer(num_words=max_words, char_level=True)\n",
    "tokenizer.fit_on_texts(balanced_df['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df['Label'])\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# RNN model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 32, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Use Adam optimizer with a lower learning rate\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17a440cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error querying WHOIS data: No match for \"CA-SSL.NET\".\n",
      ">>> Last update of whois database: 2024-02-15T19:51:09Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n",
      "Error querying WHOIS data: No match for \"CA-SSL.NET\".\n",
      ">>> Last update of whois database: 2024-02-15T19:51:09Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n",
      "Error querying WHOIS data: No match for \"CA-SSL.NET\".\n",
      ">>> Last update of whois database: 2024-02-15T19:51:09Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n",
      "Error querying WHOIS data: No match for \"CA-SSL.NET\".\n",
      ">>> Last update of whois database: 2024-02-15T19:51:09Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error querying WHOIS data: No match for \"DEREGISTER-PAYEE-SECURE-AUTH.COM\".\n",
      ">>> Last update of whois database: 2024-02-15T19:51:25Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n",
      "Error querying WHOIS data: No match for \"DEREGISTER-PAYEE-SECURE-AUTH.COM\".\n",
      ">>> Last update of whois database: 2024-02-15T19:51:25Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n",
      "Error querying WHOIS data: No match for \"DEREGISTER-PAYEE-SECURE-AUTH.COM\".\n",
      ">>> Last update of whois database: 2024-02-15T19:51:25Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n",
      "Error querying WHOIS data: No match for \"DEREGISTER-PAYEE-SECURE-AUTH.COM\".\n",
      ">>> Last update of whois database: 2024-02-15T19:51:25Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "Error querying WHOIS data: No match for \"COMMBANKNETCODE.COM\".\n",
      ">>> Last update of whois database: 2024-02-15T19:51:56Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n",
      "Error querying WHOIS data: No match for \"COMMBANKNETCODE.COM\".\n",
      ">>> Last update of whois database: 2024-02-15T19:51:56Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n",
      "Error querying WHOIS data: No match for \"COMMBANKNETCODE.COM\".\n",
      ">>> Last update of whois database: 2024-02-15T19:51:56Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n",
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error querying WHOIS data: No match for \"COIANASBSBSELOG.AZUREWEBSITES.NET\".\n",
      ">>> Last update of whois database: 2024-02-15T19:52:11Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n",
      "Error querying WHOIS data: No match for \"COIANASBSBSELOG.AZUREWEBSITES.NET\".\n",
      ">>> Last update of whois database: 2024-02-15T19:52:11Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error trying to connect to socket: closing socket - timed out\n",
      "Error querying WHOIS data: No match for \"COIANASBSBSELOG.AZUREWEBSITES.NET\".\n",
      ">>> Last update of whois database: 2024-02-15T19:52:26Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n",
      "Error trying to connect to socket: closing socket - timed out\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\whois\\whois.py:181\u001b[0m, in \u001b[0;36mNICClient.whois\u001b[1;34m(self, query, hostname, flags, many_results, quiet)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 181\u001b[0m     d \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mrecv(\u001b[38;5;241m4096\u001b[39m)\n\u001b[0;32m    182\u001b[0m     response \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d\n",
      "\u001b[1;31mTimeoutError\u001b[0m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 209\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# Assume balanced_df is your DataFrame containing URLs and Labels\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# balanced_df = pd.DataFrame({'URL': ['example.com', 'another-example.com'], 'Label': [0, 1]})\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# Feature extraction as an example\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m features \u001b[38;5;241m=\u001b[39m balanced_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pd\u001b[38;5;241m.\u001b[39mSeries({\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl_length\u001b[39m\u001b[38;5;124m'\u001b[39m: get_url_length(x),\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdot_count\u001b[39m\u001b[38;5;124m'\u001b[39m: get_dot_count(x),\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecurity_sensitive_words\u001b[39m\u001b[38;5;124m'\u001b[39m: contains_security_sensitive_words(x),\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirectory_length\u001b[39m\u001b[38;5;124m'\u001b[39m: get_directory_length(x),\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub_directory_count\u001b[39m\u001b[38;5;124m'\u001b[39m: get_sub_directory_count(x),\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_count_path\u001b[39m\u001b[38;5;124m'\u001b[39m: get_token_count_in_path(x),\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlargest_token_length\u001b[39m\u001b[38;5;124m'\u001b[39m: get_largest_token_length(x),\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_token_length\u001b[39m\u001b[38;5;124m'\u001b[39m: get_average_token_length(x),\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_length\u001b[39m\u001b[38;5;124m'\u001b[39m: get_file_length(x),\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdot_count_in_file\u001b[39m\u001b[38;5;124m'\u001b[39m: get_dot_count_in_file(x),\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelimiter_count_in_file\u001b[39m\u001b[38;5;124m'\u001b[39m: get_delimiter_count_in_file(x),\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marguments_length\u001b[39m\u001b[38;5;124m'\u001b[39m: get_arguments_length(x),\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber_of_arguments\u001b[39m\u001b[38;5;124m'\u001b[39m: get_number_of_arguments(x),\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength_of_largest_argument_value\u001b[39m\u001b[38;5;124m'\u001b[39m: get_length_of_largest_argument_value(x),\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_delimiters_in_arguments\u001b[39m\u001b[38;5;124m'\u001b[39m: get_max_delimiters_in_arguments(x),\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyphen_count_domain\u001b[39m\u001b[38;5;124m'\u001b[39m: get_hyphen_count_in_domain(x),\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontains_ip\u001b[39m\u001b[38;5;124m'\u001b[39m: contains_ip(x),\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial_character_count\u001b[39m\u001b[38;5;124m'\u001b[39m: get_special_character_count(x),\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m: get_entropy(x),\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl_shortened\u001b[39m\u001b[38;5;124m'\u001b[39m: check_url_shortened(x),\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport_number\u001b[39m\u001b[38;5;124m'\u001b[39m: get_port_number(x),\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubdomain_count\u001b[39m\u001b[38;5;124m'\u001b[39m: get_subdomain_count(x),\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuspicious_tld\u001b[39m\u001b[38;5;124m'\u001b[39m: get_suspicious_tld(x),\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: get_numeric_ratio(x),\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl_is_internationalized\u001b[39m\u001b[38;5;124m'\u001b[39m: get_url_is_internationalized(x),\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain_age_months\u001b[39m\u001b[38;5;124m'\u001b[39m: get_domain_age_months(x),\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain_expiry_age_months\u001b[39m\u001b[38;5;124m'\u001b[39m: get_domain_expiry_age_months(x),\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain_updating_age_days\u001b[39m\u001b[38;5;124m'\u001b[39m: get_domain_updating_age_days(x),\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzip_code_of_domain_holder\u001b[39m\u001b[38;5;124m'\u001b[39m: get_zip_code_of_domain_holder(x),\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m: get_word_count(x)\n\u001b[0;32m    240\u001b[0m }))\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Append features to the DataFrame\u001b[39;00m\n\u001b[0;32m    243\u001b[0m balanced_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([balanced_df, features], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[11], line 236\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# Assume balanced_df is your DataFrame containing URLs and Labels\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# balanced_df = pd.DataFrame({'URL': ['example.com', 'another-example.com'], 'Label': [0, 1]})\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# Feature extraction as an example\u001b[39;00m\n\u001b[0;32m    209\u001b[0m features \u001b[38;5;241m=\u001b[39m balanced_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pd\u001b[38;5;241m.\u001b[39mSeries({\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl_length\u001b[39m\u001b[38;5;124m'\u001b[39m: get_url_length(x),\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdot_count\u001b[39m\u001b[38;5;124m'\u001b[39m: get_dot_count(x),\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecurity_sensitive_words\u001b[39m\u001b[38;5;124m'\u001b[39m: contains_security_sensitive_words(x),\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirectory_length\u001b[39m\u001b[38;5;124m'\u001b[39m: get_directory_length(x),\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub_directory_count\u001b[39m\u001b[38;5;124m'\u001b[39m: get_sub_directory_count(x),\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_count_path\u001b[39m\u001b[38;5;124m'\u001b[39m: get_token_count_in_path(x),\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlargest_token_length\u001b[39m\u001b[38;5;124m'\u001b[39m: get_largest_token_length(x),\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_token_length\u001b[39m\u001b[38;5;124m'\u001b[39m: get_average_token_length(x),\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_length\u001b[39m\u001b[38;5;124m'\u001b[39m: get_file_length(x),\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdot_count_in_file\u001b[39m\u001b[38;5;124m'\u001b[39m: get_dot_count_in_file(x),\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelimiter_count_in_file\u001b[39m\u001b[38;5;124m'\u001b[39m: get_delimiter_count_in_file(x),\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marguments_length\u001b[39m\u001b[38;5;124m'\u001b[39m: get_arguments_length(x),\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber_of_arguments\u001b[39m\u001b[38;5;124m'\u001b[39m: get_number_of_arguments(x),\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength_of_largest_argument_value\u001b[39m\u001b[38;5;124m'\u001b[39m: get_length_of_largest_argument_value(x),\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_delimiters_in_arguments\u001b[39m\u001b[38;5;124m'\u001b[39m: get_max_delimiters_in_arguments(x),\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyphen_count_domain\u001b[39m\u001b[38;5;124m'\u001b[39m: get_hyphen_count_in_domain(x),\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontains_ip\u001b[39m\u001b[38;5;124m'\u001b[39m: contains_ip(x),\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial_character_count\u001b[39m\u001b[38;5;124m'\u001b[39m: get_special_character_count(x),\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m: get_entropy(x),\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl_shortened\u001b[39m\u001b[38;5;124m'\u001b[39m: check_url_shortened(x),\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport_number\u001b[39m\u001b[38;5;124m'\u001b[39m: get_port_number(x),\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubdomain_count\u001b[39m\u001b[38;5;124m'\u001b[39m: get_subdomain_count(x),\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuspicious_tld\u001b[39m\u001b[38;5;124m'\u001b[39m: get_suspicious_tld(x),\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: get_numeric_ratio(x),\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl_is_internationalized\u001b[39m\u001b[38;5;124m'\u001b[39m: get_url_is_internationalized(x),\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain_age_months\u001b[39m\u001b[38;5;124m'\u001b[39m: get_domain_age_months(x),\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain_expiry_age_months\u001b[39m\u001b[38;5;124m'\u001b[39m: get_domain_expiry_age_months(x),\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain_updating_age_days\u001b[39m\u001b[38;5;124m'\u001b[39m: get_domain_updating_age_days(x),\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzip_code_of_domain_holder\u001b[39m\u001b[38;5;124m'\u001b[39m: get_zip_code_of_domain_holder(x),\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m: get_word_count(x)\n\u001b[0;32m    240\u001b[0m }))\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Append features to the DataFrame\u001b[39;00m\n\u001b[0;32m    243\u001b[0m balanced_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([balanced_df, features], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 137\u001b[0m, in \u001b[0;36mget_domain_expiry_age_months\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_domain_expiry_age_months\u001b[39m(url):\n\u001b[1;32m--> 137\u001b[0m     domain_info \u001b[38;5;241m=\u001b[39m get_domain_info(url)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m domain_info \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(domain_info\u001b[38;5;241m.\u001b[39mexpiration_date, (datetime, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m    139\u001b[0m         expiry_date \u001b[38;5;241m=\u001b[39m domain_info\u001b[38;5;241m.\u001b[39mexpiration_date \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(domain_info\u001b[38;5;241m.\u001b[39mexpiration_date, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m domain_info\u001b[38;5;241m.\u001b[39mexpiration_date[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[11], line 123\u001b[0m, in \u001b[0;36mget_domain_info\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_domain_info\u001b[39m(url):\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 123\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m whois\u001b[38;5;241m.\u001b[39mwhois(url)\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# Catching all exceptions\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError querying WHOIS data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\whois\\__init__.py:51\u001b[0m, in \u001b[0;36mwhois\u001b[1;34m(url, command, flags, executable)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# try builtin client\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     nic_client \u001b[38;5;241m=\u001b[39m NICClient()\n\u001b[1;32m---> 51\u001b[0m     text \u001b[38;5;241m=\u001b[39m nic_client\u001b[38;5;241m.\u001b[39mwhois_lookup(\u001b[38;5;28;01mNone\u001b[39;00m, domain\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midna\u001b[39m\u001b[38;5;124m'\u001b[39m), flags)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m WhoisEntry\u001b[38;5;241m.\u001b[39mload(domain, text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\whois\\whois.py:322\u001b[0m, in \u001b[0;36mNICClient.whois_lookup\u001b[1;34m(self, options, query_arg, flags, quiet)\u001b[0m\n\u001b[0;32m    320\u001b[0m nichost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoose_server(query_arg)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nichost \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 322\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhois(query_arg, nichost, flags, quiet\u001b[38;5;241m=\u001b[39mquiet)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\whois\\whois.py:197\u001b[0m, in \u001b[0;36mNICClient.whois\u001b[1;34m(self, query, hostname, flags, many_results, quiet)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m exc: \u001b[38;5;66;03m# 'response' is assigned a value (also a str) even on socket timeout\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n\u001b[1;32m--> 197\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError trying to connect to socket: closing socket - \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(exc))\n\u001b[0;32m    198\u001b[0m     s\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    199\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSocket not responding: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(exc)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import math\n",
    "import re\n",
    "import whois\n",
    "from datetime import datetime\n",
    "import socket\n",
    "from collections import Counter\n",
    "\n",
    "def ensure_scheme(url):\n",
    "    if not urlparse(url).scheme:\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "def get_url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "def get_dot_count(url):\n",
    "    return url.count('.')\n",
    "\n",
    "def contains_security_sensitive_words(url):\n",
    "    security_sensitive_words = ['login', 'signin', 'auth', 'bank', 'update', 'account', 'verification', 'authenticate', 'authentication', 'verify', 'user']\n",
    "    return int(any(word in url for word in security_sensitive_words))\n",
    "\n",
    "def get_directory_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return len(path)\n",
    "\n",
    "def get_sub_directory_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return path.count('/') - 1\n",
    "\n",
    "def get_token_count_in_path(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    return len(tokens) - 1\n",
    "\n",
    "def get_largest_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    if tokens:\n",
    "        return max(len(token) for token in tokens)\n",
    "    return 0\n",
    "\n",
    "def get_average_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = [token for token in path.split('/') if token]\n",
    "    if tokens:\n",
    "        return np.mean([len(token) for token in tokens])\n",
    "    return 0\n",
    "\n",
    "def get_file_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return len(filename)\n",
    "\n",
    "def get_dot_count_in_file(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return filename.count('.')\n",
    "\n",
    "def get_delimiter_count_in_file(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    delimiters = ['.', '_', '-']\n",
    "    return sum(filename.count(delimiter) for delimiter in delimiters)\n",
    "\n",
    "def get_arguments_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(query)\n",
    "\n",
    "def get_number_of_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(parse_qs(query))\n",
    "\n",
    "def get_length_of_largest_argument_value(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    if params:\n",
    "        return max(len(max(values, key=len)) for values in params.values())\n",
    "    return 0\n",
    "\n",
    "def get_max_delimiters_in_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    delimiters = ['&', '=', '-', '_']\n",
    "    if params:\n",
    "        return max(sum(value.count(delimiter) for delimiter in delimiters) for values in params.values() for value in values)\n",
    "    return 0\n",
    "\n",
    "def get_hyphen_count_in_domain(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return domain.count('-')\n",
    "\n",
    "def contains_ip(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    try:\n",
    "        socket.inet_aton(domain)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_domain_info(url):\n",
    "    try:\n",
    "        return whois.whois(url)\n",
    "    except Exception as e:  # Catching all exceptions\n",
    "        print(f\"Error querying WHOIS data: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_domain_age_months(url):\n",
    "    domain_info = get_domain_info(url)\n",
    "    if domain_info and isinstance(domain_info.creation_date, (datetime, list)):\n",
    "        creation_date = domain_info.creation_date if not isinstance(domain_info.creation_date, list) else domain_info.creation_date[0]\n",
    "        today = datetime.now()\n",
    "        return round((today - creation_date).days / 30) if creation_date else 0\n",
    "    return 0\n",
    "\n",
    "def get_domain_expiry_age_months(url):\n",
    "    domain_info = get_domain_info(url)\n",
    "    if domain_info and isinstance(domain_info.expiration_date, (datetime, list)):\n",
    "        expiry_date = domain_info.expiration_date if not isinstance(domain_info.expiration_date, list) else domain_info.expiration_date[0]\n",
    "        today = datetime.now()\n",
    "        return round((expiry_date - today).days / 30) if expiry_date else 0\n",
    "    return 0\n",
    "\n",
    "def get_domain_updating_age_days(url):\n",
    "    domain_info = get_domain_info(url)\n",
    "    if domain_info and isinstance(domain_info.updated_date, (datetime, list)):\n",
    "        updated_date = domain_info.updated_date if not isinstance(domain_info.updated_date, list) else domain_info.updated_date[0]\n",
    "        today = datetime.now()\n",
    "        return (today - updated_date).days if updated_date else 0\n",
    "    return 0\n",
    "\n",
    "def get_zip_code_of_domain_holder(url):\n",
    "    domain_info = get_domain_info(url)\n",
    "    if domain_info and domain_info.address:\n",
    "        return domain_info.address[-1] if isinstance(domain_info.address, list) else ''\n",
    "    return ''\n",
    "\n",
    "def get_special_character_count(url):\n",
    "    special_characters = ['@', '=', '+', '*', '?', '&', '%', '$', '#', '!']\n",
    "    return sum(url.count(char) for char in special_characters)\n",
    "\n",
    "def get_entropy(url):\n",
    "    freq = Counter(url)\n",
    "    probs = [count / len(url) for count in freq.values()]\n",
    "    entropy = -sum(p * math.log(p, 2) for p in probs if p > 0)\n",
    "    return entropy\n",
    "\n",
    "def check_url_shortened(url):\n",
    "    shortened_services = ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co']\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return int(domain in shortened_services)\n",
    "\n",
    "def get_port_number(url):\n",
    "    url = ensure_scheme(url)\n",
    "    port = urlparse(url).port\n",
    "    return port if port else -1\n",
    "\n",
    "def get_subdomain_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain_parts = urlparse(url).netloc.split('.')\n",
    "    return max(0, len(domain_parts) - 2)\n",
    "\n",
    "def get_suspicious_tld(url):\n",
    "    suspicious_tlds = ['xyz', 'top', 'loan', 'win', 'club']\n",
    "    url = ensure_scheme(url)\n",
    "    tld = urlparse(url).netloc.split('.')[-1]\n",
    "    return int(tld in suspicious_tlds)\n",
    "\n",
    "def get_numeric_ratio(url):\n",
    "    numeric_chars = sum(c.isdigit() for c in url)\n",
    "    return numeric_chars / len(url) if len(url) > 0 else 0\n",
    "\n",
    "def get_word_count(url):\n",
    "    words = re.findall(r'\\w+', url)\n",
    "    return len(words)\n",
    "\n",
    "def get_url_is_internationalized(url):\n",
    "    try:\n",
    "        url.encode('ascii')\n",
    "        return 0\n",
    "    except UnicodeEncodeError:\n",
    "        return 1\n",
    "\n",
    "# Assume balanced_df is your DataFrame containing URLs and Labels\n",
    "# balanced_df = pd.DataFrame({'URL': ['example.com', 'another-example.com'], 'Label': [0, 1]})\n",
    "\n",
    "# Feature extraction as an example\n",
    "features = balanced_df['URL'].apply(lambda x: pd.Series({\n",
    "    'url_length': get_url_length(x),\n",
    "    'dot_count': get_dot_count(x),\n",
    "    'security_sensitive_words': contains_security_sensitive_words(x),\n",
    "    'directory_length': get_directory_length(x),\n",
    "    'sub_directory_count': get_sub_directory_count(x),\n",
    "    'token_count_path': get_token_count_in_path(x),\n",
    "    'largest_token_length': get_largest_token_length(x),\n",
    "    'average_token_length': get_average_token_length(x),\n",
    "    'file_length': get_file_length(x),\n",
    "    'dot_count_in_file': get_dot_count_in_file(x),\n",
    "    'delimiter_count_in_file': get_delimiter_count_in_file(x),\n",
    "    'arguments_length': get_arguments_length(x),\n",
    "    'number_of_arguments': get_number_of_arguments(x),\n",
    "    'length_of_largest_argument_value': get_length_of_largest_argument_value(x),\n",
    "    'max_delimiters_in_arguments': get_max_delimiters_in_arguments(x),\n",
    "    'hyphen_count_domain': get_hyphen_count_in_domain(x),\n",
    "    'contains_ip': contains_ip(x),\n",
    "    'special_character_count': get_special_character_count(x),\n",
    "    'entropy': get_entropy(x),\n",
    "    'url_shortened': check_url_shortened(x),\n",
    "    'port_number': get_port_number(x),\n",
    "    'subdomain_count': get_subdomain_count(x),\n",
    "    'suspicious_tld': get_suspicious_tld(x),\n",
    "    'numeric_ratio': get_numeric_ratio(x),\n",
    "    'url_is_internationalized': get_url_is_internationalized(x),\n",
    "    'domain_age_months': get_domain_age_months(x),\n",
    "    'domain_expiry_age_months': get_domain_expiry_age_months(x),\n",
    "    'domain_updating_age_days': get_domain_updating_age_days(x),\n",
    "    'zip_code_of_domain_holder': get_zip_code_of_domain_holder(x),\n",
    "    'word_count': get_word_count(x)\n",
    "}))\n",
    "\n",
    "# Append features to the DataFrame\n",
    "balanced_df = pd.concat([balanced_df, features], axis=1)\n",
    "\n",
    "# Define X and y\n",
    "X = balanced_df.drop(['Label', 'URL'], axis=1)  # Features\n",
    "y = balanced_df['Label']  # Target variable\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9be1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
