{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009c0099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import resample\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import socket\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\User\\\\Desktop\\\\url_dataset_updated.csv')\n",
    "\n",
    "# Drop all duplicates from df\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Reset the index after dropping duplicates\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "url_column_name = 'URL'  \n",
    "\n",
    "# Function to concatenate \"https://\" to benign URLs \n",
    "def add_https(url, label):\n",
    "    if label == 0:\n",
    "        return \"https://\" + url\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "# Apply the function to the URL column\n",
    "df[url_column_name] = df.apply(lambda row: add_https(row[url_column_name], row['Label']), axis=1)\n",
    "\n",
    "\n",
    "# Separate the dataset into malicious and benign\n",
    "malicious_df = df[df['Label'] == 1]\n",
    "benign_df = df[df['Label'] == 0]\n",
    "\n",
    "# Randomly sample 150,000 entries from each\n",
    "malicious_sampled_df = resample(malicious_df, n_samples=150000, random_state=42)\n",
    "benign_sampled_df = resample(benign_df, n_samples=150000, random_state=42)\n",
    "\n",
    "# Combine the sampled data\n",
    "balanced_df = pd.concat([malicious_sampled_df, benign_sampled_df])\n",
    "\n",
    "# Shuffle the combined dataset to mix malicious and benign URLs\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Now, balanced_df contains the balanced dataset ready for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a02635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_scheme(url):\n",
    "    if not urlparse(url).scheme:\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "# Feature extraction functions\n",
    "def get_url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "def get_dot_count(url):\n",
    "    return url.count('.')\n",
    "\n",
    "def contains_security_sensitive_words(url):\n",
    "    security_sensitive_words = [\n",
    "    'login', 'password', 'admin', 'root', 'secret', 'private', 'secure', 'confidential', \n",
    "    'bank', 'creditcard', 'account', 'authentication', 'authorization', 'session', 'token', \n",
    "    'apikey', 'ssl', 'https', 'secure', 'encrypted', 'auth', 'signin', 'signup', 'verification', \n",
    "    'resetpassword', 'change-password', 'forgot-password', 'otp', '2fa', 'phishing', 'malware', \n",
    "    'virus', 'trojan', 'exploit', 'hacker', 'attack', 'security', 'vulnerable', 'injection', \n",
    "    'xss', 'csrf', 'dos', 'ddos', 'bruteforce', 'firewall', 'vpn', 'proxy', 'tor', 'security-question', \n",
    "    'privacy-policy'\n",
    "]\n",
    "\n",
    "    return int(any(word in url for word in security_sensitive_words))\n",
    "\n",
    "def get_directory_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return len(path)\n",
    "\n",
    "def get_sub_directory_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return path.count('/') - 1\n",
    "\n",
    "def get_token_count_in_path(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    return len(tokens) - 1\n",
    "\n",
    "def get_largest_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    if tokens:\n",
    "        return max(len(token) for token in tokens)\n",
    "    return 0\n",
    "\n",
    "def get_average_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = [token for token in path.split('/') if token]\n",
    "    if tokens:\n",
    "        return np.mean([len(token) for token in tokens])\n",
    "    return 0\n",
    "\n",
    "def get_file_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return len(filename)\n",
    "\n",
    "def get_dot_count_in_file(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return filename.count('.')\n",
    "\n",
    "def get_delimiter_count_in_file(url):\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    delimiters = ['.', '_', '-']\n",
    "    return sum(filename.count(delimiter) for delimiter in delimiters)\n",
    "\n",
    "def get_arguments_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(query)\n",
    "\n",
    "def get_number_of_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(parse_qs(query))\n",
    "\n",
    "def get_length_of_largest_argument_value(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    if params:\n",
    "        return max(len(max(values, key=len)) for values in params.values())\n",
    "    return 0\n",
    "\n",
    "def get_max_delimiters_in_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    delimiters = ['&', '=', '-', '_']\n",
    "    if params:\n",
    "        return max(sum(value.count(delimiter) for delimiter in delimiters) for values in params.values() for value in values)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_hyphen_count_in_domain(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return domain.count('-')\n",
    "\n",
    "def contains_ip(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    try:\n",
    "        socket.inet_aton(domain)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def get_domain_features(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(ensure_scheme(url)).netloc\n",
    "    tokens = domain.split('.')\n",
    "    \n",
    "    # Domain Length\n",
    "    domain_length = len(domain)\n",
    "    \n",
    "    # Count of Tokens in the Domain\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Length of Largest Token in the Domain\n",
    "    largest_token_length = max(len(token) for token in tokens) if tokens else 0\n",
    "    \n",
    "    # Average Domain Token Length\n",
    "    average_token_length = sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n",
    "    \n",
    "    return domain_length, token_count, largest_token_length, average_token_length\n",
    "\n",
    "# New feature extraction functions\n",
    "def get_special_character_count(url):\n",
    "    special_characters = ['@', '=', '+', '*', '?', '&', '%', '$', '#', '!']\n",
    "    return sum(url.count(char) for char in special_characters)\n",
    "\n",
    "def get_entropy(url):\n",
    "    # Count the frequency of each character in the string\n",
    "    freq = Counter(url)\n",
    "    # Calculate the probabilities\n",
    "    probs = [count / len(url) for count in freq.values()]\n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -sum(p * math.log(p, 2) for p in probs if p > 0)\n",
    "    return entropy\n",
    "\n",
    "def check_url_shortened(url):\n",
    "    shortened_services = ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co']\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return int(domain in shortened_services)\n",
    "\n",
    "def get_subdomain_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain_parts = urlparse(url).netloc.split('.')\n",
    "    # Count as subdomains any parts beyond the second-level domain and TLD\n",
    "    return max(0, len(domain_parts) - 2)\n",
    "\n",
    "def get_suspicious_tld(url):\n",
    "    suspicious_tlds = ['xyz', 'top', 'loan', 'win', 'club']\n",
    "    url = ensure_scheme(url)\n",
    "    tld = urlparse(url).netloc.split('.')[-1]\n",
    "    return int(tld in suspicious_tlds)\n",
    "\n",
    "def get_numeric_ratio(url):\n",
    "    numeric_chars = sum(c.isdigit() for c in url)\n",
    "    return numeric_chars / len(url) if len(url) > 0 else 0\n",
    "\n",
    "def get_word_count(url):\n",
    "    words = re.findall(r'\\w+', url)\n",
    "    return len(words)\n",
    "\n",
    "def is_https(url, timeout=0.5):\n",
    "    return int(url.startswith(\"https\"))\n",
    "\n",
    "\n",
    "# Apply feature extraction\n",
    "features = balanced_df['URL'].apply(lambda x: pd.Series({\n",
    "    'url_length': get_url_length(x),\n",
    "    'dot_count': get_dot_count(x),\n",
    "    'hyphen_count_domain': get_hyphen_count_in_domain(x),\n",
    "    'security_sensitive_words': contains_security_sensitive_words(x),\n",
    "    'directory_length': get_directory_length(x),\n",
    "    'sub_directory_count': get_sub_directory_count(x),\n",
    "    'token_count_path': get_token_count_in_path(x),\n",
    "    'largest_token_length': get_largest_token_length(x),\n",
    "    'average_token_length': get_average_token_length(x),\n",
    "    'file_length': get_file_length(x),\n",
    "    'contains_ip': contains_ip(x),\n",
    "    'dot_count_in_file': get_dot_count_in_file(x),\n",
    "    'delimiter_count_in_file': get_delimiter_count_in_file(x),\n",
    "    'arguments_length': get_arguments_length(x),\n",
    "    'number_of_arguments': get_number_of_arguments(x),\n",
    "    'length_of_largest_argument_value': get_length_of_largest_argument_value(x),\n",
    "    'max_delimiters_in_arguments': get_max_delimiters_in_arguments(x),\n",
    "    'special_character_count': get_special_character_count(x),\n",
    "    'entropy': get_entropy(x),\n",
    "    'url_shortened': check_url_shortened(x),\n",
    "    'subdomain_count': get_subdomain_count(x),\n",
    "    'suspicious_tld': get_suspicious_tld(x),\n",
    "    'numeric_ratio': get_numeric_ratio(x),\n",
    "    'domain_length': get_domain_features(x)[0],\n",
    "    'domain_token_count': get_domain_features(x)[1],\n",
    "    'largest_domain_token_length': get_domain_features(x)[2],\n",
    "    'average_domain_token_length': get_domain_features(x)[3],\n",
    "    'word_count': get_word_count(x),\n",
    "    'is_https': is_https(x)\n",
    "}))\n",
    "\n",
    "\n",
    "# Concatenate original DF with features\n",
    "balanced_df = pd.concat([balanced_df, features], axis=1)\n",
    "\n",
    "\n",
    "# Extracting TF-IDF features from URLs\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to top 5000 features\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(balanced_df['URL'])\n",
    "\n",
    "# Convert TF-IDF features from a sparse matrix to a dense format and then to an np.ndarray\n",
    "tfidf_dense = np.asarray(tfidf_features.todense())\n",
    "\n",
    "# Define X for numerical features\n",
    "X_numerical = balanced_df.drop(['Label', 'URL'], axis=1).values  # Make sure this matches your feature extraction output\n",
    "\n",
    "# Combining TF-IDF features with numerical features\n",
    "X_combined = np.hstack((X_numerical, tfidf_dense))\n",
    "\n",
    "# Define y\n",
    "y = balanced_df['Label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71c5b570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>url_length</th>\n",
       "      <th>dot_count</th>\n",
       "      <th>hyphen_count_domain</th>\n",
       "      <th>security_sensitive_words</th>\n",
       "      <th>directory_length</th>\n",
       "      <th>sub_directory_count</th>\n",
       "      <th>token_count_path</th>\n",
       "      <th>largest_token_length</th>\n",
       "      <th>...</th>\n",
       "      <th>url_shortened</th>\n",
       "      <th>subdomain_count</th>\n",
       "      <th>suspicious_tld</th>\n",
       "      <th>numeric_ratio</th>\n",
       "      <th>domain_length</th>\n",
       "      <th>domain_token_count</th>\n",
       "      <th>largest_domain_token_length</th>\n",
       "      <th>average_domain_token_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>is_https</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://etransfers.interac.ca-ssl.net/sh/2o05I9...</td>\n",
       "      <td>1</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://betterhealthsmoothies.com/Adobe/adobe-3...</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://lloydsbank.deregister-payee-secure-auth...</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.666667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://archive.md</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://pkg00-atx.netgate.com</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>https://infomation-fb-service.e82443.repl.co</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>https://img-1000736.ad-score.com</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>http://sosyalsat.com/help/home.html</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>https://storageapi.fleek.co/12678f8a-04f9-4b69...</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.289157</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>https://trepievirealestate.com/paintdesk/auth/...</td>\n",
       "      <td>1</td>\n",
       "      <td>294.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479592</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      URL  Label  url_length  \\\n",
       "0       http://etransfers.interac.ca-ssl.net/sh/2o05I9...      1        65.0   \n",
       "1       http://betterhealthsmoothies.com/Adobe/adobe-3...      1        58.0   \n",
       "2       http://lloydsbank.deregister-payee-secure-auth...      1        60.0   \n",
       "3                                      https://archive.md      0        18.0   \n",
       "4                           https://pkg00-atx.netgate.com      0        29.0   \n",
       "...                                                   ...    ...         ...   \n",
       "299995       https://infomation-fb-service.e82443.repl.co      1        44.0   \n",
       "299996                   https://img-1000736.ad-score.com      0        32.0   \n",
       "299997                http://sosyalsat.com/help/home.html      1        35.0   \n",
       "299998  https://storageapi.fleek.co/12678f8a-04f9-4b69...      1        83.0   \n",
       "299999  https://trepievirealestate.com/paintdesk/auth/...      1       294.0   \n",
       "\n",
       "        dot_count  hyphen_count_domain  security_sensitive_words  \\\n",
       "0             4.0                  1.0                       1.0   \n",
       "1             2.0                  0.0                       0.0   \n",
       "2             3.0                  3.0                       1.0   \n",
       "3             1.0                  0.0                       1.0   \n",
       "4             2.0                  1.0                       1.0   \n",
       "...           ...                  ...                       ...   \n",
       "299995        3.0                  2.0                       1.0   \n",
       "299996        2.0                  2.0                       1.0   \n",
       "299997        2.0                  0.0                       0.0   \n",
       "299998        3.0                  0.0                       1.0   \n",
       "299999        2.0                  0.0                       1.0   \n",
       "\n",
       "        directory_length  sub_directory_count  token_count_path  \\\n",
       "0                   29.0                  3.0               4.0   \n",
       "1                   26.0                  2.0               3.0   \n",
       "2                   10.0                  0.0               1.0   \n",
       "3                    0.0                 -1.0               0.0   \n",
       "4                    0.0                 -1.0               0.0   \n",
       "...                  ...                  ...               ...   \n",
       "299995               0.0                 -1.0               0.0   \n",
       "299996               0.0                 -1.0               0.0   \n",
       "299997              15.0                  1.0               2.0   \n",
       "299998              56.0                  1.0               2.0   \n",
       "299999              33.0                  3.0               4.0   \n",
       "\n",
       "        largest_token_length  ...  url_shortened  subdomain_count  \\\n",
       "0                       12.0  ...            0.0              2.0   \n",
       "1                        9.0  ...            0.0              0.0   \n",
       "2                        9.0  ...            0.0              1.0   \n",
       "3                        0.0  ...            0.0              0.0   \n",
       "4                        0.0  ...            0.0              1.0   \n",
       "...                      ...  ...            ...              ...   \n",
       "299995                   0.0  ...            0.0              2.0   \n",
       "299996                   0.0  ...            0.0              1.0   \n",
       "299997                   9.0  ...            0.0              0.0   \n",
       "299998                  43.0  ...            0.0              1.0   \n",
       "299999                  15.0  ...            0.0              0.0   \n",
       "\n",
       "        suspicious_tld  numeric_ratio  domain_length  domain_token_count  \\\n",
       "0                  0.0       0.061538           29.0                 4.0   \n",
       "1                  0.0       0.034483           25.0                 2.0   \n",
       "2                  0.0       0.000000           43.0                 3.0   \n",
       "3                  0.0       0.000000           10.0                 2.0   \n",
       "4                  0.0       0.068966           21.0                 3.0   \n",
       "...                ...            ...            ...                 ...   \n",
       "299995             0.0       0.113636           36.0                 4.0   \n",
       "299996             0.0       0.218750           24.0                 3.0   \n",
       "299997             0.0       0.000000           13.0                 2.0   \n",
       "299998             0.0       0.289157           19.0                 3.0   \n",
       "299999             0.0       0.479592           22.0                 2.0   \n",
       "\n",
       "        largest_domain_token_length  average_domain_token_length  word_count  \\\n",
       "0                              10.0                     6.500000        11.0   \n",
       "1                              21.0                    12.000000         8.0   \n",
       "2                              28.0                    13.666667         9.0   \n",
       "3                               7.0                     4.500000         3.0   \n",
       "4                               9.0                     6.333333         5.0   \n",
       "...                             ...                          ...         ...   \n",
       "299995                         21.0                     8.250000         7.0   \n",
       "299996                         11.0                     7.333333         6.0   \n",
       "299997                          9.0                     6.000000         6.0   \n",
       "299998                         10.0                     5.666667        12.0   \n",
       "299999                         18.0                    10.500000        15.0   \n",
       "\n",
       "        is_https  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            1.0  \n",
       "4            1.0  \n",
       "...          ...  \n",
       "299995       1.0  \n",
       "299996       1.0  \n",
       "299997       0.0  \n",
       "299998       1.0  \n",
       "299999       1.0  \n",
       "\n",
       "[300000 rows x 31 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1903987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97525\n",
      "Confusion Matrix:\n",
      " [[29828   234]\n",
      " [ 1251 28687]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     30062\n",
      "           1       0.99      0.96      0.97     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Training with Logistic Regression\n",
    "model = LogisticRegression(random_state=42, max_iter=10000)  # Increased max_iter for convergence in case of large dataset\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d20ee435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95715\n",
      "Confusion Matrix:\n",
      " [[28957  1105]\n",
      " [ 1466 28472]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96     30062\n",
      "           1       0.96      0.95      0.96     29938\n",
      "\n",
      "    accuracy                           0.96     60000\n",
      "   macro avg       0.96      0.96      0.96     60000\n",
      "weighted avg       0.96      0.96      0.96     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Training with GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d995cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9786\n",
      "Confusion Matrix:\n",
      " [[29782   280]\n",
      " [ 1004 28934]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     30062\n",
      "           1       0.99      0.97      0.98     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Training with RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8353f1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9743166666666667\n",
      "Confusion Matrix:\n",
      " [[29458   604]\n",
      " [  937 29001]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97     30062\n",
      "           1       0.98      0.97      0.97     29938\n",
      "\n",
      "    accuracy                           0.97     60000\n",
      "   macro avg       0.97      0.97      0.97     60000\n",
      "weighted avg       0.97      0.97      0.97     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72e726d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9156333333333333\n",
      "Confusion Matrix:\n",
      " [[29438   624]\n",
      " [ 4438 25500]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92     30062\n",
      "           1       0.98      0.85      0.91     29938\n",
      "\n",
      "    accuracy                           0.92     60000\n",
      "   macro avg       0.92      0.92      0.92     60000\n",
      "weighted avg       0.92      0.92      0.92     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_combined)  \n",
    "\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=490)  \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0db0b83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 333s 220ms/step - loss: 0.2025 - acc: 0.9287 - val_loss: 0.1453 - val_acc: 0.9508\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 320s 213ms/step - loss: 0.1346 - acc: 0.9550 - val_loss: 0.1157 - val_acc: 0.9608\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 319s 213ms/step - loss: 0.1160 - acc: 0.9614 - val_loss: 0.1034 - val_acc: 0.9634\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 319s 213ms/step - loss: 0.1116 - acc: 0.9627 - val_loss: 0.0953 - val_acc: 0.9661\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 319s 213ms/step - loss: 0.0972 - acc: 0.9674 - val_loss: 0.0953 - val_acc: 0.9659\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 319s 213ms/step - loss: 0.0905 - acc: 0.9696 - val_loss: 0.0883 - val_acc: 0.9693\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 319s 213ms/step - loss: 0.0852 - acc: 0.9718 - val_loss: 0.0825 - val_acc: 0.9716\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 320s 214ms/step - loss: 0.0817 - acc: 0.9726 - val_loss: 0.0784 - val_acc: 0.9730\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 326s 217ms/step - loss: 0.0799 - acc: 0.9734 - val_loss: 0.0797 - val_acc: 0.9725\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 331s 221ms/step - loss: 0.0761 - acc: 0.9746 - val_loss: 0.0754 - val_acc: 0.9734\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 331s 221ms/step - loss: 0.0735 - acc: 0.9755 - val_loss: 0.0726 - val_acc: 0.9752\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 322s 214ms/step - loss: 0.0719 - acc: 0.9760 - val_loss: 0.0748 - val_acc: 0.9750\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 326s 217ms/step - loss: 0.0695 - acc: 0.9767 - val_loss: 0.0696 - val_acc: 0.9756\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 330s 220ms/step - loss: 0.0671 - acc: 0.9776 - val_loss: 0.0730 - val_acc: 0.9748\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 330s 220ms/step - loss: 0.0654 - acc: 0.9784 - val_loss: 0.0685 - val_acc: 0.9765\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 330s 220ms/step - loss: 0.0632 - acc: 0.9791 - val_loss: 0.0726 - val_acc: 0.9758\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 331s 221ms/step - loss: 0.0613 - acc: 0.9800 - val_loss: 0.0656 - val_acc: 0.9774\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 328s 219ms/step - loss: 0.0596 - acc: 0.9806 - val_loss: 0.0650 - val_acc: 0.9779\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 325s 217ms/step - loss: 0.0568 - acc: 0.9814 - val_loss: 0.0666 - val_acc: 0.9773\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 324s 216ms/step - loss: 0.0552 - acc: 0.9819 - val_loss: 0.0643 - val_acc: 0.9783\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 323s 215ms/step - loss: 0.0537 - acc: 0.9824 - val_loss: 0.0636 - val_acc: 0.9787\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 322s 215ms/step - loss: 0.0561 - acc: 0.9823 - val_loss: 0.0678 - val_acc: 0.9781\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 318s 212ms/step - loss: 0.0496 - acc: 0.9838 - val_loss: 0.0667 - val_acc: 0.9791\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 316s 211ms/step - loss: 0.0482 - acc: 0.9846 - val_loss: 0.0666 - val_acc: 0.9788\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 317s 211ms/step - loss: 0.0466 - acc: 0.9850 - val_loss: 0.0663 - val_acc: 0.9790\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 317s 211ms/step - loss: 0.0460 - acc: 0.9851 - val_loss: 0.0666 - val_acc: 0.9794\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 315s 210ms/step - loss: 0.0462 - acc: 0.9853 - val_loss: 0.0657 - val_acc: 0.9798\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 316s 211ms/step - loss: 0.0455 - acc: 0.9854 - val_loss: 0.0660 - val_acc: 0.9795\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 316s 211ms/step - loss: 0.0442 - acc: 0.9858 - val_loss: 0.0722 - val_acc: 0.9791\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 315s 210ms/step - loss: 0.0461 - acc: 0.9853 - val_loss: 0.0680 - val_acc: 0.9789\n",
      "1875/1875 [==============================] - 37s 19ms/step\n",
      "Accuracy: 0.97985\n",
      "Confusion Matrix:\n",
      " [[29724   338]\n",
      " [  871 29067]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     30062\n",
      "           1       0.99      0.97      0.98     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Tokenization and sequence padding parameters\n",
    "max_len = 200  \n",
    "max_words = 60000  \n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = Tokenizer(num_words=max_words, char_level=True)\n",
    "tokenizer.fit_on_texts(balanced_df['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df['Label'])\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 32, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))  \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Bidirectional(LSTM(32))) \n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341f748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
