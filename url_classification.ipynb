{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "009c0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import resample\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\User\\\\Desktop\\\\url_dataset_updated.csv')\n",
    "\n",
    "# Drop all duplicates from df\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Reset the index after dropping duplicates\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "url_column_name = 'URL'  # Replace with your actual column name\n",
    "\n",
    "# Function to concatenate \"https://\" to URLs labeled with 0\n",
    "def add_https(url, label):\n",
    "    if label == 0:\n",
    "        return \"https://\" + url\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "# Apply the function to the URL column\n",
    "df[url_column_name] = df.apply(lambda row: add_https(row[url_column_name], row['Label']), axis=1)\n",
    "\n",
    "\n",
    "# Separate the dataset into malicious and benign\n",
    "malicious_df = df[df['Label'] == 1]\n",
    "benign_df = df[df['Label'] == 0]\n",
    "\n",
    "# Randomly sample 150,000 entries from each\n",
    "malicious_sampled_df = resample(malicious_df, n_samples=150000, random_state=42)\n",
    "benign_sampled_df = resample(benign_df, n_samples=150000, random_state=42)\n",
    "\n",
    "# Combine the sampled data\n",
    "balanced_df = pd.concat([malicious_sampled_df, benign_sampled_df])\n",
    "\n",
    "# Shuffle the combined dataset to mix malicious and benign URLs\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Now, balanced_df contains the balanced dataset ready for further processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a02635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting maxminddb-geolite2\n",
      "  Downloading maxminddb-geolite2-2018.703.tar.gz (26.1 MB)\n",
      "     ---------------------------------------- 0.0/26.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/26.1 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/26.1 MB 325.1 kB/s eta 0:01:21\n",
      "     --------------------------------------- 0.1/26.1 MB 409.6 kB/s eta 0:01:04\n",
      "     --------------------------------------- 0.1/26.1 MB 595.3 kB/s eta 0:00:44\n",
      "     --------------------------------------- 0.2/26.1 MB 748.1 kB/s eta 0:00:35\n",
      "     ---------------------------------------- 0.3/26.1 MB 1.0 MB/s eta 0:00:25\n",
      "      --------------------------------------- 0.4/26.1 MB 1.2 MB/s eta 0:00:21\n",
      "      --------------------------------------- 0.6/26.1 MB 1.7 MB/s eta 0:00:15\n",
      "     - -------------------------------------- 0.9/26.1 MB 2.1 MB/s eta 0:00:12\n",
      "     - -------------------------------------- 1.3/26.1 MB 2.8 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 1.8/26.1 MB 3.6 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 2.6/26.1 MB 4.8 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 3.6/26.1 MB 6.1 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 5.2/26.1 MB 8.1 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 7.2/26.1 MB 10.4 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 9.8/26.1 MB 13.3 MB/s eta 0:00:02\n",
      "     ------------------ -------------------- 12.7/26.1 MB 43.7 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 15.7/26.1 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 18.7/26.1 MB 65.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 21.7/26.1 MB 65.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 24.7/26.1 MB 65.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  26.1/26.1 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 26.1/26.1 MB 46.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting maxminddb (from maxminddb-geolite2)\n",
      "  Obtaining dependency information for maxminddb from https://files.pythonhosted.org/packages/03/75/d209cef212ba44e5095e47fd9d7771cedba254e1405bb2fe3b3dfb4aa694/maxminddb-2.5.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading maxminddb-2.5.2-cp311-cp311-win_amd64.whl.metadata (5.2 kB)\n",
      "Collecting setuptools>=68.2.2 (from maxminddb->maxminddb-geolite2)\n",
      "  Obtaining dependency information for setuptools>=68.2.2 from https://files.pythonhosted.org/packages/bb/0a/203797141ec9727344c7649f6d5f6cf71b89a6c28f8f55d4f18de7a1d352/setuptools-69.1.0-py3-none-any.whl.metadata\n",
      "  Downloading setuptools-69.1.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Downloading maxminddb-2.5.2-cp311-cp311-win_amd64.whl (35 kB)\n",
      "Downloading setuptools-69.1.0-py3-none-any.whl (819 kB)\n",
      "   ---------------------------------------- 0.0/819.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 819.3/819.3 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: maxminddb-geolite2\n",
      "  Building wheel for maxminddb-geolite2 (setup.py): started\n",
      "  Building wheel for maxminddb-geolite2 (setup.py): finished with status 'done'\n",
      "  Created wheel for maxminddb-geolite2: filename=maxminddb_geolite2-2018.703-py3-none-any.whl size=26407039 sha256=6171d255e0c2c4c35ec60a28ce4fdbde8394ab2eb1d8a414411941ad722b96ae\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\2b\\94\\81\\82a7450d1609c926f4e30c642e1516679aafb576d1679cddc4\n",
      "Successfully built maxminddb-geolite2\n",
      "Installing collected packages: setuptools, maxminddb, maxminddb-geolite2\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 68.0.0\n",
      "    Uninstalling setuptools-68.0.0:\n",
      "      Successfully uninstalled setuptools-68.0.0\n",
      "Successfully installed maxminddb-2.5.2 maxminddb-geolite2-2018.703 setuptools-69.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires PyYAML==6.0.1, but you have pyyaml 6.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install maxminddb-geolite2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "71c5b570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>etransfers.interac.ca-ssl.net/sh/2o05I9/bdesj/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>betterhealthsmoothies.com/Adobe/adobe-3D6/inde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lloydsbank.deregister-payee-secure-auth.com/Lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>archive.md</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pkg00-atx.netgate.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>infomation-fb-service.e82443.repl.co</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>img-1000736.ad-score.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>sosyalsat.com/help/home.html</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>storageapi.fleek.co/12678f8a-04f9-4b69-a70f-49...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>trepievirealestate.com/paintdesk/auth/1/inner_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      URL  Label\n",
       "0       etransfers.interac.ca-ssl.net/sh/2o05I9/bdesj/...      1\n",
       "1       betterhealthsmoothies.com/Adobe/adobe-3D6/inde...      1\n",
       "2       lloydsbank.deregister-payee-secure-auth.com/Lo...      1\n",
       "3                                              archive.md      0\n",
       "4                                   pkg00-atx.netgate.com      0\n",
       "...                                                   ...    ...\n",
       "299995               infomation-fb-service.e82443.repl.co      1\n",
       "299996                           img-1000736.ad-score.com      0\n",
       "299997                       sosyalsat.com/help/home.html      1\n",
       "299998  storageapi.fleek.co/12678f8a-04f9-4b69-a70f-49...      1\n",
       "299999  trepievirealestate.com/paintdesk/auth/1/inner_...      1\n",
       "\n",
       "[300000 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1903987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9494833333333333\n",
      "Confusion Matrix:\n",
      " [[29158   904]\n",
      " [ 2127 27811]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95     30062\n",
      "           1       0.97      0.93      0.95     29938\n",
      "\n",
      "    accuracy                           0.95     60000\n",
      "   macro avg       0.95      0.95      0.95     60000\n",
      "weighted avg       0.95      0.95      0.95     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import tldextract \n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "import whois\n",
    "from datetime import datetime\n",
    "\n",
    "def ensure_scheme(url):\n",
    "    if not urlparse(url).scheme:\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "# Feature extraction functions\n",
    "def get_url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "def get_dot_count(url):\n",
    "    return url.count('.')\n",
    "\n",
    "def contains_security_sensitive_words(url):\n",
    "    security_sensitive_words = [\n",
    "    'login', 'password', 'admin', 'root', 'secret', 'private', 'secure', 'confidential', \n",
    "    'bank', 'creditcard', 'account', 'authentication', 'authorization', 'session', 'token', \n",
    "    'apikey', 'ssl', 'https', 'secure', 'encrypted', 'auth', 'signin', 'signup', 'verification', \n",
    "    'resetpassword', 'change-password', 'forgot-password', 'otp', '2fa', 'phishing', 'malware', \n",
    "    'virus', 'trojan', 'exploit', 'hacker', 'attack', 'security', 'vulnerable', 'injection', \n",
    "    'xss', 'csrf', 'dos', 'ddos', 'bruteforce', 'firewall', 'vpn', 'proxy', 'tor', 'security-question', \n",
    "    'privacy-policy'\n",
    "]\n",
    "\n",
    "    return int(any(word in url for word in security_sensitive_words))\n",
    "\n",
    "def get_directory_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return len(path)\n",
    "\n",
    "def get_sub_directory_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return path.count('/') - 1\n",
    "\n",
    "def get_token_count_in_path(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    return len(tokens) - 1\n",
    "\n",
    "def get_largest_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    if tokens:\n",
    "        return max(len(token) for token in tokens)\n",
    "    return 0\n",
    "\n",
    "def get_average_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = [token for token in path.split('/') if token]\n",
    "    if tokens:\n",
    "        return np.mean([len(token) for token in tokens])\n",
    "    return 0\n",
    "\n",
    "def get_file_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return len(filename)\n",
    "\n",
    "def get_dot_count_in_file(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return filename.count('.')\n",
    "\n",
    "def get_delimiter_count_in_file(url):\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    delimiters = ['.', '_', '-']\n",
    "    return sum(filename.count(delimiter) for delimiter in delimiters)\n",
    "\n",
    "def get_arguments_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(query)\n",
    "\n",
    "def get_number_of_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(parse_qs(query))\n",
    "\n",
    "def get_length_of_largest_argument_value(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    if params:\n",
    "        return max(len(max(values, key=len)) for values in params.values())\n",
    "    return 0\n",
    "\n",
    "def get_max_delimiters_in_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    delimiters = ['&', '=', '-', '_']\n",
    "    if params:\n",
    "        return max(sum(value.count(delimiter) for delimiter in delimiters) for values in params.values() for value in values)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_hyphen_count_in_domain(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return domain.count('-')\n",
    "\n",
    "def contains_ip(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    try:\n",
    "        socket.inet_aton(domain)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def get_domain_features(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(ensure_scheme(url)).netloc\n",
    "    tokens = domain.split('.')\n",
    "    \n",
    "    # Domain Length\n",
    "    domain_length = len(domain)\n",
    "    \n",
    "    # Count of Tokens in the Domain\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Length of Largest Token in the Domain\n",
    "    largest_token_length = max(len(token) for token in tokens) if tokens else 0\n",
    "    \n",
    "    # Average Domain Token Length\n",
    "    average_token_length = sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n",
    "    \n",
    "    return domain_length, token_count, largest_token_length, average_token_length\n",
    "\n",
    "# New feature extraction functions\n",
    "def get_special_character_count(url):\n",
    "    special_characters = ['@', '=', '+', '*', '?', '&', '%', '$', '#', '!']\n",
    "    return sum(url.count(char) for char in special_characters)\n",
    "\n",
    "def get_entropy(url):\n",
    "    # Count the frequency of each character in the string\n",
    "    freq = Counter(url)\n",
    "    # Calculate the probabilities\n",
    "    probs = [count / len(url) for count in freq.values()]\n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -sum(p * math.log(p, 2) for p in probs if p > 0)\n",
    "    return entropy\n",
    "\n",
    "def check_url_shortened(url):\n",
    "    shortened_services = ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co']\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return int(domain in shortened_services)\n",
    "\n",
    "def get_subdomain_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain_parts = urlparse(url).netloc.split('.')\n",
    "    # Count as subdomains any parts beyond the second-level domain and TLD\n",
    "    return max(0, len(domain_parts) - 2)\n",
    "\n",
    "def get_suspicious_tld(url):\n",
    "    suspicious_tlds = ['xyz', 'top', 'loan', 'win', 'club']\n",
    "    url = ensure_scheme(url)\n",
    "    tld = urlparse(url).netloc.split('.')[-1]\n",
    "    return int(tld in suspicious_tlds)\n",
    "\n",
    "def get_numeric_ratio(url):\n",
    "    numeric_chars = sum(c.isdigit() for c in url)\n",
    "    return numeric_chars / len(url) if len(url) > 0 else 0\n",
    "\n",
    "def get_word_count(url):\n",
    "    words = re.findall(r'\\w+', url)\n",
    "    return len(words)\n",
    "    \n",
    "\n",
    "# Apply feature extraction\n",
    "features = balanced_df['URL'].apply(lambda x: pd.Series({\n",
    "    'url_length': get_url_length(x),\n",
    "    'dot_count': get_dot_count(x),\n",
    "    'hyphen_count_domain': get_hyphen_count_in_domain(x),\n",
    "    'security_sensitive_words': contains_security_sensitive_words(x),\n",
    "    'directory_length': get_directory_length(x),\n",
    "    'sub_directory_count': get_sub_directory_count(x),\n",
    "    'token_count_path': get_token_count_in_path(x),\n",
    "    'largest_token_length': get_largest_token_length(x),\n",
    "    'average_token_length': get_average_token_length(x),\n",
    "    'file_length': get_file_length(x),\n",
    "    'contains_ip': contains_ip(x),\n",
    "    'dot_count_in_file': get_dot_count_in_file(x),\n",
    "    'delimiter_count_in_file': get_delimiter_count_in_file(x),\n",
    "    'arguments_length': get_arguments_length(x),\n",
    "    'number_of_arguments': get_number_of_arguments(x),\n",
    "    'length_of_largest_argument_value': get_length_of_largest_argument_value(x),\n",
    "    'max_delimiters_in_arguments': get_max_delimiters_in_arguments(x),\n",
    "    'special_character_count': get_special_character_count(x),\n",
    "    'entropy': get_entropy(x),\n",
    "    'url_shortened': check_url_shortened(x),\n",
    "    'subdomain_count': get_subdomain_count(x),\n",
    "    'suspicious_tld': get_suspicious_tld(x),\n",
    "    'numeric_ratio': get_numeric_ratio(x),\n",
    "    'domain_length': get_domain_features(x)[0],\n",
    "    'domain_token_count': get_domain_features(x)[1],\n",
    "    'largest_domain_token_length': get_domain_features(x)[2],\n",
    "    'average_domain_token_length': get_domain_features(x)[3],\n",
    "    'word_count': get_word_count(x)\n",
    "}))\n",
    "\n",
    "\n",
    "# Concatenate original DF with features\n",
    "balanced_df = pd.concat([balanced_df, features], axis=1)\n",
    "\n",
    "\n",
    "# Define X and y correctly\n",
    "X = balanced_df.drop(['Label', 'URL'], axis=1)  # Features\n",
    "y = balanced_df['Label']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "# Since your features are already numerical, directly use RandomForestClassifier without TfidfVectorizer\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d20ee435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>url_length</th>\n",
       "      <th>dot_count</th>\n",
       "      <th>hyphen_count_domain</th>\n",
       "      <th>security_sensitive_words</th>\n",
       "      <th>directory_length</th>\n",
       "      <th>sub_directory_count</th>\n",
       "      <th>token_count_path</th>\n",
       "      <th>largest_token_length</th>\n",
       "      <th>...</th>\n",
       "      <th>port_number</th>\n",
       "      <th>subdomain_count</th>\n",
       "      <th>suspicious_tld</th>\n",
       "      <th>numeric_ratio</th>\n",
       "      <th>url_is_internationalized</th>\n",
       "      <th>domain_length</th>\n",
       "      <th>domain_token_count</th>\n",
       "      <th>largest_domain_token_length</th>\n",
       "      <th>average_domain_token_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>etransfers.interac.ca-ssl.net/sh/2o05I9/bdesj/...</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>betterhealthsmoothies.com/Adobe/adobe-3D6/inde...</td>\n",
       "      <td>1</td>\n",
       "      <td>51.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lloydsbank.deregister-payee-secure-auth.com/Lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>53.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.666667</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>archive.md</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pkg00-atx.netgate.com</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>infomation-fb-service.e82443.repl.co</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>img-1000736.ad-score.com</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>sosyalsat.com/help/home.html</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>storageapi.fleek.co/12678f8a-04f9-4b69-a70f-49...</td>\n",
       "      <td>1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>trepievirealestate.com/paintdesk/auth/1/inner_...</td>\n",
       "      <td>1</td>\n",
       "      <td>286.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.493007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      URL  Label  url_length  \\\n",
       "0       etransfers.interac.ca-ssl.net/sh/2o05I9/bdesj/...      1        58.0   \n",
       "1       betterhealthsmoothies.com/Adobe/adobe-3D6/inde...      1        51.0   \n",
       "2       lloydsbank.deregister-payee-secure-auth.com/Lo...      1        53.0   \n",
       "3                                              archive.md      0        10.0   \n",
       "4                                   pkg00-atx.netgate.com      0        21.0   \n",
       "...                                                   ...    ...         ...   \n",
       "299995               infomation-fb-service.e82443.repl.co      1        36.0   \n",
       "299996                           img-1000736.ad-score.com      0        24.0   \n",
       "299997                       sosyalsat.com/help/home.html      1        28.0   \n",
       "299998  storageapi.fleek.co/12678f8a-04f9-4b69-a70f-49...      1        75.0   \n",
       "299999  trepievirealestate.com/paintdesk/auth/1/inner_...      1       286.0   \n",
       "\n",
       "        dot_count  hyphen_count_domain  security_sensitive_words  \\\n",
       "0             4.0                  1.0                       0.0   \n",
       "1             2.0                  0.0                       0.0   \n",
       "2             3.0                  3.0                       1.0   \n",
       "3             1.0                  0.0                       0.0   \n",
       "4             2.0                  1.0                       0.0   \n",
       "...           ...                  ...                       ...   \n",
       "299995        3.0                  2.0                       0.0   \n",
       "299996        2.0                  2.0                       0.0   \n",
       "299997        2.0                  0.0                       0.0   \n",
       "299998        3.0                  0.0                       0.0   \n",
       "299999        2.0                  0.0                       0.0   \n",
       "\n",
       "        directory_length  sub_directory_count  token_count_path  \\\n",
       "0                   29.0                  3.0               4.0   \n",
       "1                   26.0                  2.0               3.0   \n",
       "2                   10.0                  0.0               1.0   \n",
       "3                    0.0                 -1.0               0.0   \n",
       "4                    0.0                 -1.0               0.0   \n",
       "...                  ...                  ...               ...   \n",
       "299995               0.0                 -1.0               0.0   \n",
       "299996               0.0                 -1.0               0.0   \n",
       "299997              15.0                  1.0               2.0   \n",
       "299998              56.0                  1.0               2.0   \n",
       "299999              33.0                  3.0               4.0   \n",
       "\n",
       "        largest_token_length  ...  port_number  subdomain_count  \\\n",
       "0                       12.0  ...         -1.0              2.0   \n",
       "1                        9.0  ...         -1.0              0.0   \n",
       "2                        9.0  ...         -1.0              1.0   \n",
       "3                        0.0  ...         -1.0              0.0   \n",
       "4                        0.0  ...         -1.0              1.0   \n",
       "...                      ...  ...          ...              ...   \n",
       "299995                   0.0  ...         -1.0              2.0   \n",
       "299996                   0.0  ...         -1.0              1.0   \n",
       "299997                   9.0  ...         -1.0              0.0   \n",
       "299998                  43.0  ...         -1.0              1.0   \n",
       "299999                  15.0  ...         -1.0              0.0   \n",
       "\n",
       "        suspicious_tld  numeric_ratio  url_is_internationalized  \\\n",
       "0                  0.0       0.068966                       0.0   \n",
       "1                  0.0       0.039216                       0.0   \n",
       "2                  0.0       0.000000                       0.0   \n",
       "3                  0.0       0.000000                       0.0   \n",
       "4                  0.0       0.095238                       0.0   \n",
       "...                ...            ...                       ...   \n",
       "299995             0.0       0.138889                       0.0   \n",
       "299996             0.0       0.291667                       0.0   \n",
       "299997             0.0       0.000000                       0.0   \n",
       "299998             0.0       0.320000                       0.0   \n",
       "299999             0.0       0.493007                       0.0   \n",
       "\n",
       "        domain_length  domain_token_count  largest_domain_token_length  \\\n",
       "0                29.0                 4.0                         10.0   \n",
       "1                25.0                 2.0                         21.0   \n",
       "2                43.0                 3.0                         28.0   \n",
       "3                10.0                 2.0                          7.0   \n",
       "4                21.0                 3.0                          9.0   \n",
       "...               ...                 ...                          ...   \n",
       "299995           36.0                 4.0                         21.0   \n",
       "299996           24.0                 3.0                         11.0   \n",
       "299997           13.0                 2.0                          9.0   \n",
       "299998           19.0                 3.0                         10.0   \n",
       "299999           22.0                 2.0                         18.0   \n",
       "\n",
       "        average_domain_token_length  word_count  \n",
       "0                          6.500000        10.0  \n",
       "1                         12.000000         7.0  \n",
       "2                         13.666667         8.0  \n",
       "3                          4.500000         2.0  \n",
       "4                          6.333333         4.0  \n",
       "...                             ...         ...  \n",
       "299995                     8.250000         6.0  \n",
       "299996                     7.333333         5.0  \n",
       "299997                     6.000000         5.0  \n",
       "299998                     5.666667        11.0  \n",
       "299999                    10.500000        14.0  \n",
       "\n",
       "[300000 rows x 32 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d995cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with \"entropy\" larger than 0: 55\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'balanced_df' contains your feature \"entropy\"\n",
    "num_rows_entropy_greater_than_zero = (balanced_df['port_number'] > 0).sum()\n",
    "\n",
    "print(f'Number of rows with \"entropy\" larger than 0: {num_rows_entropy_greater_than_zero}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0db0b83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 349s 228ms/step - loss: 0.1978 - acc: 0.9306 - val_loss: 0.1403 - val_acc: 0.9510\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 325s 217ms/step - loss: 0.1309 - acc: 0.9562 - val_loss: 0.1167 - val_acc: 0.9589\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 311s 207ms/step - loss: 0.1130 - acc: 0.9624 - val_loss: 0.1047 - val_acc: 0.9632\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 306s 204ms/step - loss: 0.1114 - acc: 0.9620 - val_loss: 0.1042 - val_acc: 0.9644\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 308s 206ms/step - loss: 0.0940 - acc: 0.9682 - val_loss: 0.0892 - val_acc: 0.9678\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 311s 207ms/step - loss: 0.0873 - acc: 0.9708 - val_loss: 0.0850 - val_acc: 0.9717\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 311s 208ms/step - loss: 0.0826 - acc: 0.9723 - val_loss: 0.0823 - val_acc: 0.9715\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 309s 206ms/step - loss: 0.0793 - acc: 0.9735 - val_loss: 0.0774 - val_acc: 0.9735\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 305s 204ms/step - loss: 0.0771 - acc: 0.9744 - val_loss: 0.0758 - val_acc: 0.9736\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 309s 206ms/step - loss: 0.0746 - acc: 0.9753 - val_loss: 0.0745 - val_acc: 0.9744\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 333s 222ms/step - loss: 0.0719 - acc: 0.9759 - val_loss: 0.0743 - val_acc: 0.9753\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 331s 221ms/step - loss: 0.0700 - acc: 0.9767 - val_loss: 0.0725 - val_acc: 0.9746\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 330s 220ms/step - loss: 0.0676 - acc: 0.9776 - val_loss: 0.0704 - val_acc: 0.9765\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 330s 220ms/step - loss: 0.0656 - acc: 0.9783 - val_loss: 0.0682 - val_acc: 0.9761\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 334s 223ms/step - loss: 0.0636 - acc: 0.9789 - val_loss: 0.0702 - val_acc: 0.9771\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 289s 192ms/step - loss: 0.0614 - acc: 0.9798 - val_loss: 0.0677 - val_acc: 0.9773\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 289s 193ms/step - loss: 0.0596 - acc: 0.9802 - val_loss: 0.0664 - val_acc: 0.9777\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 287s 191ms/step - loss: 0.0578 - acc: 0.9809 - val_loss: 0.0660 - val_acc: 0.9783\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 286s 191ms/step - loss: 0.0555 - acc: 0.9818 - val_loss: 0.0646 - val_acc: 0.9787\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 286s 191ms/step - loss: 0.0537 - acc: 0.9824 - val_loss: 0.0633 - val_acc: 0.9798\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 287s 191ms/step - loss: 0.0516 - acc: 0.9831 - val_loss: 0.0657 - val_acc: 0.9796\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 287s 191ms/step - loss: 0.0501 - acc: 0.9838 - val_loss: 0.0625 - val_acc: 0.9789\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 286s 191ms/step - loss: 0.0486 - acc: 0.9842 - val_loss: 0.0623 - val_acc: 0.9795\n",
      "1875/1875 [==============================] - 38s 20ms/step\n",
      "Accuracy: 0.97925\n",
      "Confusion Matrix:\n",
      " [[29811   251]\n",
      " [  994 28944]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     30062\n",
      "           1       0.99      0.97      0.98     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Assuming balanced_df is your DataFrame with URLs and labels\n",
    "\n",
    "# Tokenization and sequence padding parameters\n",
    "max_len = 100  # Adjust based on the length of the longest URL in your dataset\n",
    "max_words = 60000  # Adjust based on the size of your vocabulary\n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = Tokenizer(num_words=max_words, char_level=True)\n",
    "tokenizer.fit_on_texts(balanced_df['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df['Label'])\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 32, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))  # Add return_sequences if stacking LSTM layers\n",
    "model.add(Dropout(0.5))  # Adjust dropout rate as needed\n",
    "model.add(Bidirectional(LSTM(32)))  # Second LSTM layer, without return_sequences\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))  # Add L2 regularization\n",
    "model.add(Dropout(0.5))  # Adjust dropout rate as needed\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5fdf8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3000/3000 [==============================] - 249s 82ms/step - loss: 0.2540 - accuracy: 0.9023 - val_loss: 0.2597 - val_accuracy: 0.8998\n",
      "Epoch 2/10\n",
      "  72/3000 [..............................] - ETA: 3:38 - loss: 0.2518 - accuracy: 0.9028"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[0;32m     59\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    869\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    870\u001b[0m   )\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, LSTM, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# Tokenization and sequence padding parameters\n",
    "max_length = 200  # Adjust based on the length of the longest URL in your dataset\n",
    "max_words = 60000  # Adjust based on the size of your vocabulary\n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = Tokenizer(num_words=max_words, char_level=True)\n",
    "tokenizer.fit_on_texts(balanced_df['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df['Label'])\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Normalize features (important for neural network models)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Convert labels to categorical if they're not already (for binary classification, this step might be optional)\n",
    "# y_train_categorical = to_categorical(y_train)\n",
    "# y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "# Define the RNN model with improvements\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=32, input_length=max_length))\n",
    "model.add(SimpleRNN(64, return_sequences=True))\n",
    "model.add(Dropout(0.5))  # Add dropout with a 50% rate\n",
    "model.add(BatchNormalization())  # Add batch normalization\n",
    "model.add(SimpleRNN(64, return_sequences=False))  # Another RNN layer, can adjust based on need\n",
    "model.add(Dropout(0.5))  # Add dropout\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Setup early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17a440cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9785833333333334\n",
      "Confusion Matrix:\n",
      " [[29780   282]\n",
      " [ 1003 28935]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     30062\n",
      "           1       0.99      0.97      0.98     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import tldextract \n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import whois\n",
    "import ssl\n",
    "import socket\n",
    "from geolite2 import geolite2\n",
    "\n",
    "\n",
    "\n",
    "def ensure_scheme(url):\n",
    "    if not urlparse(url).scheme:\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "# Feature extraction functions\n",
    "def get_url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "def get_dot_count(url):\n",
    "    return url.count('.')\n",
    "\n",
    "def contains_security_sensitive_words(url):\n",
    "    security_sensitive_words = [\n",
    "    'login', 'password', 'admin', 'root', 'secret', 'private', 'secure', 'confidential', \n",
    "    'bank', 'creditcard', 'account', 'authentication', 'authorization', 'session', 'token', \n",
    "    'apikey', 'ssl', 'https', 'secure', 'encrypted', 'auth', 'signin', 'signup', 'verification', \n",
    "    'resetpassword', 'change-password', 'forgot-password', 'otp', '2fa', 'phishing', 'malware', \n",
    "    'virus', 'trojan', 'exploit', 'hacker', 'attack', 'security', 'vulnerable', 'injection', \n",
    "    'xss', 'csrf', 'dos', 'ddos', 'bruteforce', 'firewall', 'vpn', 'proxy', 'tor', 'security-question', \n",
    "    'privacy-policy'\n",
    "]\n",
    "\n",
    "    return int(any(word in url for word in security_sensitive_words))\n",
    "\n",
    "def get_directory_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return len(path)\n",
    "\n",
    "def get_sub_directory_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return path.count('/') - 1\n",
    "\n",
    "def get_token_count_in_path(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    return len(tokens) - 1\n",
    "\n",
    "def get_largest_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    if tokens:\n",
    "        return max(len(token) for token in tokens)\n",
    "    return 0\n",
    "\n",
    "def get_average_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = [token for token in path.split('/') if token]\n",
    "    if tokens:\n",
    "        return np.mean([len(token) for token in tokens])\n",
    "    return 0\n",
    "\n",
    "def get_file_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return len(filename)\n",
    "\n",
    "def get_dot_count_in_file(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return filename.count('.')\n",
    "\n",
    "def get_delimiter_count_in_file(url):\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    delimiters = ['.', '_', '-']\n",
    "    return sum(filename.count(delimiter) for delimiter in delimiters)\n",
    "\n",
    "def get_arguments_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(query)\n",
    "\n",
    "def get_number_of_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(parse_qs(query))\n",
    "\n",
    "def get_length_of_largest_argument_value(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    if params:\n",
    "        return max(len(max(values, key=len)) for values in params.values())\n",
    "    return 0\n",
    "\n",
    "def get_max_delimiters_in_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    delimiters = ['&', '=', '-', '_']\n",
    "    if params:\n",
    "        return max(sum(value.count(delimiter) for delimiter in delimiters) for values in params.values() for value in values)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_hyphen_count_in_domain(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return domain.count('-')\n",
    "\n",
    "def contains_ip(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    try:\n",
    "        socket.inet_aton(domain)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def get_domain_features(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(ensure_scheme(url)).netloc\n",
    "    tokens = domain.split('.')\n",
    "    \n",
    "    # Domain Length\n",
    "    domain_length = len(domain)\n",
    "    \n",
    "    # Count of Tokens in the Domain\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Length of Largest Token in the Domain\n",
    "    largest_token_length = max(len(token) for token in tokens) if tokens else 0\n",
    "    \n",
    "    # Average Domain Token Length\n",
    "    average_token_length = sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n",
    "    \n",
    "    return domain_length, token_count, largest_token_length, average_token_length\n",
    "\n",
    "# New feature extraction functions\n",
    "def get_special_character_count(url):\n",
    "    special_characters = ['@', '=', '+', '*', '?', '&', '%', '$', '#', '!']\n",
    "    return sum(url.count(char) for char in special_characters)\n",
    "\n",
    "def get_entropy(url):\n",
    "    # Count the frequency of each character in the string\n",
    "    freq = Counter(url)\n",
    "    # Calculate the probabilities\n",
    "    probs = [count / len(url) for count in freq.values()]\n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -sum(p * math.log(p, 2) for p in probs if p > 0)\n",
    "    return entropy\n",
    "\n",
    "def check_url_shortened(url):\n",
    "    shortened_services = ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co']\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return int(domain in shortened_services)\n",
    "\n",
    "def get_subdomain_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain_parts = urlparse(url).netloc.split('.')\n",
    "    # Count as subdomains any parts beyond the second-level domain and TLD\n",
    "    return max(0, len(domain_parts) - 2)\n",
    "\n",
    "def get_suspicious_tld(url):\n",
    "    suspicious_tlds = ['xyz', 'top', 'loan', 'win', 'club']\n",
    "    url = ensure_scheme(url)\n",
    "    tld = urlparse(url).netloc.split('.')[-1]\n",
    "    return int(tld in suspicious_tlds)\n",
    "\n",
    "def get_numeric_ratio(url):\n",
    "    numeric_chars = sum(c.isdigit() for c in url)\n",
    "    return numeric_chars / len(url) if len(url) > 0 else 0\n",
    "\n",
    "def get_word_count(url):\n",
    "    words = re.findall(r'\\w+', url)\n",
    "    return len(words)\n",
    "\n",
    "def is_https(url, timeout=0.5):\n",
    "    return int(url.startswith(\"https\"))\n",
    "\n",
    "\n",
    "# Apply feature extraction\n",
    "features = balanced_df['URL'].apply(lambda x: pd.Series({\n",
    "    'url_length': get_url_length(x),\n",
    "    'dot_count': get_dot_count(x),\n",
    "    'hyphen_count_domain': get_hyphen_count_in_domain(x),\n",
    "    'security_sensitive_words': contains_security_sensitive_words(x),\n",
    "    'directory_length': get_directory_length(x),\n",
    "    'sub_directory_count': get_sub_directory_count(x),\n",
    "    'token_count_path': get_token_count_in_path(x),\n",
    "    'largest_token_length': get_largest_token_length(x),\n",
    "    'average_token_length': get_average_token_length(x),\n",
    "    'file_length': get_file_length(x),\n",
    "    'contains_ip': contains_ip(x),\n",
    "    'dot_count_in_file': get_dot_count_in_file(x),\n",
    "    'delimiter_count_in_file': get_delimiter_count_in_file(x),\n",
    "    'arguments_length': get_arguments_length(x),\n",
    "    'number_of_arguments': get_number_of_arguments(x),\n",
    "    'length_of_largest_argument_value': get_length_of_largest_argument_value(x),\n",
    "    'max_delimiters_in_arguments': get_max_delimiters_in_arguments(x),\n",
    "    'special_character_count': get_special_character_count(x),\n",
    "    'entropy': get_entropy(x),\n",
    "    'url_shortened': check_url_shortened(x),\n",
    "    'subdomain_count': get_subdomain_count(x),\n",
    "    'suspicious_tld': get_suspicious_tld(x),\n",
    "    'numeric_ratio': get_numeric_ratio(x),\n",
    "    'domain_length': get_domain_features(x)[0],\n",
    "    'domain_token_count': get_domain_features(x)[1],\n",
    "    'largest_domain_token_length': get_domain_features(x)[2],\n",
    "    'average_domain_token_length': get_domain_features(x)[3],\n",
    "    'word_count': get_word_count(x),\n",
    "    'is_https': is_https(x)\n",
    "}))\n",
    "\n",
    "\n",
    "# Concatenate original DF with features\n",
    "balanced_df = pd.concat([balanced_df, features], axis=1)\n",
    "\n",
    "\n",
    "# Extracting TF-IDF features from URLs\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to top 5000 features\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(balanced_df['URL'])\n",
    "\n",
    "# Convert TF-IDF features from a sparse matrix to a dense format and then to an np.ndarray\n",
    "tfidf_dense = np.asarray(tfidf_features.todense())\n",
    "\n",
    "# Define X for numerical features\n",
    "X_numerical = balanced_df.drop(['Label', 'URL'], axis=1).values  # Make sure this matches your feature extraction output\n",
    "\n",
    "# Combining TF-IDF features with numerical features\n",
    "X_combined = np.hstack((X_numerical, tfidf_dense))\n",
    "\n",
    "# Define y\n",
    "y = balanced_df['Label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training with RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69f9be1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>url_length</th>\n",
       "      <th>dot_count</th>\n",
       "      <th>hyphen_count_domain</th>\n",
       "      <th>security_sensitive_words</th>\n",
       "      <th>directory_length</th>\n",
       "      <th>sub_directory_count</th>\n",
       "      <th>token_count_path</th>\n",
       "      <th>largest_token_length</th>\n",
       "      <th>...</th>\n",
       "      <th>entropy</th>\n",
       "      <th>url_shortened</th>\n",
       "      <th>subdomain_count</th>\n",
       "      <th>suspicious_tld</th>\n",
       "      <th>numeric_ratio</th>\n",
       "      <th>domain_length</th>\n",
       "      <th>domain_token_count</th>\n",
       "      <th>largest_domain_token_length</th>\n",
       "      <th>average_domain_token_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://etransfers.interac.ca-ssl.net/sh/2o05I9...</td>\n",
       "      <td>1</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.338934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://betterhealthsmoothies.com/Adobe/adobe-3...</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.232897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://lloydsbank.deregister-payee-secure-auth...</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.412048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.666667</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://archive.md</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.836592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://pkg00-atx.netgate.com</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.974890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>https://infomation-fb-service.e82443.repl.co</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.436260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>https://img-1000736.ad-score.com</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.351410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>http://sosyalsat.com/help/home.html</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.671582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>https://storageapi.fleek.co/12678f8a-04f9-4b69...</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.761515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.289157</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>https://trepievirealestate.com/paintdesk/auth/...</td>\n",
       "      <td>1</td>\n",
       "      <td>294.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.465085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479592</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      URL  Label  url_length  \\\n",
       "0       http://etransfers.interac.ca-ssl.net/sh/2o05I9...      1        65.0   \n",
       "1       http://betterhealthsmoothies.com/Adobe/adobe-3...      1        58.0   \n",
       "2       http://lloydsbank.deregister-payee-secure-auth...      1        60.0   \n",
       "3                                      https://archive.md      0        18.0   \n",
       "4                           https://pkg00-atx.netgate.com      0        29.0   \n",
       "...                                                   ...    ...         ...   \n",
       "299995       https://infomation-fb-service.e82443.repl.co      1        44.0   \n",
       "299996                   https://img-1000736.ad-score.com      0        32.0   \n",
       "299997                http://sosyalsat.com/help/home.html      1        35.0   \n",
       "299998  https://storageapi.fleek.co/12678f8a-04f9-4b69...      1        83.0   \n",
       "299999  https://trepievirealestate.com/paintdesk/auth/...      1       294.0   \n",
       "\n",
       "        dot_count  hyphen_count_domain  security_sensitive_words  \\\n",
       "0             4.0                  1.0                       1.0   \n",
       "1             2.0                  0.0                       0.0   \n",
       "2             3.0                  3.0                       1.0   \n",
       "3             1.0                  0.0                       1.0   \n",
       "4             2.0                  1.0                       1.0   \n",
       "...           ...                  ...                       ...   \n",
       "299995        3.0                  2.0                       1.0   \n",
       "299996        2.0                  2.0                       1.0   \n",
       "299997        2.0                  0.0                       0.0   \n",
       "299998        3.0                  0.0                       1.0   \n",
       "299999        2.0                  0.0                       1.0   \n",
       "\n",
       "        directory_length  sub_directory_count  token_count_path  \\\n",
       "0                   29.0                  3.0               4.0   \n",
       "1                   26.0                  2.0               3.0   \n",
       "2                   10.0                  0.0               1.0   \n",
       "3                    0.0                 -1.0               0.0   \n",
       "4                    0.0                 -1.0               0.0   \n",
       "...                  ...                  ...               ...   \n",
       "299995               0.0                 -1.0               0.0   \n",
       "299996               0.0                 -1.0               0.0   \n",
       "299997              15.0                  1.0               2.0   \n",
       "299998              56.0                  1.0               2.0   \n",
       "299999              33.0                  3.0               4.0   \n",
       "\n",
       "        largest_token_length  ...   entropy  url_shortened  subdomain_count  \\\n",
       "0                       12.0  ...  4.338934            0.0              2.0   \n",
       "1                        9.0  ...  4.232897            0.0              0.0   \n",
       "2                        9.0  ...  4.412048            0.0              1.0   \n",
       "3                        0.0  ...  3.836592            0.0              0.0   \n",
       "4                        0.0  ...  3.974890            0.0              1.0   \n",
       "...                      ...  ...       ...            ...              ...   \n",
       "299995                   0.0  ...  4.436260            0.0              2.0   \n",
       "299996                   0.0  ...  4.351410            0.0              1.0   \n",
       "299997                   9.0  ...  3.671582            0.0              0.0   \n",
       "299998                  43.0  ...  4.761515            0.0              1.0   \n",
       "299999                  15.0  ...  4.465085            0.0              0.0   \n",
       "\n",
       "        suspicious_tld  numeric_ratio  domain_length  domain_token_count  \\\n",
       "0                  0.0       0.061538           29.0                 4.0   \n",
       "1                  0.0       0.034483           25.0                 2.0   \n",
       "2                  0.0       0.000000           43.0                 3.0   \n",
       "3                  0.0       0.000000           10.0                 2.0   \n",
       "4                  0.0       0.068966           21.0                 3.0   \n",
       "...                ...            ...            ...                 ...   \n",
       "299995             0.0       0.113636           36.0                 4.0   \n",
       "299996             0.0       0.218750           24.0                 3.0   \n",
       "299997             0.0       0.000000           13.0                 2.0   \n",
       "299998             0.0       0.289157           19.0                 3.0   \n",
       "299999             0.0       0.479592           22.0                 2.0   \n",
       "\n",
       "        largest_domain_token_length  average_domain_token_length  word_count  \n",
       "0                              10.0                     6.500000        11.0  \n",
       "1                              21.0                    12.000000         8.0  \n",
       "2                              28.0                    13.666667         9.0  \n",
       "3                               7.0                     4.500000         3.0  \n",
       "4                               9.0                     6.333333         5.0  \n",
       "...                             ...                          ...         ...  \n",
       "299995                         21.0                     8.250000         7.0  \n",
       "299996                         11.0                     7.333333         6.0  \n",
       "299997                          9.0                     6.000000         6.0  \n",
       "299998                         10.0                     5.666667        12.0  \n",
       "299999                         18.0                    10.500000        15.0  \n",
       "\n",
       "[300000 rows x 30 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b442f190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
