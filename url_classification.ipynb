{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac17c243",
   "metadata": {},
   "source": [
    "# URL Classification model using Machine Learning\n",
    "### For Computer Science B.Sc. Ariel University\n",
    "\n",
    "**By Yuval Yurzdichinsky, Matan Aviv and Roy Simanovich**\n",
    "\n",
    "## Introduction\n",
    "In this project we will build a machine learning model that will classify URLs based on if they are malicious (phishing, malware, etc.) or benign. We will use a dataset of URLs that are labeled as either malicious or benign and use it to train a model that will be able to classify new URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242fd86",
   "metadata": {},
   "source": [
    "### Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ae9a164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gensim in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting python-whois\n",
      "  Downloading python-whois-0.8.0.tar.gz (109 kB)\n",
      "     ---------------------------------------- 0.0/109.6 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/109.6 kB ? eta -:--:--\n",
      "     ---------- -------------------------- 30.7/109.6 kB 262.6 kB/s eta 0:00:01\n",
      "     -------------------- ---------------- 61.4/109.6 kB 409.6 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 102.4/109.6 kB 590.8 kB/s eta 0:00:01\n",
      "     ------------------------------------ 109.6/109.6 kB 528.8 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting future (from python-whois)\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "     ---------------------------------------- 0.0/840.9 kB ? eta -:--:--\n",
      "     ----- ---------------------------------- 112.6/840.9 kB ? eta -:--:--\n",
      "     ---------------- --------------------- 358.4/840.9 kB 4.5 MB/s eta 0:00:01\n",
      "     -------------------------- ----------- 593.9/840.9 kB 5.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 840.9/840.9 kB 5.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: python-whois, future\n",
      "  Building wheel for python-whois (setup.py): started\n",
      "  Building wheel for python-whois (setup.py): finished with status 'done'\n",
      "  Created wheel for python-whois: filename=python_whois-0.8.0-py3-none-any.whl size=103273 sha256=9439ac940e674d6af5a38438e001d080ad3e923d2d2f6630d73cb15ddad351ff\n",
      "  Stored in directory: c:\\users\\siman\\appdata\\local\\pip\\cache\\wheels\\e6\\e9\\d3\\1e41a6c95b398de12c5a332ff28805aa44e68aa317ea60266d\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492054 sha256=a0a933b4de941df98ee80629f2e1c6c1ba44be940afbfaa2aab4b37d1731c764\n",
      "  Stored in directory: c:\\users\\siman\\appdata\\local\\pip\\cache\\wheels\\bf\\5d\\6a\\2e53874f7ec4e2bede522385439531fafec8fafe005b5c3d1b\n",
      "Successfully built python-whois future\n",
      "Installing collected packages: future, python-whois\n",
      "Successfully installed future-0.18.3 python-whois-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tldextract\n",
      "  Downloading tldextract-5.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tldextract) (3.6)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tldextract) (2.31.0)\n",
      "Collecting requests-file>=1.4 (from tldextract)\n",
      "  Downloading requests_file-2.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting filelock>=3.0.8 (from tldextract)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from requests>=2.1.0->tldextract) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from requests>=2.1.0->tldextract) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from requests>=2.1.0->tldextract) (2024.2.2)\n",
      "Downloading tldextract-5.1.1-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.7 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/97.7 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 41.0/97.7 kB 653.6 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 92.2/97.7 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 97.7/97.7 kB 794.9 kB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Installing collected packages: filelock, requests-file, tldextract\n",
      "Successfully installed filelock-3.13.1 requests-file-2.0.0 tldextract-5.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Some importent libraries installtion via pip\n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install gensim nltk\n",
    "%pip install matplotlib\n",
    "%pip install python-whois\n",
    "%pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4fd24ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siman\\AppData\\Local\\Temp\\ipykernel_6732\\1236473410.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "## Regular Libraries that's come with python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import socket as sock\n",
    "import math as m\n",
    "import re\n",
    "import whois as who\n",
    "import tldextract as tld\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "## URL Parsing\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "## SKLearn Libraries for Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "## SKLean metrics for model evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "## Tensorflow Libraries for Deep Learning\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83fb0b",
   "metadata": {},
   "source": [
    "### Feature extraction functions\n",
    "We will use the following feature extraction functions to extract features from the URLs:\n",
    "1. `get_url_length`: This function returns the length of the URL.\n",
    "2. `get_dot_count`: This function returns the number of dots in the URL.\n",
    "3. `get_hyphen_count_in_domain`: This function returns the number of hyphens in the domain of the URL.\n",
    "4. `contains_security_sensitive_words`: This function returns 1 if the URL contains security sensitive words (e.g., \"login\", \"signin\", \"account\", etc.) and 0 otherwise.\n",
    "5. `get_directory_length`: This function returns the length of the directory part of the URL.\n",
    "6. `get_sub_directory_count`: This function returns the number of subdirectories in the URL.\n",
    "7. `contains_ip`: This function returns 1 if the URL contains an IP address and 0 otherwise.\n",
    "8. `get_token_count_in_path`: This function returns the number of tokens in the path part of the URL.\n",
    "9. `get_largest_token_length`: This function returns the length of the largest token in the path part of the URL.\n",
    "10. `get_avarage_token_length`: This function returns the average length of the tokens in the path part of the URL.\n",
    "11. `get_file_length`: This function returns the length of the file part of the URL.\n",
    "12. `get_dot_count_in_file`: This function returns the number of dots in the file part of the URL.\n",
    "13. `get_delimiter_count_in_file`: This function returns the number of delimiters in the file part of the URL.\n",
    "14. `get_arguments_length`: This function returns the length of the arguments part of the URL.\n",
    "15. `get_number_of_arguments`: This function returns the number of arguments in the URL.\n",
    "16. `get_length_of_largest_argument_value`: This function returns the length of the largest argument value in the URL.\n",
    "17. `get_max_delimiters_in_arguments`: This function returns the maximum number of delimiters in the arguments part of the URL.\n",
    "18. `get_domain_features`: This function returns the following features of the domain part of the URL: length, number of dots, number of hyphens, number of digits, number of special characters, entropy, whether the domain is a subdomain, and whether the domain is an IP address.\n",
    "19. `get_special_character_count`: This function returns the number of special characters in the URL.\n",
    "20. `get_entropy`: This function returns the entropy of the URL.\n",
    "21. `check_url_shortened`: This function returns 1 if the URL is shortened and 0 otherwise.\n",
    "22. `get_port_number`: This function returns the port number of the URL, if it exists, and -1 otherwise.\n",
    "23. `get_subdomain_count`: This function returns the number of subdomains in the URL.\n",
    "24. `get_suspicious_tld`: This function returns 1 if the top-level domain (TLD) of the URL is suspicious (e.g., \"tk\", \"ml\", \"ga\", etc.) and 0 otherwise.\n",
    "25. `get_numeric_ratio`: This function returns the ratio of numeric characters in the URL.\n",
    "26. `get_word_count`: This function returns the number of words in the URL.\n",
    "27. `get_url_is_internationalized`: This function returns 1 if the URL is internationalized and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86207970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to ensure that the URL has a scheme (http:// or https://)\n",
    "def ensure_scheme(url):\n",
    "    if not urlparse(url).scheme:\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "# Feature extraction functions\n",
    "\n",
    "## Getting the length of the URL\n",
    "def get_url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "## Getting the dot count in the URL\n",
    "def get_dot_count(url):\n",
    "    return url.count('.')\n",
    "\n",
    "## Check if the URL contains security sensitive words\n",
    "def contains_security_sensitive_words(url):\n",
    "    security_sensitive_words = ['login', 'signin', 'auth', 'bank', 'update', 'account', 'verification', 'authenticate','authentication','verify','user']\n",
    "    return int(any(word in url for word in security_sensitive_words))\n",
    "\n",
    "## Get the directory length\n",
    "def get_directory_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return len(path)\n",
    "\n",
    "## Get the subdirectory count\n",
    "def get_sub_directory_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return path.count('/') - 1\n",
    "\n",
    "## Get the token count in the path\n",
    "def get_token_count_in_path(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    return len(tokens) - 1\n",
    "\n",
    "## Get the length of the largest token in the path\n",
    "def get_largest_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    if tokens:\n",
    "        return max(len(token) for token in tokens)\n",
    "    return 0\n",
    "\n",
    "## Get the average token length in the path\n",
    "def get_average_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = [token for token in path.split('/') if token]\n",
    "    if tokens:\n",
    "        return np.mean([len(token) for token in tokens])\n",
    "    return 0\n",
    "\n",
    "## Get the file length\n",
    "def get_file_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return len(filename)\n",
    "\n",
    "## Get the dot count in the file\n",
    "def get_dot_count_in_file(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return filename.count('.')\n",
    "\n",
    "## Get the delimiter count in the file\n",
    "def get_delimiter_count_in_file(url):\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    delimiters = ['.', '_', '-']\n",
    "    return sum(filename.count(delimiter) for delimiter in delimiters)\n",
    "\n",
    "## Get the arguments length\n",
    "def get_arguments_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(query)\n",
    "\n",
    "## Get the number of arguments\n",
    "def get_number_of_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(parse_qs(query))\n",
    "\n",
    "## Get the length of the largest argument value\n",
    "def get_length_of_largest_argument_value(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    if params:\n",
    "        return max(len(max(values, key=len)) for values in params.values())\n",
    "    return 0\n",
    "\n",
    "## Get the average argument value length\n",
    "def get_max_delimiters_in_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    delimiters = ['&', '=', '-', '_']\n",
    "    if params:\n",
    "        return max(sum(value.count(delimiter) for delimiter in delimiters) for values in params.values() for value in values)\n",
    "    return 0\n",
    "\n",
    "## Get the hyphen count in the domain\n",
    "def get_hyphen_count_in_domain(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return domain.count('-')\n",
    "\n",
    "## Get the digit count in the domain\n",
    "def contains_ip(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    try:\n",
    "        sock.inet_aton(domain)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "## Get the domain length, token count, largest token length, and average token length\n",
    "def get_domain_features(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(ensure_scheme(url)).netloc\n",
    "    tokens = domain.split('.')\n",
    "    \n",
    "    # Domain Length\n",
    "    domain_length = len(domain)\n",
    "    \n",
    "    # Count of Tokens in the Domain\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Length of Largest Token in the Domain\n",
    "    largest_token_length = max(len(token) for token in tokens) if tokens else 0\n",
    "    \n",
    "    # Average Domain Token Length\n",
    "    average_token_length = sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n",
    "    \n",
    "    return domain_length, token_count, largest_token_length, average_token_length\n",
    "\n",
    "## Check if the URL contains special characters\n",
    "def get_special_character_count(url):\n",
    "    special_characters = ['@', '=', '+', '*', '?', '&', '%', '$', '#', '!']\n",
    "    return sum(url.count(char) for char in special_characters)\n",
    "\n",
    "## Get the entropy of the URL\n",
    "def get_entropy(url):\n",
    "    # Count the frequency of each character in the string\n",
    "    freq = Counter(url)\n",
    "    # Calculate the probabilities\n",
    "    probs = [count / len(url) for count in freq.values()]\n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -sum(p * m.log(p, 2) for p in probs if p > 0)\n",
    "    return entropy\n",
    "\n",
    "## Check if the URL is shortened\n",
    "def check_url_shortened(url):\n",
    "    shortened_services = ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co']\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return int(domain in shortened_services)\n",
    "\n",
    "## Get the port number, if specified\n",
    "def get_port_number(url):\n",
    "    url = ensure_scheme(url)\n",
    "    port = urlparse(url).port\n",
    "    return port if port else -1  # Return -1 if no port specified\n",
    "\n",
    "## Get the subdomain count\n",
    "def get_subdomain_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain_parts = urlparse(url).netloc.split('.')\n",
    "    # Count as subdomains any parts beyond the second-level domain and TLD\n",
    "    return max(0, len(domain_parts) - 2)\n",
    "\n",
    "## Check if the URL TLD is suspicious (e.g., .xyz, .top, .loan, .win, .club)\n",
    "def get_suspicious_tld(url):\n",
    "    suspicious_tlds = ['xyz', 'top', 'loan', 'win', 'club']\n",
    "    url = ensure_scheme(url)\n",
    "    tld = urlparse(url).netloc.split('.')[-1]\n",
    "    return int(tld in suspicious_tlds)\n",
    "\n",
    "## Get the ratio of numeric characters in the URL\n",
    "def get_numeric_ratio(url):\n",
    "    numeric_chars = sum(c.isdigit() for c in url)\n",
    "    return numeric_chars / len(url) if len(url) > 0 else 0\n",
    "\n",
    "## Get the word count in the URL\n",
    "def get_word_count(url):\n",
    "    words = re.findall(r'\\w+', url)\n",
    "    return len(words)\n",
    "\n",
    "## Check if the URL is internationalized (i.e., contains non-ASCII characters)\n",
    "def get_url_is_internationalized(url):\n",
    "    try:\n",
    "        url.encode('ascii')\n",
    "        return 0\n",
    "    except UnicodeEncodeError:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29f9c7",
   "metadata": {},
   "source": [
    "### Now load the dataset and do some preprocessing on it\n",
    "The dataset is a CSV file with two columns: `url` and `label`. The `url` column contains the URL and the `label` column contains the label of the URL. The label is 1 if the URL is malicious and 0 if the URL is benign. Since the dataset is huge (more than 1 million rows), we will only use a small subset of it for this project (150,000 rows for each class, 300,000 rows in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "009c0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('url_dataset_updated.csv')\n",
    "\n",
    "# Column name for the URL\n",
    "url_column_name = 'URL'\n",
    "\n",
    "# Remove 'http://' and 'https://' from all URLs\n",
    "df[url_column_name] = df[url_column_name].str.replace('http://', '', regex=False)\n",
    "df[url_column_name] = df[url_column_name].str.replace('https://', '', regex=False)\n",
    "\n",
    "# Separate the dataset into malicious and benign\n",
    "malicious_df = df[df['Label'] == 1]\n",
    "benign_df = df[df['Label'] == 0]\n",
    "\n",
    "# Randomly sample 150,000 entries from each\n",
    "malicious_sampled_df = resample(malicious_df, n_samples=150000, random_state=42)\n",
    "benign_sampled_df = resample(benign_df, n_samples=150000, random_state=42)\n",
    "\n",
    "# Combine the sampled data\n",
    "balanced_df = pd.concat([malicious_sampled_df, benign_sampled_df])\n",
    "\n",
    "# Shuffle the combined dataset to mix malicious and benign URLs\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Drop all duplicates from balanced_df\n",
    "balanced_df = balanced_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after dropping duplicates\n",
    "balanced_df = balanced_df.reset_index(drop=True)\n",
    "\n",
    "# Now, balanced_df contains the balanced dataset ready for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d87432",
   "metadata": {},
   "source": [
    "## Start testing with the options\n",
    "**NOTE:** We'll split the truncated dataset into a training set and a test set (80% training, 20% test) and use the training set to train the model and the test set to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ce677",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer with random forest classifier, no feature extraction\n",
    "We will start by using the TF-IDF vectorizer to convert the URLs into numerical features and then use a random forest classifier to classify the URLs. We will not use any feature extraction functions for now. This is for reference only, as we will use feature extraction functions and better methods later.\n",
    "* **Warning:** This will take a long time to run, as the dataset is huge, and TF-IDF vectorization is a slow process. Don't run this unless you have a powerful computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec8b3606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9046580631323061\n",
      "Confusion Matrix:\n",
      " [[27659   514]\n",
      " [ 4820 22953]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91     28173\n",
      "           1       0.98      0.83      0.90     27773\n",
      "\n",
      "    accuracy                           0.90     55946\n",
      "   macro avg       0.91      0.90      0.90     55946\n",
      "weighted avg       0.91      0.90      0.90     55946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For simplicity, we'll use TF-IDF on the URLs themselves. Advanced features can be added based on URL structure and content.\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Prepare the data\n",
    "X = balanced_df['URL']\n",
    "y = balanced_df['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "# Create a pipeline that first transforms the data using TfidfVectorizer then applies RandomForestClassifier\n",
    "model = make_pipeline(TfidfVectorizer(), RandomForestClassifier(n_estimators=20, random_state=42))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5a356",
   "metadata": {},
   "source": [
    "### Random forest classifier, with feature extraction\n",
    "Next thing we will do is use the feature extraction functions to extract features from the URLs and then use a random forest classifier to classify the URLs. We will use the same training and test sets as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06fa5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9091266578486398\n",
      "Confusion Matrix:\n",
      " [[26480  1693]\n",
      " [ 3391 24382]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91     28173\n",
      "           1       0.94      0.88      0.91     27773\n",
      "\n",
      "    accuracy                           0.91     55946\n",
      "   macro avg       0.91      0.91      0.91     55946\n",
      "weighted avg       0.91      0.91      0.91     55946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply feature extraction\n",
    "features = balanced_df['URL'].apply(lambda x: pd.Series({\n",
    "    'url_length': get_url_length(x),\n",
    "    'dot_count': get_dot_count(x),\n",
    "    'hyphen_count_domain': get_hyphen_count_in_domain(x),\n",
    "    'security_sensitive_words': contains_security_sensitive_words(x),\n",
    "    'directory_length': get_directory_length(x),\n",
    "    'sub_directory_count': get_sub_directory_count(x),\n",
    "    'token_count_path': get_token_count_in_path(x),\n",
    "    'largest_token_length': get_largest_token_length(x),\n",
    "    'average_token_length': get_average_token_length(x),\n",
    "    'file_length': get_file_length(x),\n",
    "    'contains_ip': contains_ip(x),\n",
    "    'dot_count_in_file': get_dot_count_in_file(x),\n",
    "    'delimiter_count_in_file': get_delimiter_count_in_file(x),\n",
    "    'arguments_length': get_arguments_length(x),\n",
    "    'number_of_arguments': get_number_of_arguments(x),\n",
    "    'length_of_largest_argument_value': get_length_of_largest_argument_value(x),\n",
    "    'max_delimiters_in_arguments': get_max_delimiters_in_arguments(x),\n",
    "    'special_character_count': get_special_character_count(x),\n",
    "    'entropy': get_entropy(x),\n",
    "    'url_shortened': check_url_shortened(x),\n",
    "    'port_number': get_port_number(x),\n",
    "    'subdomain_count': get_subdomain_count(x),\n",
    "    'suspicious_tld': get_suspicious_tld(x),\n",
    "    'numeric_ratio': get_numeric_ratio(x),\n",
    "    'url_is_internationalized': get_url_is_internationalized(x),\n",
    "    'domain_length': get_domain_features(x)[0],\n",
    "    'domain_token_count': get_domain_features(x)[1],\n",
    "    'largest_domain_token_length': get_domain_features(x)[2],\n",
    "    'average_domain_token_length': get_domain_features(x)[3],\n",
    "    'word_count': get_word_count(x)\n",
    "}))\n",
    "\n",
    "\n",
    "# Concatenate original DF with features\n",
    "balanced_df_with_features = pd.concat([balanced_df, features], axis=1)\n",
    "\n",
    "# Define X and y\n",
    "X = balanced_df_with_features.drop(['Label', 'URL'], axis=1)\n",
    "y = balanced_df_with_features['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "# Since your features are already numerical, directly use RandomForestClassifier without TfidfVectorizer\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bcad1",
   "metadata": {},
   "source": [
    "## TensorFlow neural network, with feature extraction\n",
    "Now let's use a neural network to classify the URLs. We will use the feature extraction functions to extract features from the URLs and then use a neural network to classify the URLs. We will use the same training and test sets as before. As the same, the training set is 80% and the test set is 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a08f25",
   "metadata": {},
   "source": [
    "### Preprocessing for Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6492a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and sequence padding parameters\n",
    "max_len = 100  # Adjust based on the length of the longest URL in your dataset\n",
    "max_words = 60000  # Adjust based on the size of your vocabulary\n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(balanced_df_with_features['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df_with_features['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df_with_features['Label'])\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7517f2",
   "metadata": {},
   "source": [
    "### Using RNN model with feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0db0b83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1399/1399 [==============================] - 56s 40ms/step - loss: 0.1959 - acc: 0.9212 - val_loss: 0.1530 - val_acc: 0.9376\n",
      "Epoch 2/10\n",
      "1399/1399 [==============================] - 56s 40ms/step - loss: 0.1409 - acc: 0.9423 - val_loss: 0.1434 - val_acc: 0.9423\n",
      "Epoch 3/10\n",
      "1399/1399 [==============================] - 62s 44ms/step - loss: 0.1273 - acc: 0.9486 - val_loss: 0.1393 - val_acc: 0.9447\n",
      "Epoch 4/10\n",
      "1399/1399 [==============================] - 60s 43ms/step - loss: 0.1303 - acc: 0.9485 - val_loss: 0.1339 - val_acc: 0.9466\n",
      "Epoch 5/10\n",
      "1399/1399 [==============================] - 61s 43ms/step - loss: 0.1151 - acc: 0.9547 - val_loss: 0.1299 - val_acc: 0.9481\n",
      "Epoch 6/10\n",
      "1399/1399 [==============================] - 62s 45ms/step - loss: 0.1122 - acc: 0.9559 - val_loss: 0.1280 - val_acc: 0.9489\n",
      "Epoch 7/10\n",
      "1399/1399 [==============================] - 60s 43ms/step - loss: 0.1060 - acc: 0.9583 - val_loss: 0.1272 - val_acc: 0.9498\n",
      "Epoch 8/10\n",
      "1399/1399 [==============================] - 64s 46ms/step - loss: 0.1030 - acc: 0.9593 - val_loss: 0.1254 - val_acc: 0.9502\n",
      "Epoch 9/10\n",
      "1399/1399 [==============================] - 63s 45ms/step - loss: 0.1032 - acc: 0.9595 - val_loss: 0.1261 - val_acc: 0.9496\n",
      "Epoch 10/10\n",
      "1399/1399 [==============================] - 63s 45ms/step - loss: 0.1012 - acc: 0.9606 - val_loss: 0.1254 - val_acc: 0.9508\n",
      "1749/1749 [==============================] - 28s 16ms/step\n",
      "Accuracy: 0.9522217852929611\n",
      "Confusion Matrix:\n",
      " [[27459   714]\n",
      " [ 1959 25814]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95     28173\n",
      "           1       0.97      0.93      0.95     27773\n",
      "\n",
      "    accuracy                           0.95     55946\n",
      "   macro avg       0.95      0.95      0.95     55946\n",
      "weighted avg       0.95      0.95      0.95     55946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RNN model definition\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_words, 32, input_length=max_len))\n",
    "model.add(tf.keras.layers.SimpleRNN(32))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping, using the CPU since the model is too simple for GPU acceleration\n",
    "with tf.device('/device:CPU:0'):\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded79e41",
   "metadata": {},
   "source": [
    "#### Optimize the TensorFlow neural network using RNN and Adam optimizer\n",
    "Now let's optimize the neural network using RNN and the Adam optimizer. We will use the feature extraction functions to extract features from the URLs and then use a neural network with RNN and the Adam optimizer to classify the URLs. We will use the same training and test sets as before. Since the model now is much more complex, we'll use TensorFlow GPU acceleration to speed up the training process, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5fdf8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on GPU\n",
      "\n",
      "Epoch 1/30\n",
      "1399/1399 [==============================] - 93s 61ms/step - loss: 0.2421 - acc: 0.9068 - val_loss: 0.1949 - val_acc: 0.9229\n",
      "Epoch 2/30\n",
      "1399/1399 [==============================] - 100s 71ms/step - loss: 0.1843 - acc: 0.9295 - val_loss: 0.1682 - val_acc: 0.9333\n",
      "Epoch 3/30\n",
      "1399/1399 [==============================] - 100s 71ms/step - loss: 0.1589 - acc: 0.9401 - val_loss: 0.1533 - val_acc: 0.9418\n",
      "Epoch 4/30\n",
      "1399/1399 [==============================] - 107s 76ms/step - loss: 0.1472 - acc: 0.9450 - val_loss: 0.1461 - val_acc: 0.9445\n",
      "Epoch 5/30\n",
      "1399/1399 [==============================] - 101s 72ms/step - loss: 0.1397 - acc: 0.9480 - val_loss: 0.1406 - val_acc: 0.9463\n",
      "Epoch 6/30\n",
      "1399/1399 [==============================] - 96s 69ms/step - loss: 0.1348 - acc: 0.9499 - val_loss: 0.1336 - val_acc: 0.9495\n",
      "Epoch 7/30\n",
      "1399/1399 [==============================] - 94s 67ms/step - loss: 0.1302 - acc: 0.9514 - val_loss: 0.1330 - val_acc: 0.9496\n",
      "Epoch 8/30\n",
      "1399/1399 [==============================] - 101s 72ms/step - loss: 0.1262 - acc: 0.9532 - val_loss: 0.1288 - val_acc: 0.9516\n",
      "Epoch 9/30\n",
      "1399/1399 [==============================] - 97s 69ms/step - loss: 0.1220 - acc: 0.9543 - val_loss: 0.1267 - val_acc: 0.9522\n",
      "Epoch 10/30\n",
      "1399/1399 [==============================] - 101s 72ms/step - loss: 0.1186 - acc: 0.9563 - val_loss: 0.1258 - val_acc: 0.9529\n",
      "Epoch 11/30\n",
      "1399/1399 [==============================] - 96s 69ms/step - loss: 0.1146 - acc: 0.9574 - val_loss: 0.1215 - val_acc: 0.9539\n",
      "Epoch 12/30\n",
      "1399/1399 [==============================] - 100s 72ms/step - loss: 0.1112 - acc: 0.9586 - val_loss: 0.1202 - val_acc: 0.9554\n",
      "Epoch 13/30\n",
      "1399/1399 [==============================] - 108s 77ms/step - loss: 0.1078 - acc: 0.9598 - val_loss: 0.1181 - val_acc: 0.9561\n",
      "Epoch 14/30\n",
      "1399/1399 [==============================] - 117s 84ms/step - loss: 0.1044 - acc: 0.9612 - val_loss: 0.1168 - val_acc: 0.9584\n",
      "Epoch 15/30\n",
      "1399/1399 [==============================] - 112s 80ms/step - loss: 0.1010 - acc: 0.9629 - val_loss: 0.1141 - val_acc: 0.9580\n",
      "Epoch 16/30\n",
      " 240/1399 [====>.........................] - ETA: 1:14 - loss: 0.0958 - acc: 0.9636"
     ]
    }
   ],
   "source": [
    "# Tokenization and sequence padding parameters\n",
    "max_len = 100  # Adjust based on the length of the longest URL in your dataset\n",
    "max_words = 60000  # Adjust based on the size of your vocabulary\n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words, char_level=True)\n",
    "tokenizer.fit_on_texts(balanced_df['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df['Label'])\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Model definition\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_words, 32, input_length=max_len))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))) # Add return_sequences if stacking LSTM layers\n",
    "model.add(tf.keras.layers.Dropout(0.5))  # Adjust dropout rate as needed\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)))  # Second LSTM layer, without return_sequences\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))  # Add L2 regularization\n",
    "model.add(tf.keras.layers.Dropout(0.5))  # Adjust dropout rate as needed\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "try:\n",
    "    with tf.device('/device:GPU:0'): # Use GPU for training, as the model is more complex and very slow to train on CPU\n",
    "        print(\"Training the model on GPU\\n\")\n",
    "        history = model.fit(X_train, y_train, epochs=30, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# If you don't have a GPU, you can train the model on the CPU\n",
    "except Exception as e:\n",
    "    print(\"Could not train the model on GPU. Training on CPU.\\n\")\n",
    "    history = model.fit(X_train, y_train, epochs=30, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "825f16a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save(\"RNN_and_Adam_trained_model.h5\")\n",
    "\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aca729",
   "metadata": {},
   "source": [
    "#### Another optimization for the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f07cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and sequence padding parameters\n",
    "max_len = 100  # Adjust based on the length of the longest URL in your dataset\n",
    "max_words = 60000  # Adjust based on the size of your vocabulary\n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words, char_level=True)\n",
    "tokenizer.fit_on_texts(balanced_df['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df['Label'])\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# RNN model definition\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_words, 32, input_length=max_len)) # Embedding layer\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2))) # Bidirectional LSTM layer\n",
    "\n",
    "# Multi-layer dense network\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "# Dropout layer\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# Use Adam optimizer with a lower learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "try:\n",
    "    # Use GPU for training, as the model is more complex and very slow to train on CPU\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        print(\"Training the model on GPU\\n\")\n",
    "        history = model.fit(X_train, y_train, epochs=30, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# If you don't have a GPU, you can train the model on the CPU\n",
    "except Exception as e:\n",
    "    print(\"Could not train the model on GPU. Training on CPU.\\n\")\n",
    "    history = model.fit(X_train, y_train, epochs=30, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
