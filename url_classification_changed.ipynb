{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac17c243",
   "metadata": {},
   "source": [
    "# URL Classification model using Machine Learning\n",
    "### For Computer Science B.Sc. Ariel University\n",
    "\n",
    "**By Yuval Yurzdichinsky, Matan Aviv and Roy Simanovich**\n",
    "\n",
    "## Introduction\n",
    "In this project we will build a machine learning model that will classify URLs based on if they are malicious (phishing, malware, etc.) or benign. We will use a dataset of URLs that are labeled as either malicious or benign and use it to train a model that will be able to classify new URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242fd86",
   "metadata": {},
   "source": [
    "### Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae9a164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gensim in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from matplotlib) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-whois in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: future in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from python-whois) (0.18.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tldextract in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: idna in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tldextract) (3.6)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tldextract) (2.31.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tldextract) (2.0.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tldextract) (3.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from requests>=2.1.0->tldextract) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from requests>=2.1.0->tldextract) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from requests>=2.1.0->tldextract) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting chardet\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/199.4 kB ? eta -:--:--\n",
      "   -------- ------------------------------ 41.0/199.4 kB 495.5 kB/s eta 0:00:01\n",
      "   ------------------ -------------------- 92.2/199.4 kB 880.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  194.6/199.4 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 199.4/199.4 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: chardet\n",
      "Successfully installed chardet-5.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Some importent libraries installtion via pip\n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install gensim nltk\n",
    "%pip install matplotlib\n",
    "%pip install python-whois\n",
    "%pip install tldextract\n",
    "%pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4fd24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "## Regular Libraries that's come with python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import socket as sock\n",
    "import math as m\n",
    "import re\n",
    "import whois as who\n",
    "import tldextract as tld\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "## URL Parsing\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "## SKLearn Libraries for Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "## SKLean metrics for model evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "## Tensorflow Libraries for Deep Learning\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83fb0b",
   "metadata": {},
   "source": [
    "### Feature extraction functions\n",
    "We will use the following feature extraction functions to extract features from the URLs:\n",
    "1. `get_url_length`: This function returns the length of the URL.\n",
    "2. `get_dot_count`: This function returns the number of dots in the URL.\n",
    "3. `get_hyphen_count_in_domain`: This function returns the number of hyphens in the domain of the URL.\n",
    "4. `contains_security_sensitive_words`: This function returns 1 if the URL contains security sensitive words (e.g., \"login\", \"signin\", \"account\", etc.) and 0 otherwise.\n",
    "5. `get_directory_length`: This function returns the length of the directory part of the URL.\n",
    "6. `get_sub_directory_count`: This function returns the number of subdirectories in the URL.\n",
    "7. `contains_ip`: This function returns 1 if the URL contains an IP address and 0 otherwise.\n",
    "8. `get_token_count_in_path`: This function returns the number of tokens in the path part of the URL.\n",
    "9. `get_largest_token_length`: This function returns the length of the largest token in the path part of the URL.\n",
    "10. `get_avarage_token_length`: This function returns the average length of the tokens in the path part of the URL.\n",
    "11. `get_file_length`: This function returns the length of the file part of the URL.\n",
    "12. `get_dot_count_in_file`: This function returns the number of dots in the file part of the URL.\n",
    "13. `get_delimiter_count_in_file`: This function returns the number of delimiters in the file part of the URL.\n",
    "14. `get_arguments_length`: This function returns the length of the arguments part of the URL.\n",
    "15. `get_number_of_arguments`: This function returns the number of arguments in the URL.\n",
    "16. `get_length_of_largest_argument_value`: This function returns the length of the largest argument value in the URL.\n",
    "17. `get_max_delimiters_in_arguments`: This function returns the maximum number of delimiters in the arguments part of the URL.\n",
    "18. `get_domain_features`: This function returns the following features of the domain part of the URL: length, number of dots, number of hyphens, number of digits, number of special characters, entropy, whether the domain is a subdomain, and whether the domain is an IP address.\n",
    "19. `get_special_character_count`: This function returns the number of special characters in the URL.\n",
    "20. `get_entropy`: This function returns the entropy of the URL.\n",
    "21. `check_url_shortened`: This function returns 1 if the URL is shortened and 0 otherwise.\n",
    "22. `get_port_number`: This function returns the port number of the URL, if it exists, and -1 otherwise.\n",
    "23. `get_subdomain_count`: This function returns the number of subdomains in the URL.\n",
    "24. `get_suspicious_tld`: This function returns 1 if the top-level domain (TLD) of the URL is suspicious (e.g., \"tk\", \"ml\", \"ga\", etc.) and 0 otherwise.\n",
    "25. `get_numeric_ratio`: This function returns the ratio of numeric characters in the URL.\n",
    "26. `get_word_count`: This function returns the number of words in the URL.\n",
    "27. `get_url_is_internationalized`: This function returns 1 if the URL is internationalized and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86207970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to ensure that the URL has a scheme (http:// or https://)\n",
    "def ensure_scheme(url):\n",
    "    if not urlparse(url).scheme:\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "# Function to concatenate \"https://\" to URLs labeled with 0\n",
    "def add_https(url, label):\n",
    "    if label == 0:\n",
    "        return \"https://\" + url\n",
    "    else:\n",
    "        return url\n",
    "    \n",
    "# Evaluation function for the model\n",
    "def model_evaluation(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature extraction functions\n",
    "\n",
    "## Getting the length of the URL\n",
    "def get_url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "## Getting the dot count in the URL\n",
    "def get_dot_count(url):\n",
    "    return url.count('.')\n",
    "\n",
    "## Check if the URL contains security sensitive words\n",
    "def contains_security_sensitive_words(url):\n",
    "    security_sensitive_words = [\n",
    "    'login', 'password', 'admin', 'root', 'secret', 'private', 'secure', 'confidential', \n",
    "    'bank', 'creditcard', 'account', 'authentication', 'authorization', 'session', 'token', \n",
    "    'apikey', 'ssl', 'https', 'secure', 'encrypted', 'auth', 'signin', 'signup', 'verification', \n",
    "    'resetpassword', 'change-password', 'forgot-password', 'otp', '2fa', 'phishing', 'malware', \n",
    "    'virus', 'trojan', 'exploit', 'hacker', 'attack', 'security', 'vulnerable', 'injection', \n",
    "    'xss', 'csrf', 'dos', 'ddos', 'bruteforce', 'firewall', 'vpn', 'proxy', 'tor', 'security-question', \n",
    "    'privacy-policy'\n",
    "]\n",
    "\n",
    "    return int(any(word in url for word in security_sensitive_words))\n",
    "\n",
    "## Get the directory length\n",
    "def get_directory_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return len(path)\n",
    "\n",
    "## Get the subdirectory count\n",
    "def get_sub_directory_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return path.count('/') - 1\n",
    "\n",
    "## Get the token count in the path\n",
    "def get_token_count_in_path(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    return len(tokens) - 1\n",
    "\n",
    "## Get the length of the largest token in the path\n",
    "def get_largest_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    if tokens:\n",
    "        return max(len(token) for token in tokens)\n",
    "    return 0\n",
    "\n",
    "## Get the average token length in the path\n",
    "def get_average_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = [token for token in path.split('/') if token]\n",
    "    if tokens:\n",
    "        return np.mean([len(token) for token in tokens])\n",
    "    return 0\n",
    "\n",
    "## Get the file length\n",
    "def get_file_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return len(filename)\n",
    "\n",
    "## Get the dot count in the file\n",
    "def get_dot_count_in_file(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return filename.count('.')\n",
    "\n",
    "## Get the delimiter count in the file\n",
    "def get_delimiter_count_in_file(url):\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    delimiters = ['.', '_', '-']\n",
    "    return sum(filename.count(delimiter) for delimiter in delimiters)\n",
    "\n",
    "## Get the arguments length\n",
    "def get_arguments_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(query)\n",
    "\n",
    "## Get the number of arguments\n",
    "def get_number_of_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(parse_qs(query))\n",
    "\n",
    "## Get the length of the largest argument value\n",
    "def get_length_of_largest_argument_value(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    if params:\n",
    "        return max(len(max(values, key=len)) for values in params.values())\n",
    "    return 0\n",
    "\n",
    "## Get the average argument value length\n",
    "def get_max_delimiters_in_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    delimiters = ['&', '=', '-', '_']\n",
    "    if params:\n",
    "        return max(sum(value.count(delimiter) for delimiter in delimiters) for values in params.values() for value in values)\n",
    "    return 0\n",
    "\n",
    "## Get the hyphen count in the domain\n",
    "def get_hyphen_count_in_domain(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return domain.count('-')\n",
    "\n",
    "## Get the digit count in the domain\n",
    "def contains_ip(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    try:\n",
    "        sock.inet_aton(domain)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "## Get the domain length, token count, largest token length, and average token length\n",
    "def get_domain_features(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(ensure_scheme(url)).netloc\n",
    "    tokens = domain.split('.')\n",
    "    \n",
    "    # Domain Length\n",
    "    domain_length = len(domain)\n",
    "    \n",
    "    # Count of Tokens in the Domain\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Length of Largest Token in the Domain\n",
    "    largest_token_length = max(len(token) for token in tokens) if tokens else 0\n",
    "    \n",
    "    # Average Domain Token Length\n",
    "    average_token_length = sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n",
    "    \n",
    "    return domain_length, token_count, largest_token_length, average_token_length\n",
    "\n",
    "## Check if the URL contains special characters\n",
    "def get_special_character_count(url):\n",
    "    special_characters = ['@', '=', '+', '*', '?', '&', '%', '$', '#', '!']\n",
    "    return sum(url.count(char) for char in special_characters)\n",
    "\n",
    "## Get the entropy of the URL\n",
    "def get_entropy(url):\n",
    "    # Count the frequency of each character in the string\n",
    "    freq = Counter(url)\n",
    "    # Calculate the probabilities\n",
    "    probs = [count / len(url) for count in freq.values()]\n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -sum(p * m.log(p, 2) for p in probs if p > 0)\n",
    "    return entropy\n",
    "\n",
    "## Check if the URL is shortened\n",
    "def check_url_shortened(url):\n",
    "    shortened_services = ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co']\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return int(domain in shortened_services)\n",
    "\n",
    "## Get the port number, if specified\n",
    "def get_port_number(url):\n",
    "    url = ensure_scheme(url)\n",
    "    port = urlparse(url).port\n",
    "    return port if port else -1  # Return -1 if no port specified\n",
    "\n",
    "## Get the subdomain count\n",
    "def get_subdomain_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain_parts = urlparse(url).netloc.split('.')\n",
    "    # Count as subdomains any parts beyond the second-level domain and TLD\n",
    "    return max(0, len(domain_parts) - 2)\n",
    "\n",
    "## Check if the URL TLD is suspicious (e.g., .xyz, .top, .loan, .win, .club)\n",
    "def get_suspicious_tld(url):\n",
    "    suspicious_tlds = ['xyz', 'top', 'loan', 'win', 'club']\n",
    "    url = ensure_scheme(url)\n",
    "    tld = urlparse(url).netloc.split('.')[-1]\n",
    "    return int(tld in suspicious_tlds)\n",
    "\n",
    "## Get the ratio of numeric characters in the URL\n",
    "def get_numeric_ratio(url):\n",
    "    numeric_chars = sum(c.isdigit() for c in url)\n",
    "    return numeric_chars / len(url) if len(url) > 0 else 0\n",
    "\n",
    "## Get the word count in the URL\n",
    "def get_word_count(url):\n",
    "    words = re.findall(r'\\w+', url)\n",
    "    return len(words)\n",
    "\n",
    "## Check if the URL is internationalized (i.e., contains non-ASCII characters)\n",
    "def get_url_is_internationalized(url):\n",
    "    try:\n",
    "        url.encode('ascii')\n",
    "        return 0\n",
    "    except UnicodeEncodeError:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29f9c7",
   "metadata": {},
   "source": [
    "### Now load the dataset and do some preprocessing on it\n",
    "The dataset is a CSV file with two columns: `url` and `label`. The `url` column contains the URL and the `label` column contains the label of the URL. The label is 1 if the URL is malicious and 0 if the URL is benign. Since the dataset is huge (more than 1 million rows), we will only use a small subset of it for this project (150,000 rows for each class, 300,000 rows in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "009c0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('url_dataset_updated.csv')\n",
    "\n",
    "# Drop all duplicates from df\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Reset the index after dropping duplicates\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "url_column_name = 'URL'  # Replace with your actual column name\n",
    "\n",
    "# Function to concatenate \"https://\" to URLs labeled with 0\n",
    "def add_https(url, label):\n",
    "    if label == 0:\n",
    "        return \"https://\" + url\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "# Apply the function to the URL column\n",
    "df[url_column_name] = df.apply(lambda row: add_https(row[url_column_name], row['Label']), axis=1)\n",
    "\n",
    "# Separate the dataset into malicious and benign\n",
    "malicious_df = df[df['Label'] == 1]\n",
    "benign_df = df[df['Label'] == 0]\n",
    "\n",
    "# Randomly sample 150,000 entries from each\n",
    "malicious_sampled_df = resample(malicious_df, n_samples=150000, random_state=42)\n",
    "benign_sampled_df = resample(benign_df, n_samples=150000, random_state=42)\n",
    "\n",
    "# Combine the sampled data\n",
    "balanced_df = pd.concat([malicious_sampled_df, benign_sampled_df])\n",
    "\n",
    "# Shuffle the combined dataset to mix malicious and benign URLs\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Drop all duplicates from balanced_df\n",
    "balanced_df = balanced_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after dropping duplicates\n",
    "balanced_df = balanced_df.reset_index(drop=True)\n",
    "\n",
    "# Now, balanced_df contains the balanced dataset ready for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d87432",
   "metadata": {},
   "source": [
    "## Start testing with the options\n",
    "**NOTE:** We'll split the truncated dataset into a training set and a test set (80% training, 20% test) and use the training set to train the model and the test set to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ce677",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer with random forest classifier, no feature extraction\n",
    "We will start by using the TF-IDF vectorizer to convert the URLs into numerical features and then use a random forest classifier to classify the URLs. We will not use any feature extraction functions for now. This is for reference only, as we will use feature extraction functions and better methods later.\n",
    "* **Warning:** This will take a long time to run, as the dataset is huge, and TF-IDF vectorization is a slow process. Don't run this unless you have a powerful computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec8b3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we'll use TF-IDF on the URLs themselves. Advanced features can be added based on URL structure and content.\n",
    "#vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Prepare the data\n",
    "#X = balanced_df['URL']\n",
    "#y = balanced_df['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "# Create a pipeline that first transforms the data using TfidfVectorizer then applies RandomForestClassifier\n",
    "#model = make_pipeline(TfidfVectorizer(), RandomForestClassifier(n_estimators=20, random_state=42))\n",
    "\n",
    "# Train the model\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "#y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "#print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5a356",
   "metadata": {},
   "source": [
    "### Random forest classifier, with feature extraction\n",
    "Next thing we will do is use the feature extraction functions to extract features from the URLs and then use a random forest classifier to classify the URLs. We will use the same training and test sets as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b06fa5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.945581752306841\n",
      "Confusion Matrix:\n",
      " [[26911   896]\n",
      " [ 2153 26069]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95     27807\n",
      "           1       0.97      0.92      0.94     28222\n",
      "\n",
      "    accuracy                           0.95     56029\n",
      "   macro avg       0.95      0.95      0.95     56029\n",
      "weighted avg       0.95      0.95      0.95     56029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply feature extraction\n",
    "features = balanced_df['URL'].apply(lambda x: pd.Series({\n",
    "    'url_length': get_url_length(x),\n",
    "    'dot_count': get_dot_count(x),\n",
    "    'hyphen_count_domain': get_hyphen_count_in_domain(x),\n",
    "    'security_sensitive_words': contains_security_sensitive_words(x),\n",
    "    'directory_length': get_directory_length(x),\n",
    "    'sub_directory_count': get_sub_directory_count(x),\n",
    "    'token_count_path': get_token_count_in_path(x),\n",
    "    'largest_token_length': get_largest_token_length(x),\n",
    "    'average_token_length': get_average_token_length(x),\n",
    "    'file_length': get_file_length(x),\n",
    "    'contains_ip': contains_ip(x),\n",
    "    'dot_count_in_file': get_dot_count_in_file(x),\n",
    "    'delimiter_count_in_file': get_delimiter_count_in_file(x),\n",
    "    'arguments_length': get_arguments_length(x),\n",
    "    'number_of_arguments': get_number_of_arguments(x),\n",
    "    'length_of_largest_argument_value': get_length_of_largest_argument_value(x),\n",
    "    'max_delimiters_in_arguments': get_max_delimiters_in_arguments(x),\n",
    "    'special_character_count': get_special_character_count(x),\n",
    "    'entropy': get_entropy(x),\n",
    "    'url_shortened': check_url_shortened(x),\n",
    "    'port_number': get_port_number(x),\n",
    "    'subdomain_count': get_subdomain_count(x),\n",
    "    'suspicious_tld': get_suspicious_tld(x),\n",
    "    'numeric_ratio': get_numeric_ratio(x),\n",
    "    'url_is_internationalized': get_url_is_internationalized(x),\n",
    "    'domain_length': get_domain_features(x)[0],\n",
    "    'domain_token_count': get_domain_features(x)[1],\n",
    "    'largest_domain_token_length': get_domain_features(x)[2],\n",
    "    'average_domain_token_length': get_domain_features(x)[3],\n",
    "    'word_count': get_word_count(x)\n",
    "}))\n",
    "\n",
    "\n",
    "# Concatenate original DF with features\n",
    "balanced_df_with_features = pd.concat([balanced_df, features], axis=1)\n",
    "\n",
    "# Define X and y\n",
    "X = balanced_df_with_features.drop(['Label', 'URL'], axis=1)\n",
    "y = balanced_df_with_features['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "# Since your features are already numerical, directly use RandomForestClassifier without TfidfVectorizer\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "feature_selection_pipeline = Pipeline([\n",
    "    ('variance_threshold', VarianceThreshold(threshold=0))\n",
    "])\n",
    "\n",
    "# Fit and transform the pipeline on the training data\n",
    "X_train_transformed = feature_selection_pipeline.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transform the test data based on the fitted pipeline\n",
    "X_test_transformed = feature_selection_pipeline.transform(X_test)\n",
    "\n",
    "# Now, train your model on X_train_transformed and test on X_test_transformed\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predictions\n",
    "model_evaluation(model, X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bcad1",
   "metadata": {},
   "source": [
    "## TensorFlow neural network, with feature extraction\n",
    "Now let's use a neural network to classify the URLs. We will use the feature extraction functions to extract features from the URLs and then use a neural network to classify the URLs. We will use the same training and test sets as before. As the same, the training set is 80% and the test set is 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a08f25",
   "metadata": {},
   "source": [
    "### Preprocessing for Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6492a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and sequence padding parameters\n",
    "max_len = 100  # Adjust based on the length of the longest URL in your dataset\n",
    "max_words = 60000  # Adjust based on the size of your vocabulary\n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(balanced_df_with_features['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df_with_features['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df_with_features['Label'])\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "feature_selection_pipeline = Pipeline([\n",
    "    ('variance_threshold', VarianceThreshold(threshold=0))\n",
    "])\n",
    "\n",
    "# Fit and transform the pipeline on the training data\n",
    "X_train_transformed = feature_selection_pipeline.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transform the test data based on the fitted pipeline\n",
    "X_test_transformed = feature_selection_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7517f2",
   "metadata": {},
   "source": [
    "### Using RNN model with feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0db0b83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1401/1401 [==============================] - 88s 62ms/step - loss: 0.1359 - acc: 0.9515 - val_loss: 0.1019 - val_acc: 0.9654\n",
      "Epoch 2/10\n",
      "1401/1401 [==============================] - 90s 64ms/step - loss: 0.0959 - acc: 0.9664 - val_loss: 0.0926 - val_acc: 0.9666\n",
      "Epoch 3/10\n",
      "1401/1401 [==============================] - 91s 65ms/step - loss: 0.0913 - acc: 0.9686 - val_loss: 0.0921 - val_acc: 0.9673\n",
      "Epoch 4/10\n",
      "1401/1401 [==============================] - 91s 65ms/step - loss: 0.0837 - acc: 0.9710 - val_loss: 0.0868 - val_acc: 0.9694\n",
      "Epoch 5/10\n",
      "1401/1401 [==============================] - 82s 58ms/step - loss: 0.0840 - acc: 0.9712 - val_loss: 0.0841 - val_acc: 0.9703\n",
      "Epoch 6/10\n",
      "1401/1401 [==============================] - 70s 50ms/step - loss: 0.0717 - acc: 0.9751 - val_loss: 0.0832 - val_acc: 0.9710\n",
      "Epoch 7/10\n",
      "1401/1401 [==============================] - 63s 45ms/step - loss: 0.0685 - acc: 0.9761 - val_loss: 0.0823 - val_acc: 0.9711\n",
      "Epoch 8/10\n",
      "1401/1401 [==============================] - 62s 44ms/step - loss: 0.0663 - acc: 0.9773 - val_loss: 0.0814 - val_acc: 0.9720\n",
      "Epoch 9/10\n",
      "1401/1401 [==============================] - 63s 45ms/step - loss: 0.0723 - acc: 0.9752 - val_loss: 0.0812 - val_acc: 0.9721\n",
      "Epoch 10/10\n",
      "1401/1401 [==============================] - 62s 44ms/step - loss: 0.0635 - acc: 0.9782 - val_loss: 0.0805 - val_acc: 0.9719\n",
      "1751/1751 [==============================] - 31s 16ms/step\n",
      "Accuracy: 0.9723179067982651\n",
      "Confusion Matrix:\n",
      " [[27297   510]\n",
      " [ 1041 27181]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     27807\n",
      "           1       0.98      0.96      0.97     28222\n",
      "\n",
      "    accuracy                           0.97     56029\n",
      "   macro avg       0.97      0.97      0.97     56029\n",
      "weighted avg       0.97      0.97      0.97     56029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RNN model definition\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_words, 32, input_length=X_train_transformed.shape[1]))\n",
    "model.add(tf.keras.layers.SimpleRNN(32))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping, using the CPU since the model is too simple for GPU acceleration\n",
    "with tf.device('/device:CPU:0'):\n",
    "    history = model.fit(X_train_transformed, y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded79e41",
   "metadata": {},
   "source": [
    "#### Optimize the TensorFlow neural network using RNN and Adam optimizer\n",
    "Now let's optimize the neural network using RNN and the Adam optimizer. We will use the feature extraction functions to extract features from the URLs and then use a neural network with RNN and the Adam optimizer to classify the URLs. We will use the same training and test sets as before. Since the model now is much more complex, we'll use TensorFlow GPU acceleration to speed up the training process, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5fdf8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1401/1401 [==============================] - 116s 78ms/step - loss: 0.1524 - acc: 0.9491 - val_loss: 0.0913 - val_acc: 0.9688\n",
      "Epoch 2/30\n",
      "1401/1401 [==============================] - 108s 77ms/step - loss: 0.0758 - acc: 0.9750 - val_loss: 0.0871 - val_acc: 0.9700\n",
      "Epoch 3/30\n",
      "1401/1401 [==============================] - 109s 78ms/step - loss: 0.0627 - acc: 0.9796 - val_loss: 0.0863 - val_acc: 0.9702\n",
      "Epoch 4/30\n",
      "1401/1401 [==============================] - 108s 77ms/step - loss: 0.0582 - acc: 0.9805 - val_loss: 0.0868 - val_acc: 0.9718\n",
      "Epoch 5/30\n",
      "1401/1401 [==============================] - 109s 78ms/step - loss: 0.0545 - acc: 0.9816 - val_loss: 0.0883 - val_acc: 0.9721\n",
      "Epoch 6/30\n",
      "1401/1401 [==============================] - 116s 83ms/step - loss: 0.0527 - acc: 0.9819 - val_loss: 0.0907 - val_acc: 0.9718\n",
      "Epoch 7/30\n",
      "1401/1401 [==============================] - 120s 86ms/step - loss: 0.0510 - acc: 0.9823 - val_loss: 0.0903 - val_acc: 0.9717\n",
      "Epoch 8/30\n",
      "1401/1401 [==============================] - 83s 59ms/step - loss: 0.0498 - acc: 0.9828 - val_loss: 0.0906 - val_acc: 0.9715\n",
      "1751/1751 [==============================] - 33s 18ms/step\n",
      "Accuracy: 0.9716932302914562\n",
      "Confusion Matrix:\n",
      " [[27386   421]\n",
      " [ 1165 27057]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     27807\n",
      "           1       0.98      0.96      0.97     28222\n",
      "\n",
      "    accuracy                           0.97     56029\n",
      "   macro avg       0.97      0.97      0.97     56029\n",
      "weighted avg       0.97      0.97      0.97     56029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and sequence padding parameters\n",
    "max_len = 100  # Adjust based on the length of the longest URL in your dataset\n",
    "max_words = 60000  # Adjust based on the size of your vocabulary\n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(balanced_df_with_features['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df_with_features['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df_with_features['Label'])\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "feature_selection_pipeline = Pipeline([\n",
    "    ('variance_threshold', VarianceThreshold(threshold=0))\n",
    "])\n",
    "\n",
    "# Fit and transform the pipeline on the training data\n",
    "X_train_transformed = feature_selection_pipeline.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transform the test data based on the fitted pipeline\n",
    "X_test_transformed = feature_selection_pipeline.transform(X_test)\n",
    "\n",
    "\n",
    "# Model definition\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_words, 32, input_length=X_train_transformed.shape[1]))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))) # Add return_sequences if stacking LSTM layers\n",
    "model.add(tf.keras.layers.Dropout(0.5))  # Adjust dropout rate as needed\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)))  # Second LSTM layer, without return_sequences\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))  # Add L2 regularization\n",
    "model.add(tf.keras.layers.Dropout(0.5))  # Adjust dropout rate as needed\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "# Use GPU for training, as the model is more complex and very slow to train on CPU\n",
    "history = model.fit(X_train_transformed, y_train, epochs=30, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "825f16a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save(\"RNN_and_Adam_trained_model.h5\")\n",
    "\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aca729",
   "metadata": {},
   "source": [
    "#### Another optimization for the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1f07cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/30\n",
      "1399/1399 [==============================] - 267s 187ms/step - loss: 0.2404 - acc: 0.9037 - val_loss: 0.2019 - val_acc: 0.9225\n",
      "Epoch 2/30\n",
      "1399/1399 [==============================] - 262s 187ms/step - loss: 0.1944 - acc: 0.9252 - val_loss: 0.1802 - val_acc: 0.9299\n",
      "Epoch 3/30\n",
      "1399/1399 [==============================] - 260s 186ms/step - loss: 0.1763 - acc: 0.9329 - val_loss: 0.1625 - val_acc: 0.9358\n",
      "Epoch 4/30\n",
      "1399/1399 [==============================] - 257s 184ms/step - loss: 0.1671 - acc: 0.9365 - val_loss: 0.1621 - val_acc: 0.9369\n",
      "Epoch 5/30\n",
      "1399/1399 [==============================] - 240s 172ms/step - loss: 0.1603 - acc: 0.9395 - val_loss: 0.1487 - val_acc: 0.9435\n",
      "Epoch 6/30\n",
      "1399/1399 [==============================] - 264s 188ms/step - loss: 0.1549 - acc: 0.9415 - val_loss: 0.1464 - val_acc: 0.9451\n",
      "Epoch 7/30\n",
      "1399/1399 [==============================] - 260s 186ms/step - loss: 0.1514 - acc: 0.9430 - val_loss: 0.1394 - val_acc: 0.9463\n",
      "Epoch 8/30\n",
      "1399/1399 [==============================] - 250s 178ms/step - loss: 0.1463 - acc: 0.9450 - val_loss: 0.1358 - val_acc: 0.9478\n",
      "Epoch 9/30\n",
      "1399/1399 [==============================] - 254s 182ms/step - loss: 0.1436 - acc: 0.9462 - val_loss: 0.1333 - val_acc: 0.9491\n",
      "Epoch 10/30\n",
      "1399/1399 [==============================] - 261s 187ms/step - loss: 0.1407 - acc: 0.9478 - val_loss: 0.1346 - val_acc: 0.9493\n",
      "Epoch 11/30\n",
      "1399/1399 [==============================] - 260s 186ms/step - loss: 0.1383 - acc: 0.9491 - val_loss: 0.1338 - val_acc: 0.9502\n",
      "Epoch 12/30\n",
      "1399/1399 [==============================] - 260s 186ms/step - loss: 0.1358 - acc: 0.9499 - val_loss: 0.1345 - val_acc: 0.9488\n",
      "Epoch 13/30\n",
      "1399/1399 [==============================] - 261s 187ms/step - loss: 0.1348 - acc: 0.9496 - val_loss: 0.1258 - val_acc: 0.9521\n",
      "Epoch 14/30\n",
      "1399/1399 [==============================] - 260s 186ms/step - loss: 0.1326 - acc: 0.9506 - val_loss: 0.1250 - val_acc: 0.9514\n",
      "Epoch 15/30\n",
      "1399/1399 [==============================] - 260s 186ms/step - loss: 0.1320 - acc: 0.9511 - val_loss: 0.1290 - val_acc: 0.9517\n",
      "Epoch 16/30\n",
      "1399/1399 [==============================] - 261s 186ms/step - loss: 0.1305 - acc: 0.9517 - val_loss: 0.1238 - val_acc: 0.9524\n",
      "Epoch 17/30\n",
      "1399/1399 [==============================] - 256s 183ms/step - loss: 0.1286 - acc: 0.9523 - val_loss: 0.1247 - val_acc: 0.9519\n",
      "Epoch 18/30\n",
      "1399/1399 [==============================] - 257s 184ms/step - loss: 0.1275 - acc: 0.9526 - val_loss: 0.1204 - val_acc: 0.9530\n",
      "Epoch 19/30\n",
      "1399/1399 [==============================] - 262s 188ms/step - loss: 0.1269 - acc: 0.9531 - val_loss: 0.1204 - val_acc: 0.9534\n",
      "Epoch 20/30\n",
      "1399/1399 [==============================] - 260s 186ms/step - loss: 0.1260 - acc: 0.9534 - val_loss: 0.1210 - val_acc: 0.9525\n",
      "Epoch 21/30\n",
      "1399/1399 [==============================] - 260s 186ms/step - loss: 0.1245 - acc: 0.9538 - val_loss: 0.1192 - val_acc: 0.9544\n",
      "Epoch 22/30\n",
      "1399/1399 [==============================] - 260s 186ms/step - loss: 0.1236 - acc: 0.9546 - val_loss: 0.1214 - val_acc: 0.9527\n",
      "Epoch 23/30\n",
      "1399/1399 [==============================] - 261s 186ms/step - loss: 0.1235 - acc: 0.9544 - val_loss: 0.1164 - val_acc: 0.9552\n",
      "Epoch 24/30\n",
      "1399/1399 [==============================] - 261s 187ms/step - loss: 0.1223 - acc: 0.9544 - val_loss: 0.1214 - val_acc: 0.9533\n",
      "Epoch 25/30\n",
      "1399/1399 [==============================] - 258s 185ms/step - loss: 0.1214 - acc: 0.9552 - val_loss: 0.1169 - val_acc: 0.9558\n",
      "Epoch 26/30\n",
      "1399/1399 [==============================] - 251s 180ms/step - loss: 0.1206 - acc: 0.9555 - val_loss: 0.1168 - val_acc: 0.9554\n",
      "Epoch 27/30\n",
      "1399/1399 [==============================] - 259s 185ms/step - loss: 0.1199 - acc: 0.9555 - val_loss: 0.1162 - val_acc: 0.9563\n",
      "Epoch 28/30\n",
      "1399/1399 [==============================] - 251s 179ms/step - loss: 0.1192 - acc: 0.9555 - val_loss: 0.1143 - val_acc: 0.9565\n",
      "Epoch 29/30\n",
      "1399/1399 [==============================] - 261s 187ms/step - loss: 0.1193 - acc: 0.9560 - val_loss: 0.1153 - val_acc: 0.9559\n",
      "Epoch 30/30\n",
      "1399/1399 [==============================] - 260s 186ms/step - loss: 0.1184 - acc: 0.9562 - val_loss: 0.1149 - val_acc: 0.9561\n",
      "1749/1749 [==============================] - 261s 149ms/step\n",
      "Accuracy: 0.9603367532978229\n",
      "Confusion Matrix:\n",
      " [[27673   500]\n",
      " [ 1719 26054]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96     28173\n",
      "           1       0.98      0.94      0.96     27773\n",
      "\n",
      "    accuracy                           0.96     55946\n",
      "   macro avg       0.96      0.96      0.96     55946\n",
      "weighted avg       0.96      0.96      0.96     55946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and sequence padding parameters\n",
    "max_len = 100  # Adjust based on the length of the longest URL in your dataset\n",
    "max_words = 60000  # Adjust based on the size of your vocabulary\n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words, char_level=True)\n",
    "tokenizer.fit_on_texts(balanced_df['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df['Label'])\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# RNN model definition\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_words, 32, input_length=max_len)) # Embedding layer\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2))) # Bidirectional LSTM layer\n",
    "\n",
    "# Multi-layer dense network\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "# Dropout layer\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# Use Adam optimizer with a lower learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "# Use GPU for training, as the model is more complex and very slow to train on CPU\n",
    "with tf.device('/CPU:0'):\n",
    "    history = model.fit(X_train, y_train, epochs=30, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f82c37b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
