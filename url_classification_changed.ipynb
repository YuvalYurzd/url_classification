{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac17c243",
   "metadata": {},
   "source": [
    "# URL Classification model using Machine Learning\n",
    "### For Computer Science B.Sc. Ariel University\n",
    "\n",
    "**By Yuval Yurzdichansky, Matan Aviv and Roy Simanovich**\n",
    "\n",
    "## Introduction\n",
    "In this project we will build a machine learning model that will classify URLs based on if they are malicious (phishing, malware, etc.) or benign. We will use a dataset of URLs that are labeled as either malicious or benign and use it to train a model that will be able to classify new URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242fd86",
   "metadata": {},
   "source": [
    "### Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae9a164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (1.26.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (2.10.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (1.60.1)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.28.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Some importent libraries installtion via pip\n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4fd24ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siman\\AppData\\Local\\Temp\\ipykernel_3636\\1534196349.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries for the project #\n",
    "\n",
    "## Regular Libraries that's come with python\n",
    "### Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "### Numerical Computing\n",
    "import numpy as np\n",
    "### Collection of Data Structures\n",
    "import collections as col\n",
    "### Regular Expressions\n",
    "import re as regex\n",
    "### URL Handling\n",
    "import urllib as urlhndl\n",
    "### Mathematical Operations\n",
    "import math\n",
    "### Socket Programming\n",
    "import socket\n",
    "\n",
    "## SKLearn Libraries for Machine Learning\n",
    "### Splitting the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "### Metrics for Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "### Preprocessing the Data\n",
    "from sklearn.utils import resample\n",
    "### TF-IDF Vectorizer for Text Data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "### Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "### Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "### Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "### K-Nearest Neighbors Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## Tensorflow Libraries for Deep Learning\n",
    "import tensorflow as tf\n",
    "\n",
    "## Pickle for saving the model to disk\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83fb0b",
   "metadata": {},
   "source": [
    "### Feature extraction functions\n",
    "We will use the following feature extraction functions to extract features from the URLs:\n",
    "1. `get_url_length`: This function returns the length of the URL.\n",
    "2. `get_dot_count`: This function returns the number of dots in the URL.\n",
    "3. `get_hyphen_count_in_domain`: This function returns the number of hyphens in the domain of the URL.\n",
    "4. `contains_security_sensitive_words`: This function returns 1 if the URL contains security sensitive words (e.g., \"login\", \"signin\", \"account\", etc.) and 0 otherwise.\n",
    "5. `get_directory_length`: This function returns the length of the directory part of the URL.\n",
    "6. `get_sub_directory_count`: This function returns the number of subdirectories in the URL.\n",
    "7. `contains_ip`: This function returns 1 if the URL contains an IP address and 0 otherwise.\n",
    "8. `get_token_count_in_path`: This function returns the number of tokens in the path part of the URL.\n",
    "9. `get_largest_token_length`: This function returns the length of the largest token in the path part of the URL.\n",
    "10. `get_avarage_token_length`: This function returns the average length of the tokens in the path part of the URL.\n",
    "11. `get_file_length`: This function returns the length of the file part of the URL.\n",
    "12. `get_dot_count_in_file`: This function returns the number of dots in the file part of the URL.\n",
    "13. `get_delimiter_count_in_file`: This function returns the number of delimiters in the file part of the URL.\n",
    "14. `get_arguments_length`: This function returns the length of the arguments part of the URL.\n",
    "15. `get_number_of_arguments`: This function returns the number of arguments in the URL.\n",
    "16. `get_length_of_largest_argument_value`: This function returns the length of the largest argument value in the URL.\n",
    "17. `get_max_delimiters_in_arguments`: This function returns the maximum number of delimiters in the arguments part of the URL.\n",
    "18. `get_domain_features`: This function returns the following features of the domain part of the URL: length, number of dots, number of hyphens, number of digits, number of special characters, entropy, whether the domain is a subdomain, and whether the domain is an IP address.\n",
    "19. `get_special_character_count`: This function returns the number of special characters in the URL.\n",
    "20. `get_entropy`: This function returns the entropy of the URL.\n",
    "21. `check_url_shortened`: This function returns 1 if the URL is shortened and 0 otherwise.\n",
    "22. `get_port_number`: This function returns the port number of the URL, if it exists, and -1 otherwise.\n",
    "23. `get_subdomain_count`: This function returns the number of subdomains in the URL.\n",
    "24. `get_suspicious_tld`: This function returns 1 if the top-level domain (TLD) of the URL is suspicious (e.g., \"tk\", \"ml\", \"ga\", etc.) and 0 otherwise.\n",
    "25. `get_numeric_ratio`: This function returns the ratio of numeric characters in the URL.\n",
    "26. `get_word_count`: This function returns the number of words in the URL.\n",
    "27. `get_url_is_internationalized`: This function returns 1 if the URL is internationalized and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86207970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to concatenate \"https://\" to benign URLs \n",
    "def add_https(url, label):\n",
    "    if label == 0:\n",
    "        return \"https://\" + url\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "def ensure_scheme(url):\n",
    "    if not urlhndl.parse.urlparse(url).scheme:\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "# Feature extraction functions\n",
    "def get_url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "def get_dot_count(url):\n",
    "    return url.count('.')\n",
    "\n",
    "def contains_security_sensitive_words(url):\n",
    "    security_sensitive_words = [\n",
    "    'login', 'password', 'admin', 'root', 'secret', 'private', 'secure', 'confidential', \n",
    "    'bank', 'creditcard', 'account', 'authentication', 'authorization', 'session', 'token', \n",
    "    'apikey', 'ssl', 'https', 'secure', 'encrypted', 'auth', 'signin', 'signup', 'verification', \n",
    "    'resetpassword', 'change-password', 'forgot-password', 'otp', '2fa', 'phishing', 'malware', \n",
    "    'virus', 'trojan', 'exploit', 'hacker', 'attack', 'security', 'vulnerable', 'injection', \n",
    "    'xss', 'csrf', 'dos', 'ddos', 'bruteforce', 'firewall', 'vpn', 'proxy', 'tor', 'security-question', \n",
    "    'privacy-policy']\n",
    "    return int(any(word in url for word in security_sensitive_words))\n",
    "\n",
    "def get_directory_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    return len(path)\n",
    "\n",
    "def get_sub_directory_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    return path.count('/') - 1\n",
    "\n",
    "def get_token_count_in_path(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    return len(tokens) - 1\n",
    "\n",
    "def get_largest_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    if tokens:\n",
    "        return max(len(token) for token in tokens)\n",
    "    return 0\n",
    "\n",
    "def get_average_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    tokens = [token for token in path.split('/') if token]\n",
    "    if tokens:\n",
    "        return np.mean([len(token) for token in tokens])\n",
    "    return 0\n",
    "\n",
    "def get_file_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return len(filename)\n",
    "\n",
    "def get_dot_count_in_file(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return filename.count('.')\n",
    "\n",
    "def get_delimiter_count_in_file(url):\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    delimiters = ['.', '_', '-']\n",
    "    return sum(filename.count(delimiter) for delimiter in delimiters)\n",
    "\n",
    "def get_arguments_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    return len(query)\n",
    "\n",
    "def get_number_of_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    return len(urlhndl.parse.parse_qs(query))\n",
    "\n",
    "def get_length_of_largest_argument_value(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    params = urlhndl.parse.parse_qs(query)\n",
    "    if params:\n",
    "        return max(len(max(values, key=len)) for values in params.values())\n",
    "    return 0\n",
    "\n",
    "def get_max_delimiters_in_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    params = urlhndl.parse.parse_qs(query)\n",
    "    delimiters = ['&', '=', '-', '_']\n",
    "    if params:\n",
    "        return max(sum(value.count(delimiter) for delimiter in delimiters) for values in params.values() for value in values)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_hyphen_count_in_domain(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(url).netloc\n",
    "    return domain.count('-')\n",
    "\n",
    "def contains_ip(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(url).netloc\n",
    "    try:\n",
    "        socket.inet_aton(domain)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def get_domain_features(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(ensure_scheme(url)).netloc\n",
    "    tokens = domain.split('.')\n",
    "    \n",
    "    # Domain Length\n",
    "    domain_length = len(domain)\n",
    "    \n",
    "    # Count of Tokens in the Domain\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Length of Largest Token in the Domain\n",
    "    largest_token_length = max(len(token) for token in tokens) if tokens else 0\n",
    "    \n",
    "    # Average Domain Token Length\n",
    "    average_token_length = sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n",
    "    \n",
    "    return domain_length, token_count, largest_token_length, average_token_length\n",
    "\n",
    "# New feature extraction functions\n",
    "def get_special_character_count(url):\n",
    "    special_characters = ['@', '=', '+', '*', '?', '&', '%', '$', '#', '!']\n",
    "    return sum(url.count(char) for char in special_characters)\n",
    "\n",
    "def get_entropy(url):\n",
    "    # Count the frequency of each character in the string\n",
    "    freq = col.Counter(url)\n",
    "    # Calculate the probabilities\n",
    "    probs = [count / len(url) for count in freq.values()]\n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -sum(p * math.log(p, 2) for p in probs if p > 0)\n",
    "    return entropy\n",
    "\n",
    "def check_url_shortened(url):\n",
    "    shortened_services = ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co']\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(url).netloc\n",
    "    return int(domain in shortened_services)\n",
    "\n",
    "def get_subdomain_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain_parts = urlhndl.parse.urlparse(url).netloc.split('.')\n",
    "    # Count as subdomains any parts beyond the second-level domain and TLD\n",
    "    return max(0, len(domain_parts) - 2)\n",
    "\n",
    "def get_suspicious_tld(url):\n",
    "    suspicious_tlds = ['xyz', 'top', 'loan', 'win', 'club']\n",
    "    url = ensure_scheme(url)\n",
    "    tld = urlhndl.parse.urlparse(url).netloc.split('.')[-1]\n",
    "    return int(tld in suspicious_tlds)\n",
    "\n",
    "def get_numeric_ratio(url):\n",
    "    numeric_chars = sum(c.isdigit() for c in url)\n",
    "    return numeric_chars / len(url) if len(url) > 0 else 0\n",
    "\n",
    "def get_word_count(url):\n",
    "    words = regex.findall(r'\\w+', url)\n",
    "    return len(words)\n",
    "\n",
    "def is_https(url, timeout=0.5):\n",
    "    return int(url.startswith(\"https\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29f9c7",
   "metadata": {},
   "source": [
    "### Now load the dataset and do some preprocessing on it\n",
    "The dataset is a CSV file with two columns: `url` and `label`. The `url` column contains the URL and the `label` column contains the label of the URL. The label is 1 if the URL is malicious and 0 if the URL is benign. Since the dataset is huge (more than 1 million rows), we will only use a small subset of it for this project (150,000 rows for each class, 300,000 rows in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "009c0099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Dataset Shape: (300000, 2)\n",
      "Balanced Dataset Distribution: Label\n",
      "1    150000\n",
      "0    150000\n",
      "Name: count, dtype: int64\n",
      "Balanced Dataset Head (First 10 Rows):\n",
      "                                                 URL  Label\n",
      "0  http://etransfers.interac.ca-ssl.net/sh/2o05I9...      1\n",
      "1  http://betterhealthsmoothies.com/Adobe/adobe-3...      1\n",
      "2  http://lloydsbank.deregister-payee-secure-auth...      1\n",
      "3                                 https://archive.md      0\n",
      "4                      https://pkg00-atx.netgate.com      0\n",
      "5  https://www.shareholds.com/gbr/5df72f77-f30d-4...      1\n",
      "6                  https://sub-166-141-241.myvzw.com      0\n",
      "7     https://hgdggdgfghygsugfytsfgssytstys.gq/83cbc      1\n",
      "8                    https://www.commbanknetcode.com      1\n",
      "9                    https://autodiscover.ons.gov.uk      0\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('url_dataset_updated.csv')\n",
    "\n",
    "# Drop all duplicates from df\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Reset the index after dropping duplicates\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Name of the URL column, change this if your dataset has a different column name\n",
    "url_column_name = 'URL'\n",
    "\n",
    "# Apply the function to the URL column\n",
    "df[url_column_name] = df.apply(lambda row: add_https(row[url_column_name], row['Label']), axis=1)\n",
    "\n",
    "# Separate the dataset into malicious and benign\n",
    "malicious_df = df[df['Label'] == 1]\n",
    "benign_df = df[df['Label'] == 0]\n",
    "\n",
    "# Randomly sample 150,000 entries from each\n",
    "malicious_sampled_df = resample(malicious_df, n_samples=150000, random_state=42)\n",
    "benign_sampled_df = resample(benign_df, n_samples=150000, random_state=42)\n",
    "\n",
    "# Combine the sampled data\n",
    "balanced_df = pd.concat([malicious_sampled_df, benign_sampled_df])\n",
    "\n",
    "# Shuffle the combined dataset to mix malicious and benign URLs\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print('Balanced Dataset Shape:', balanced_df.shape)\n",
    "print('Balanced Dataset Distribution:', balanced_df['Label'].value_counts())\n",
    "print('Balanced Dataset Head (First 10 Rows):')\n",
    "print(balanced_df.head(10))\n",
    "\n",
    "# Now, balanced_df contains the balanced dataset ready for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d87432",
   "metadata": {},
   "source": [
    "## Start testing with the options\n",
    "**NOTE:** We'll split the truncated dataset into a training set and a test set (80% training, 20% test) and use the training set to train the model and the test set to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ce677",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer preprocessing\n",
    "We will start by using the TF-IDF vectorizer to convert the URLs into numerical features for preprocessing, and split the dataset into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b06fa5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction...\n",
      "Balanced Dataset Shape: (300000, 31)\n",
      "Balanced Dataset:\n",
      "                                                      URL  Label  url_length  \\\n",
      "0       http://etransfers.interac.ca-ssl.net/sh/2o05I9...      1        65.0   \n",
      "1       http://betterhealthsmoothies.com/Adobe/adobe-3...      1        58.0   \n",
      "2       http://lloydsbank.deregister-payee-secure-auth...      1        60.0   \n",
      "3                                      https://archive.md      0        18.0   \n",
      "4                           https://pkg00-atx.netgate.com      0        29.0   \n",
      "...                                                   ...    ...         ...   \n",
      "299995       https://infomation-fb-service.e82443.repl.co      1        44.0   \n",
      "299996                   https://img-1000736.ad-score.com      0        32.0   \n",
      "299997                http://sosyalsat.com/help/home.html      1        35.0   \n",
      "299998  https://storageapi.fleek.co/12678f8a-04f9-4b69...      1        83.0   \n",
      "299999  https://trepievirealestate.com/paintdesk/auth/...      1       294.0   \n",
      "\n",
      "        dot_count  hyphen_count_domain  security_sensitive_words  \\\n",
      "0             4.0                  1.0                       1.0   \n",
      "1             2.0                  0.0                       0.0   \n",
      "2             3.0                  3.0                       1.0   \n",
      "3             1.0                  0.0                       1.0   \n",
      "4             2.0                  1.0                       1.0   \n",
      "...           ...                  ...                       ...   \n",
      "299995        3.0                  2.0                       1.0   \n",
      "299996        2.0                  2.0                       1.0   \n",
      "299997        2.0                  0.0                       0.0   \n",
      "299998        3.0                  0.0                       1.0   \n",
      "299999        2.0                  0.0                       1.0   \n",
      "\n",
      "        directory_length  sub_directory_count  token_count_path  \\\n",
      "0                   29.0                  3.0               4.0   \n",
      "1                   26.0                  2.0               3.0   \n",
      "2                   10.0                  0.0               1.0   \n",
      "3                    0.0                 -1.0               0.0   \n",
      "4                    0.0                 -1.0               0.0   \n",
      "...                  ...                  ...               ...   \n",
      "299995               0.0                 -1.0               0.0   \n",
      "299996               0.0                 -1.0               0.0   \n",
      "299997              15.0                  1.0               2.0   \n",
      "299998              56.0                  1.0               2.0   \n",
      "299999              33.0                  3.0               4.0   \n",
      "\n",
      "        largest_token_length  ...  url_shortened  subdomain_count  \\\n",
      "0                       12.0  ...            0.0              2.0   \n",
      "1                        9.0  ...            0.0              0.0   \n",
      "2                        9.0  ...            0.0              1.0   \n",
      "3                        0.0  ...            0.0              0.0   \n",
      "4                        0.0  ...            0.0              1.0   \n",
      "...                      ...  ...            ...              ...   \n",
      "299995                   0.0  ...            0.0              2.0   \n",
      "299996                   0.0  ...            0.0              1.0   \n",
      "299997                   9.0  ...            0.0              0.0   \n",
      "299998                  43.0  ...            0.0              1.0   \n",
      "299999                  15.0  ...            0.0              0.0   \n",
      "\n",
      "        suspicious_tld  numeric_ratio  domain_length  domain_token_count  \\\n",
      "0                  0.0       0.061538           29.0                 4.0   \n",
      "1                  0.0       0.034483           25.0                 2.0   \n",
      "2                  0.0       0.000000           43.0                 3.0   \n",
      "3                  0.0       0.000000           10.0                 2.0   \n",
      "4                  0.0       0.068966           21.0                 3.0   \n",
      "...                ...            ...            ...                 ...   \n",
      "299995             0.0       0.113636           36.0                 4.0   \n",
      "299996             0.0       0.218750           24.0                 3.0   \n",
      "299997             0.0       0.000000           13.0                 2.0   \n",
      "299998             0.0       0.289157           19.0                 3.0   \n",
      "299999             0.0       0.479592           22.0                 2.0   \n",
      "\n",
      "        largest_domain_token_length  average_domain_token_length  word_count  \\\n",
      "0                              10.0                     6.500000        11.0   \n",
      "1                              21.0                    12.000000         8.0   \n",
      "2                              28.0                    13.666667         9.0   \n",
      "3                               7.0                     4.500000         3.0   \n",
      "4                               9.0                     6.333333         5.0   \n",
      "...                             ...                          ...         ...   \n",
      "299995                         21.0                     8.250000         7.0   \n",
      "299996                         11.0                     7.333333         6.0   \n",
      "299997                          9.0                     6.000000         6.0   \n",
      "299998                         10.0                     5.666667        12.0   \n",
      "299999                         18.0                    10.500000        15.0   \n",
      "\n",
      "        is_https  \n",
      "0            0.0  \n",
      "1            0.0  \n",
      "2            0.0  \n",
      "3            1.0  \n",
      "4            1.0  \n",
      "...          ...  \n",
      "299995       1.0  \n",
      "299996       1.0  \n",
      "299997       0.0  \n",
      "299998       1.0  \n",
      "299999       1.0  \n",
      "\n",
      "[300000 rows x 31 columns]\n",
      "Fitting and transforming the TF-IDF Vectorizer (max_features=5000) to the URLs in the dataset...\n",
      "TF-IDF Vectorization complete.\n",
      "Saving the TF-IDF Vectorizer to disk...\n",
      "TF-IDF Vectorizer saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# Apply feature extraction\n",
    "print('Feature extraction...')\n",
    "features = balanced_df['URL'].apply(lambda x: pd.Series({\n",
    "    'url_length': get_url_length(x),\n",
    "    'dot_count': get_dot_count(x),\n",
    "    'hyphen_count_domain': get_hyphen_count_in_domain(x),\n",
    "    'security_sensitive_words': contains_security_sensitive_words(x),\n",
    "    'directory_length': get_directory_length(x),\n",
    "    'sub_directory_count': get_sub_directory_count(x),\n",
    "    'token_count_path': get_token_count_in_path(x),\n",
    "    'largest_token_length': get_largest_token_length(x),\n",
    "    'average_token_length': get_average_token_length(x),\n",
    "    'file_length': get_file_length(x),\n",
    "    'contains_ip': contains_ip(x),\n",
    "    'dot_count_in_file': get_dot_count_in_file(x),\n",
    "    'delimiter_count_in_file': get_delimiter_count_in_file(x),\n",
    "    'arguments_length': get_arguments_length(x),\n",
    "    'number_of_arguments': get_number_of_arguments(x),\n",
    "    'length_of_largest_argument_value': get_length_of_largest_argument_value(x),\n",
    "    'max_delimiters_in_arguments': get_max_delimiters_in_arguments(x),\n",
    "    'special_character_count': get_special_character_count(x),\n",
    "    'entropy': get_entropy(x),\n",
    "    'url_shortened': check_url_shortened(x),\n",
    "    'subdomain_count': get_subdomain_count(x),\n",
    "    'suspicious_tld': get_suspicious_tld(x),\n",
    "    'numeric_ratio': get_numeric_ratio(x),\n",
    "    'domain_length': get_domain_features(x)[0],\n",
    "    'domain_token_count': get_domain_features(x)[1],\n",
    "    'largest_domain_token_length': get_domain_features(x)[2],\n",
    "    'average_domain_token_length': get_domain_features(x)[3],\n",
    "    'word_count': get_word_count(x),\n",
    "    'is_https': is_https(x)\n",
    "}))\n",
    "\n",
    "# Concatenate original DF with features\n",
    "balanced_df = pd.concat([balanced_df, features], axis=1)\n",
    "print('Balanced Dataset Shape:', balanced_df.shape)\n",
    "print('Balanced Dataset:')\n",
    "print(balanced_df)\n",
    "\n",
    "# Extracting TF-IDF features from URLs\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to top 5000 features\n",
    "\n",
    "# Fit and transform the TF-IDF vectorizer\n",
    "print('Fitting and transforming the TF-IDF Vectorizer (max_features=5000) to the URLs in the dataset...')\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(balanced_df['URL'])\n",
    "print('TF-IDF Vectorization complete.')\n",
    "\n",
    "# Convert TF-IDF features from a sparse matrix to a dense format and then to an np.ndarray\n",
    "tfidf_dense = np.asarray(tfidf_features.todense())\n",
    "\n",
    "# Define X for numerical features\n",
    "X_numerical = balanced_df.drop(['Label', 'URL'], axis=1).values  # Make sure this matches your feature extraction output\n",
    "\n",
    "# Combining TF-IDF features with numerical features\n",
    "X_combined = np.hstack((X_numerical, tfidf_dense))\n",
    "\n",
    "# Define y\n",
    "y = balanced_df['Label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Saving the TF-IDF Vectorizer to disk...')\n",
    "\n",
    "# Save the TF-IDF vectorizer to disk\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pkl.dump(tfidf_vectorizer, f)\n",
    "\n",
    "print('TF-IDF Vectorizer saved to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f715b",
   "metadata": {},
   "source": [
    "### Logistic Regression model\n",
    "We will use the logistic regression model to classify the URLs and evaluate the model using the test set. We'll use 10,000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b38e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Logistic Regression model with max_iter=10000 and random_state=42...\n",
      "Logistic Regression model training complete, making predictions...\n",
      "Accuracy: 0.9752333333333333\n",
      "Confusion Matrix:\n",
      " [[29825   237]\n",
      " [ 1249 28689]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     30062\n",
      "           1       0.99      0.96      0.97     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n",
      "Saving the Logistic Regression model to disk...\n",
      "Logistic Regression model saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# Model Training with Logistic Regression\n",
    "print('Training the Logistic Regression model with max_iter=10000 and random_state=42...')\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=10000) # Increase max_iter if the model does not converge\n",
    "lr_model.fit(X_train, y_train)\n",
    "print('Logistic Regression model training complete, making predictions...')\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "print('Saving the Logistic Regression model to disk...')\n",
    "\n",
    "# Save the model to disk\n",
    "with open('logistic_regression_model.pkl', 'wb') as f:\n",
    "    pkl.dump(lr_model, f)\n",
    "\n",
    "print('Logistic Regression model saved to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4ee1a",
   "metadata": {},
   "source": [
    "### Guassian Naive Bayes model\n",
    "We will use the Guassian Naive Bayes model to classify the URLs and evaluate the model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1fd6ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Gaussian Naive Bayes model with the combined features...\n",
      "Gaussian Naive Bayes model training complete, making predictions...\n",
      "Accuracy: 0.95715\n",
      "Confusion Matrix:\n",
      " [[28957  1105]\n",
      " [ 1466 28472]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96     30062\n",
      "           1       0.96      0.95      0.96     29938\n",
      "\n",
      "    accuracy                           0.96     60000\n",
      "   macro avg       0.96      0.96      0.96     60000\n",
      "weighted avg       0.96      0.96      0.96     60000\n",
      "\n",
      "Saving the Gaussian Naive Bayes model to disk...\n",
      "Gaussian Naive Bayes model saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# Model Training with GaussianNB\n",
    "print('Training the Gaussian Naive Bayes model with the combined features...')\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "print('Gaussian Naive Bayes model training complete, making predictions...')\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = gnb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "print('Saving the Gaussian Naive Bayes model to disk...')\n",
    "\n",
    "# Save the model to disk\n",
    "with open('gaussian_nb_model.pkl', 'wb') as f:\n",
    "    pkl.dump(gnb_model, f)\n",
    "\n",
    "print('Gaussian Naive Bayes model saved to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8599d3",
   "metadata": {},
   "source": [
    "### Random Forest model\n",
    "Next, we will use the random forest model to classify the URLs and evaluate the model using the test set. The model will use 100 estimators (trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dca287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Random Forest model with 100 estimators and a random state of 42...\n",
      "Random Forest model training complete, making predictions...\n",
      "Accuracy: 0.9786\n",
      "Confusion Matrix:\n",
      " [[29782   280]\n",
      " [ 1004 28934]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     30062\n",
      "           1       0.99      0.97      0.98     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n",
      "Saving the Random Forest model to disk...\n",
      "Random Forest model saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# Model Training with RandomForestClassifier\n",
    "print('Training the Random Forest model with 100 estimators and a random state of 42...')\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "print('Random Forest model training complete, making predictions...')\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "print('Saving the Random Forest model to disk...')\n",
    "\n",
    "# Save the model to disk\n",
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pkl.dump(rf_model, f)\n",
    "\n",
    "print('Random Forest model saved to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584e5b1",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors model\n",
    "We will use K-nearest neighbors model to classify the URLs and evaluate the model using the test set. We'll use 490 neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd934ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the K-Nearest Neighbors model with k=490...\n",
      "K-Nearest Neighbors model training complete, making predictions...\n",
      "Accuracy: 0.9156166666666666\n",
      "Confusion Matrix:\n",
      " [[29438   624]\n",
      " [ 4439 25499]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92     30062\n",
      "           1       0.98      0.85      0.91     29938\n",
      "\n",
      "    accuracy                           0.92     60000\n",
      "   macro avg       0.92      0.92      0.92     60000\n",
      "weighted avg       0.92      0.92      0.92     60000\n",
      "\n",
      "Saving the K-Nearest Neighbors model to disk...\n",
      "K-Nearest Neighbors model saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors Classifier\n",
    "print('Training the K-Nearest Neighbors model with k=490...')\n",
    "knn_model = KNeighborsClassifier(n_neighbors=490)  \n",
    "knn_model.fit(X_train, y_train)\n",
    "print('K-Nearest Neighbors model training complete, making predictions...')\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "print('Saving the K-Nearest Neighbors model to disk...')\n",
    "\n",
    "# Dump the model to disk\n",
    "with open('knn_model.pkl', 'wb') as f:\n",
    "    pkl.dump(knn_model, f)\n",
    "\n",
    "print('K-Nearest Neighbors model saved to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06bc9e8",
   "metadata": {},
   "source": [
    "### Decision Tree model\n",
    "We will use Decision Tree model to classify the URLs and evaluate the model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eccfbdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Decision Tree model with a random state of 42...\n",
      "Decision Tree model training complete, making predictions...\n",
      "Accuracy: 0.9743166666666667\n",
      "Confusion Matrix:\n",
      " [[29458   604]\n",
      " [  937 29001]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97     30062\n",
      "           1       0.98      0.97      0.97     29938\n",
      "\n",
      "    accuracy                           0.97     60000\n",
      "   macro avg       0.97      0.97      0.97     60000\n",
      "weighted avg       0.97      0.97      0.97     60000\n",
      "\n",
      "Saving the Decision Tree model to disk...\n",
      "Decision Tree model saved to disk.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "print('Training the Decision Tree model with a random state of 42...')\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print('Decision Tree model training complete, making predictions...')\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "print('Saving the Decision Tree model to disk...')\n",
    "\n",
    "# Save the model to disk\n",
    "with open('decision_tree_model.pkl', 'wb') as f:\n",
    "    pkl.dump(rf_model, f)\n",
    "\n",
    "print('Decision Tree model saved to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bcad1",
   "metadata": {},
   "source": [
    "## TensorFlow neural network, with feature extraction\n",
    "Now let's use a neural network to classify the URLs. We will use the feature extraction functions to extract features from the URLs and then use a neural network to classify the URLs. We will use the same training and test sets as before. As the same, the training set is 80% and the test set is 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a08f25",
   "metadata": {},
   "source": [
    "### Preprocessing for Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6492a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and sequence padding the URLs...\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and sequence padding parameters\n",
    "max_len = 100  # Adjust based on the length of the longest URL in your dataset\n",
    "max_words = 60000  # Adjust based on the size of your vocabulary\n",
    "\n",
    "print('Tokenizing and sequence padding the URLs...')\n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(balanced_df['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df['Label'])\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7517f2",
   "metadata": {},
   "source": [
    "### Using RNN model with feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db0b83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the RNN model...\n",
      "Compiling the RNN model...\n",
      "Training the RNN model with early stopping...\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 114s 73ms/step - loss: 0.1409 - acc: 0.9526 - val_loss: 0.0879 - val_acc: 0.9699\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 105s 70ms/step - loss: 0.0708 - acc: 0.9766 - val_loss: 0.0816 - val_acc: 0.9730\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 109s 73ms/step - loss: 0.0601 - acc: 0.9799 - val_loss: 0.0802 - val_acc: 0.9732\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 107s 71ms/step - loss: 0.0558 - acc: 0.9812 - val_loss: 0.0821 - val_acc: 0.9737\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 108s 72ms/step - loss: 0.0533 - acc: 0.9821 - val_loss: 0.0852 - val_acc: 0.9725\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 108s 72ms/step - loss: 0.0513 - acc: 0.9824 - val_loss: 0.0948 - val_acc: 0.9724\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 109s 73ms/step - loss: 0.0499 - acc: 0.9827 - val_loss: 0.0875 - val_acc: 0.9735\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 111s 74ms/step - loss: 0.0487 - acc: 0.9832 - val_loss: 0.0924 - val_acc: 0.9740\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 109s 72ms/step - loss: 0.0475 - acc: 0.9833 - val_loss: 0.0880 - val_acc: 0.9730\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 109s 72ms/step - loss: 0.0468 - acc: 0.9837 - val_loss: 0.0933 - val_acc: 0.9735\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 110s 73ms/step - loss: 0.0464 - acc: 0.9839 - val_loss: 0.0982 - val_acc: 0.9734\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 107s 71ms/step - loss: 0.0458 - acc: 0.9840 - val_loss: 0.0978 - val_acc: 0.9737\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 110s 74ms/step - loss: 0.0452 - acc: 0.9842 - val_loss: 0.1026 - val_acc: 0.9737\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 108s 72ms/step - loss: 0.0448 - acc: 0.9844 - val_loss: 0.0986 - val_acc: 0.9729\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 110s 73ms/step - loss: 0.0446 - acc: 0.9843 - val_loss: 0.0974 - val_acc: 0.9734\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 110s 73ms/step - loss: 0.0440 - acc: 0.9846 - val_loss: 0.1040 - val_acc: 0.9735\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 106s 71ms/step - loss: 0.0437 - acc: 0.9847 - val_loss: 0.0953 - val_acc: 0.9731\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 106s 71ms/step - loss: 0.0436 - acc: 0.9847 - val_loss: 0.1092 - val_acc: 0.9735\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 108s 72ms/step - loss: 0.0434 - acc: 0.9847 - val_loss: 0.0986 - val_acc: 0.9735\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 107s 71ms/step - loss: 0.0432 - acc: 0.9849 - val_loss: 0.1098 - val_acc: 0.9734\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 111s 74ms/step - loss: 0.0429 - acc: 0.9849 - val_loss: 0.1091 - val_acc: 0.9723\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 108s 72ms/step - loss: 0.0426 - acc: 0.9851 - val_loss: 0.1097 - val_acc: 0.9735\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 109s 73ms/step - loss: 0.0424 - acc: 0.9850 - val_loss: 0.1118 - val_acc: 0.9732\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 106s 70ms/step - loss: 0.0426 - acc: 0.9850 - val_loss: 0.1045 - val_acc: 0.9741\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 105s 70ms/step - loss: 0.0422 - acc: 0.9851 - val_loss: 0.1176 - val_acc: 0.9737\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 106s 71ms/step - loss: 0.0489 - acc: 0.9838 - val_loss: 0.1077 - val_acc: 0.9734\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 102s 68ms/step - loss: 0.0427 - acc: 0.9851 - val_loss: 0.1164 - val_acc: 0.9739\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 107s 71ms/step - loss: 0.0417 - acc: 0.9853 - val_loss: 0.1062 - val_acc: 0.9734\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 108s 72ms/step - loss: 0.0418 - acc: 0.9854 - val_loss: 0.1104 - val_acc: 0.9738\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 109s 73ms/step - loss: 0.0416 - acc: 0.9853 - val_loss: 0.1151 - val_acc: 0.9733\n",
      "RNN model training complete, making predictions...\n",
      "1875/1875 [==============================] - 46s 24ms/step\n",
      "Accuracy: 0.9728\n",
      "Confusion Matrix:\n",
      " [[29543   519]\n",
      " [ 1113 28825]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     30062\n",
      "           1       0.98      0.96      0.97     29938\n",
      "\n",
      "    accuracy                           0.97     60000\n",
      "   macro avg       0.97      0.97      0.97     60000\n",
      "weighted avg       0.97      0.97      0.97     60000\n",
      "\n",
      "Saving the RNN model to disk...\n",
      "RNN model saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# RNN model definition\n",
    "print('Defining the RNN model...')\n",
    "RNN_model = tf.keras.models.Sequential()\n",
    "RNN_model.add(tf.keras.layers.Embedding(max_words, 32, input_length=max_len))\n",
    "RNN_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)))\n",
    "RNN_model.add(tf.keras.layers.Dropout(0.5))\n",
    "RNN_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)))\n",
    "RNN_model.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "RNN_model.add(tf.keras.layers.Dropout(0.5))\n",
    "RNN_model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "print('Compiling the RNN model...')\n",
    "RNN_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=30, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping, using the GPU if available (requires TensorFlow-GPU), or the CPU otherwise, for 30 epochs\n",
    "# If the model does not improve for 3 consecutive epochs, training will stop early\n",
    "print('Training the RNN model with early stopping...')\n",
    "history = RNN_model.fit(X_train, y_train, epochs=3, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "print('RNN model training complete, making predictions...')\n",
    "\n",
    "# Predictions\n",
    "y_pred = RNN_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))\n",
    "\n",
    "print('Saving the RNN model to disk...')\n",
    "\n",
    "# Save the model to disk\n",
    "RNN_model.save('rnn_model.h5')\n",
    "\n",
    "print('RNN model saved to disk.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0dfe34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
