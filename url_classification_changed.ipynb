{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac17c243",
   "metadata": {},
   "source": [
    "# URL Classification model using Machine Learning\n",
    "### For Computer Science B.Sc. Ariel University\n",
    "\n",
    "**By Yuval Yurzdichansky, Matan Aviv and Roy Simanovich**\n",
    "\n",
    "## Introduction\n",
    "In this project we will build a machine learning model that will classify URLs based on if they are malicious (phishing, malware, etc.) or benign. We will use a dataset of URLs that are labeled as either malicious or benign and use it to train a model that will be able to classify new URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242fd86",
   "metadata": {},
   "source": [
    "### Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae9a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some importent libraries installtion via pip\n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4fd24ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siman\\AppData\\Local\\Temp\\ipykernel_7220\\1534196349.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries for the project #\n",
    "\n",
    "## Regular Libraries that's come with python\n",
    "### Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "### Numerical Computing\n",
    "import numpy as np\n",
    "### Collection of Data Structures\n",
    "import collections as col\n",
    "### Regular Expressions\n",
    "import re as regex\n",
    "### URL Handling\n",
    "import urllib as urlhndl\n",
    "### Mathematical Operations\n",
    "import math\n",
    "### Socket Programming\n",
    "import socket\n",
    "\n",
    "## SKLearn Libraries for Machine Learning\n",
    "### Splitting the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "### Metrics for Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "### Preprocessing the Data\n",
    "from sklearn.utils import resample\n",
    "### TF-IDF Vectorizer for Text Data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "### Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "### Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "### Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "### K-Nearest Neighbors Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## Tensorflow Libraries for Deep Learning\n",
    "import tensorflow as tf\n",
    "\n",
    "## Pickle for saving the model to disk\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83fb0b",
   "metadata": {},
   "source": [
    "### Feature extraction functions\n",
    "We will use the following feature extraction functions to extract features from the URLs:\n",
    "1. `get_url_length`: This function returns the length of the URL.\n",
    "2. `get_dot_count`: This function returns the number of dots in the URL.\n",
    "3. `get_hyphen_count_in_domain`: This function returns the number of hyphens in the domain of the URL.\n",
    "4. `contains_security_sensitive_words`: This function returns 1 if the URL contains security sensitive words (e.g., \"login\", \"signin\", \"account\", etc.) and 0 otherwise.\n",
    "5. `get_directory_length`: This function returns the length of the directory part of the URL.\n",
    "6. `get_sub_directory_count`: This function returns the number of subdirectories in the URL.\n",
    "7. `contains_ip`: This function returns 1 if the URL contains an IP address and 0 otherwise.\n",
    "8. `get_token_count_in_path`: This function returns the number of tokens in the path part of the URL.\n",
    "9. `get_largest_token_length`: This function returns the length of the largest token in the path part of the URL.\n",
    "10. `get_avarage_token_length`: This function returns the average length of the tokens in the path part of the URL.\n",
    "11. `get_file_length`: This function returns the length of the file part of the URL.\n",
    "12. `get_dot_count_in_file`: This function returns the number of dots in the file part of the URL.\n",
    "13. `get_delimiter_count_in_file`: This function returns the number of delimiters in the file part of the URL.\n",
    "14. `get_arguments_length`: This function returns the length of the arguments part of the URL.\n",
    "15. `get_number_of_arguments`: This function returns the number of arguments in the URL.\n",
    "16. `get_length_of_largest_argument_value`: This function returns the length of the largest argument value in the URL.\n",
    "17. `get_max_delimiters_in_arguments`: This function returns the maximum number of delimiters in the arguments part of the URL.\n",
    "18. `get_domain_features`: This function returns the following features of the domain part of the URL: length, number of dots, number of hyphens, number of digits, number of special characters, entropy, whether the domain is a subdomain, and whether the domain is an IP address.\n",
    "19. `get_special_character_count`: This function returns the number of special characters in the URL.\n",
    "20. `get_entropy`: This function returns the entropy of the URL.\n",
    "21. `check_url_shortened`: This function returns 1 if the URL is shortened and 0 otherwise.\n",
    "22. `get_port_number`: This function returns the port number of the URL, if it exists, and -1 otherwise.\n",
    "23. `get_subdomain_count`: This function returns the number of subdomains in the URL.\n",
    "24. `get_suspicious_tld`: This function returns 1 if the top-level domain (TLD) of the URL is suspicious (e.g., \"tk\", \"ml\", \"ga\", etc.) and 0 otherwise.\n",
    "25. `get_numeric_ratio`: This function returns the ratio of numeric characters in the URL.\n",
    "26. `get_word_count`: This function returns the number of words in the URL.\n",
    "27. `get_url_is_internationalized`: This function returns 1 if the URL is internationalized and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86207970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to concatenate \"https://\" to benign URLs \n",
    "def add_https(url, label):\n",
    "    if label == 0:\n",
    "        return \"https://\" + url\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "def ensure_scheme(url):\n",
    "    if not urlhndl.parse.urlparse(url).scheme:\n",
    "        url = 'https://' + url\n",
    "    return url\n",
    "\n",
    "# Feature extraction functions\n",
    "def get_url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "def get_dot_count(url):\n",
    "    return url.count('.')\n",
    "\n",
    "def contains_security_sensitive_words(url):\n",
    "    security_sensitive_words = [\n",
    "    'login', 'password', 'admin', 'root', 'secret', 'private', 'secure', 'confidential', \n",
    "    'bank', 'creditcard', 'account', 'authentication', 'authorization', 'session', 'token', \n",
    "    'apikey', 'ssl', 'https', 'secure', 'encrypted', 'auth', 'signin', 'signup', 'verification', \n",
    "    'resetpassword', 'change-password', 'forgot-password', 'otp', '2fa', 'phishing', 'malware', \n",
    "    'virus', 'trojan', 'exploit', 'hacker', 'attack', 'security', 'vulnerable', 'injection', \n",
    "    'xss', 'csrf', 'dos', 'ddos', 'bruteforce', 'firewall', 'vpn', 'proxy', 'tor', 'security-question', \n",
    "    'privacy-policy']\n",
    "    return int(any(word in url for word in security_sensitive_words))\n",
    "\n",
    "def get_directory_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    return len(path)\n",
    "\n",
    "def get_sub_directory_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    return path.count('/') - 1\n",
    "\n",
    "def get_token_count_in_path(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    return len(tokens) - 1\n",
    "\n",
    "def get_largest_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    if tokens:\n",
    "        return max(len(token) for token in tokens)\n",
    "    return 0\n",
    "\n",
    "def get_average_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    tokens = [token for token in path.split('/') if token]\n",
    "    if tokens:\n",
    "        return np.mean([len(token) for token in tokens])\n",
    "    return 0\n",
    "\n",
    "def get_file_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return len(filename)\n",
    "\n",
    "def get_dot_count_in_file(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return filename.count('.')\n",
    "\n",
    "def get_delimiter_count_in_file(url):\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    delimiters = ['.', '_', '-']\n",
    "    return sum(filename.count(delimiter) for delimiter in delimiters)\n",
    "\n",
    "def get_arguments_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    return len(query)\n",
    "\n",
    "def get_number_of_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    return len(urlhndl.parse.parse_qs(query))\n",
    "\n",
    "def get_length_of_largest_argument_value(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    params = urlhndl.parse.parse_qs(query)\n",
    "    if params:\n",
    "        return max(len(max(values, key=len)) for values in params.values())\n",
    "    return 0\n",
    "\n",
    "def get_max_delimiters_in_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    params = urlhndl.parse.parse_qs(query)\n",
    "    delimiters = ['&', '=', '-', '_']\n",
    "    if params:\n",
    "        return max(sum(value.count(delimiter) for delimiter in delimiters) for values in params.values() for value in values)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_hyphen_count_in_domain(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(url).netloc\n",
    "    return domain.count('-')\n",
    "\n",
    "def contains_ip(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(url).netloc\n",
    "    try:\n",
    "        socket.inet_aton(domain)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def get_domain_features(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(ensure_scheme(url)).netloc\n",
    "    tokens = domain.split('.')\n",
    "    \n",
    "    # Domain Length\n",
    "    domain_length = len(domain)\n",
    "    \n",
    "    # Count of Tokens in the Domain\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Length of Largest Token in the Domain\n",
    "    largest_token_length = max(len(token) for token in tokens) if tokens else 0\n",
    "    \n",
    "    # Average Domain Token Length\n",
    "    average_token_length = sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n",
    "    \n",
    "    return domain_length, token_count, largest_token_length, average_token_length\n",
    "\n",
    "# New feature extraction functions\n",
    "def get_special_character_count(url):\n",
    "    special_characters = ['@', '=', '+', '*', '?', '&', '%', '$', '#', '!']\n",
    "    return sum(url.count(char) for char in special_characters)\n",
    "\n",
    "def get_entropy(url):\n",
    "    # Count the frequency of each character in the string\n",
    "    freq = col.Counter(url)\n",
    "    # Calculate the probabilities\n",
    "    probs = [count / len(url) for count in freq.values()]\n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -sum(p * math.log(p, 2) for p in probs if p > 0)\n",
    "    return entropy\n",
    "\n",
    "def check_url_shortened(url):\n",
    "    shortened_services = ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co']\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(url).netloc\n",
    "    return int(domain in shortened_services)\n",
    "\n",
    "def get_subdomain_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain_parts = urlhndl.parse.urlparse(url).netloc.split('.')\n",
    "    # Count as subdomains any parts beyond the second-level domain and TLD\n",
    "    return max(0, len(domain_parts) - 2)\n",
    "\n",
    "def get_suspicious_tld(url):\n",
    "    suspicious_tlds = ['xyz', 'top', 'loan', 'win', 'club']\n",
    "    url = ensure_scheme(url)\n",
    "    tld = urlhndl.parse.urlparse(url).netloc.split('.')[-1]\n",
    "    return int(tld in suspicious_tlds)\n",
    "\n",
    "def get_numeric_ratio(url):\n",
    "    numeric_chars = sum(c.isdigit() for c in url)\n",
    "    return numeric_chars / len(url) if len(url) > 0 else 0\n",
    "\n",
    "def get_word_count(url):\n",
    "    words = regex.findall(r'\\w+', url)\n",
    "    return len(words)\n",
    "\n",
    "def is_https(url, timeout=0.5):\n",
    "    return int(url.startswith(\"https\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29f9c7",
   "metadata": {},
   "source": [
    "### Now load the dataset and do some preprocessing on it\n",
    "The dataset is a CSV file with two columns: `url` and `label`. The `url` column contains the URL and the `label` column contains the label of the URL. The label is 1 if the URL is malicious and 0 if the URL is benign. Since the dataset is huge (more than 1 million rows), we will only use a small subset of it for this project (150,000 rows for each class, 300,000 rows in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "009c0099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Dataset Shape: (300000, 2)\n",
      "Balanced Dataset Distribution: Label\n",
      "1    150000\n",
      "0    150000\n",
      "Name: count, dtype: int64\n",
      "Balanced Dataset Head (First 10 Rows):\n",
      "                                                 URL  Label\n",
      "0  http://etransfers.interac.ca-ssl.net/sh/2o05I9...      1\n",
      "1  http://betterhealthsmoothies.com/Adobe/adobe-3...      1\n",
      "2  http://lloydsbank.deregister-payee-secure-auth...      1\n",
      "3                                 https://archive.md      0\n",
      "4                      https://pkg00-atx.netgate.com      0\n",
      "5  https://www.shareholds.com/gbr/5df72f77-f30d-4...      1\n",
      "6                  https://sub-166-141-241.myvzw.com      0\n",
      "7     https://hgdggdgfghygsugfytsfgssytstys.gq/83cbc      1\n",
      "8                    https://www.commbanknetcode.com      1\n",
      "9                    https://autodiscover.ons.gov.uk      0\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('url_dataset_updated.csv')\n",
    "\n",
    "# Drop all duplicates from df\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Reset the index after dropping duplicates\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Name of the URL column, change this if your dataset has a different column name\n",
    "url_column_name = 'URL'\n",
    "\n",
    "# Apply the function to the URL column\n",
    "df[url_column_name] = df.apply(lambda row: add_https(row[url_column_name], row['Label']), axis=1)\n",
    "\n",
    "# Separate the dataset into malicious and benign\n",
    "malicious_df = df[df['Label'] == 1]\n",
    "benign_df = df[df['Label'] == 0]\n",
    "\n",
    "# Randomly sample 150,000 entries from each\n",
    "malicious_sampled_df = resample(malicious_df, n_samples=150000, random_state=42)\n",
    "benign_sampled_df = resample(benign_df, n_samples=150000, random_state=42)\n",
    "\n",
    "# Combine the sampled data\n",
    "balanced_df = pd.concat([malicious_sampled_df, benign_sampled_df])\n",
    "\n",
    "# Shuffle the combined dataset to mix malicious and benign URLs\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print('Balanced Dataset Shape:', balanced_df.shape)\n",
    "print('Balanced Dataset Distribution:', balanced_df['Label'].value_counts())\n",
    "print('Balanced Dataset Head (First 10 Rows):')\n",
    "print(balanced_df.head(10))\n",
    "\n",
    "# Now, balanced_df contains the balanced dataset ready for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d87432",
   "metadata": {},
   "source": [
    "## Start testing with the options\n",
    "**NOTE:** We'll split the truncated dataset into a training set and a test set (80% training, 20% test) and use the training set to train the model and the test set to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472900a3",
   "metadata": {},
   "source": [
    "### Feature extraction preprocessing\n",
    "We'll get the features from the URLs using the feature extraction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8fa3db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction...\n",
      "Tokenizing URLs...\n",
      "Tokenization complete.\n",
      "Balanced Dataset Shape: (300000, 31)\n",
      "Balanced Dataset:\n",
      "                                                      URL  Label  url_length  \\\n",
      "0       http etransfers interac ca ssl net sh 2o05I9 b...      1        65.0   \n",
      "1       http betterhealthsmoothies com Adobe adobe 3D6...      1        58.0   \n",
      "2       http lloydsbank deregister payee secure auth c...      1        60.0   \n",
      "3                                        https archive md      0        18.0   \n",
      "4                             https pkg00 atx netgate com      0        29.0   \n",
      "...                                                   ...    ...         ...   \n",
      "299995         https infomation fb service e82443 repl co      1        44.0   \n",
      "299996                     https img 1000736 ad score com      0        32.0   \n",
      "299997                  http sosyalsat com help home html      1        35.0   \n",
      "299998  https storageapi fleek co 12678f8a 04f9 4b69 a...      1        83.0   \n",
      "299999  https trepievirealestate com paintdesk auth 1 ...      1       294.0   \n",
      "\n",
      "        dot_count  hyphen_count_domain  security_sensitive_words  \\\n",
      "0             4.0                  1.0                       1.0   \n",
      "1             2.0                  0.0                       0.0   \n",
      "2             3.0                  3.0                       1.0   \n",
      "3             1.0                  0.0                       1.0   \n",
      "4             2.0                  1.0                       1.0   \n",
      "...           ...                  ...                       ...   \n",
      "299995        3.0                  2.0                       1.0   \n",
      "299996        2.0                  2.0                       1.0   \n",
      "299997        2.0                  0.0                       0.0   \n",
      "299998        3.0                  0.0                       1.0   \n",
      "299999        2.0                  0.0                       1.0   \n",
      "\n",
      "        directory_length  sub_directory_count  token_count_path  \\\n",
      "0                   29.0                  3.0               4.0   \n",
      "1                   26.0                  2.0               3.0   \n",
      "2                   10.0                  0.0               1.0   \n",
      "3                    0.0                 -1.0               0.0   \n",
      "4                    0.0                 -1.0               0.0   \n",
      "...                  ...                  ...               ...   \n",
      "299995               0.0                 -1.0               0.0   \n",
      "299996               0.0                 -1.0               0.0   \n",
      "299997              15.0                  1.0               2.0   \n",
      "299998              56.0                  1.0               2.0   \n",
      "299999              33.0                  3.0               4.0   \n",
      "\n",
      "        largest_token_length  ...  url_shortened  subdomain_count  \\\n",
      "0                       12.0  ...            0.0              2.0   \n",
      "1                        9.0  ...            0.0              0.0   \n",
      "2                        9.0  ...            0.0              1.0   \n",
      "3                        0.0  ...            0.0              0.0   \n",
      "4                        0.0  ...            0.0              1.0   \n",
      "...                      ...  ...            ...              ...   \n",
      "299995                   0.0  ...            0.0              2.0   \n",
      "299996                   0.0  ...            0.0              1.0   \n",
      "299997                   9.0  ...            0.0              0.0   \n",
      "299998                  43.0  ...            0.0              1.0   \n",
      "299999                  15.0  ...            0.0              0.0   \n",
      "\n",
      "        suspicious_tld  numeric_ratio  domain_length  domain_token_count  \\\n",
      "0                  0.0       0.061538           29.0                 4.0   \n",
      "1                  0.0       0.034483           25.0                 2.0   \n",
      "2                  0.0       0.000000           43.0                 3.0   \n",
      "3                  0.0       0.000000           10.0                 2.0   \n",
      "4                  0.0       0.068966           21.0                 3.0   \n",
      "...                ...            ...            ...                 ...   \n",
      "299995             0.0       0.113636           36.0                 4.0   \n",
      "299996             0.0       0.218750           24.0                 3.0   \n",
      "299997             0.0       0.000000           13.0                 2.0   \n",
      "299998             0.0       0.289157           19.0                 3.0   \n",
      "299999             0.0       0.479592           22.0                 2.0   \n",
      "\n",
      "        largest_domain_token_length  average_domain_token_length  word_count  \\\n",
      "0                              10.0                     6.500000        11.0   \n",
      "1                              21.0                    12.000000         8.0   \n",
      "2                              28.0                    13.666667         9.0   \n",
      "3                               7.0                     4.500000         3.0   \n",
      "4                               9.0                     6.333333         5.0   \n",
      "...                             ...                          ...         ...   \n",
      "299995                         21.0                     8.250000         7.0   \n",
      "299996                         11.0                     7.333333         6.0   \n",
      "299997                          9.0                     6.000000         6.0   \n",
      "299998                         10.0                     5.666667        12.0   \n",
      "299999                         18.0                    10.500000        15.0   \n",
      "\n",
      "        is_https  \n",
      "0            0.0  \n",
      "1            0.0  \n",
      "2            0.0  \n",
      "3            1.0  \n",
      "4            1.0  \n",
      "...          ...  \n",
      "299995       1.0  \n",
      "299996       1.0  \n",
      "299997       0.0  \n",
      "299998       1.0  \n",
      "299999       1.0  \n",
      "\n",
      "[300000 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply feature extraction\n",
    "print('Feature extraction...')\n",
    "features = balanced_df['URL'].apply(lambda x: pd.Series({\n",
    "    'url_length': get_url_length(x),\n",
    "    'dot_count': get_dot_count(x),\n",
    "    'hyphen_count_domain': get_hyphen_count_in_domain(x),\n",
    "    'security_sensitive_words': contains_security_sensitive_words(x),\n",
    "    'directory_length': get_directory_length(x),\n",
    "    'sub_directory_count': get_sub_directory_count(x),\n",
    "    'token_count_path': get_token_count_in_path(x),\n",
    "    'largest_token_length': get_largest_token_length(x),\n",
    "    'average_token_length': get_average_token_length(x),\n",
    "    'file_length': get_file_length(x),\n",
    "    'contains_ip': contains_ip(x),\n",
    "    'dot_count_in_file': get_dot_count_in_file(x),\n",
    "    'delimiter_count_in_file': get_delimiter_count_in_file(x),\n",
    "    'arguments_length': get_arguments_length(x),\n",
    "    'number_of_arguments': get_number_of_arguments(x),\n",
    "    'length_of_largest_argument_value': get_length_of_largest_argument_value(x),\n",
    "    'max_delimiters_in_arguments': get_max_delimiters_in_arguments(x),\n",
    "    'special_character_count': get_special_character_count(x),\n",
    "    'entropy': get_entropy(x),\n",
    "    'url_shortened': check_url_shortened(x),\n",
    "    'subdomain_count': get_subdomain_count(x),\n",
    "    'suspicious_tld': get_suspicious_tld(x),\n",
    "    'numeric_ratio': get_numeric_ratio(x),\n",
    "    'domain_length': get_domain_features(x)[0],\n",
    "    'domain_token_count': get_domain_features(x)[1],\n",
    "    'largest_domain_token_length': get_domain_features(x)[2],\n",
    "    'average_domain_token_length': get_domain_features(x)[3],\n",
    "    'word_count': get_word_count(x),\n",
    "    'is_https': is_https(x)\n",
    "}))\n",
    "\n",
    "# Concatenate original DF with features\n",
    "balanced_df = pd.concat([balanced_df, features], axis=1)\n",
    "\n",
    "# Regex tokenization function\n",
    "def tokenize_url(url):\n",
    "    # Tokenize the URL using regex\n",
    "    tokens = regex.findall(r'\\w+', url)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Tokenize the URLs\n",
    "print('Tokenizing URLs...')\n",
    "balanced_df['URL'] = balanced_df['URL'].apply(tokenize_url)\n",
    "print('Tokenization complete.')\n",
    "\n",
    "print('Balanced Dataset Shape:', balanced_df.shape)\n",
    "print('Balanced Dataset:')\n",
    "print(balanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ce677",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer preprocessing\n",
    "We will start by using the TF-IDF vectorizer to convert the URLs into numerical features for preprocessing, and split the dataset into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06fa5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting and transforming the TF-IDF Vectorizer (max_features=5000) to the URLs in the dataset...\n",
      "TF-IDF Vectorization complete.\n",
      "Converting TF-IDF features from a sparse matrix to a dense format and then to an np.ndarray...\n",
      "Conversion complete.\n",
      "Combining TF-IDF features with numerical features...\n",
      "Combination complete.\n",
      "Saving the TF-IDF Vectorizer to disk...\n",
      "TF-IDF Vectorizer saved to disk.\n",
      "Splitting the data into training and testing sets...\n",
      "Data split complete.\n"
     ]
    }
   ],
   "source": [
    "# Extracting TF-IDF features from URLs\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to top 5000 features\n",
    "\n",
    "# Fit and transform the TF-IDF vectorizer\n",
    "print('Fitting and transforming the TF-IDF Vectorizer (max_features=5000) to the URLs in the dataset...')\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(balanced_df['URL'])\n",
    "print('TF-IDF Vectorization complete.')\n",
    "\n",
    "# Convert TF-IDF features from a sparse matrix to a dense format and then to an np.ndarray\n",
    "print('Converting TF-IDF features from a sparse matrix to a dense format and then to an np.ndarray...')\n",
    "tfidf_dense = np.asarray(tfidf_features.todense())\n",
    "print('Conversion complete.')\n",
    "\n",
    "# Define X for numerical features\n",
    "X_numerical = balanced_df.drop(['Label', 'URL'], axis=1).values  # Make sure this matches your feature extraction output\n",
    "\n",
    "# Combining TF-IDF features with numerical features\n",
    "print('Combining TF-IDF features with numerical features...')\n",
    "X_combined = np.hstack((X_numerical, tfidf_dense))\n",
    "print('Combination complete.')\n",
    "\n",
    "# Define y\n",
    "y = balanced_df['Label'].values\n",
    "\n",
    "# Save the TF-IDF vectorizer to disk\n",
    "print('Saving the TF-IDF Vectorizer to disk...')\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pkl.dump(tfidf_vectorizer, f)\n",
    "print('TF-IDF Vectorizer saved to disk.')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "print('Splitting the data into training and testing sets...')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "print('Data split complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f715b",
   "metadata": {},
   "source": [
    "### Logistic Regression model\n",
    "We will use the logistic regression model to classify the URLs and evaluate the model using the test set. We'll use 10,000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b38e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training with Logistic Regression\n",
    "print('Training the Logistic Regression model with max_iter=10000 and random_state=42...')\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=10000) # Increase max_iter if the model does not converge\n",
    "lr_model.fit(X_train, y_train)\n",
    "print('Logistic Regression model training complete, making predictions...')\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "print('Saving the Logistic Regression model to disk...')\n",
    "\n",
    "# Save the model to disk\n",
    "with open('logistic_regression_model.pkl', 'wb') as f:\n",
    "    pkl.dump(lr_model, f)\n",
    "\n",
    "print('Logistic Regression model saved to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4ee1a",
   "metadata": {},
   "source": [
    "### Guassian Naive Bayes model\n",
    "We will use the Guassian Naive Bayes model to classify the URLs and evaluate the model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training with GaussianNB\n",
    "print('Training the Gaussian Naive Bayes model with the combined features...')\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "print('Gaussian Naive Bayes model training complete, making predictions...')\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = gnb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "print('Saving the Gaussian Naive Bayes model to disk...')\n",
    "\n",
    "# Save the model to disk\n",
    "with open('gaussian_nb_model.pkl', 'wb') as f:\n",
    "    pkl.dump(gnb_model, f)\n",
    "\n",
    "print('Gaussian Naive Bayes model saved to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8599d3",
   "metadata": {},
   "source": [
    "### Random Forest model\n",
    "Next, we will use the random forest model to classify the URLs and evaluate the model using the test set. The model will use 100 estimators (trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dca287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Random Forest model with 100 estimators and a random state of 42...\n",
      "Random Forest model training complete, making predictions...\n",
      "Accuracy: 0.9786\n",
      "Confusion Matrix:\n",
      " [[29782   280]\n",
      " [ 1004 28934]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     30062\n",
      "           1       0.99      0.97      0.98     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n",
      "Testing the Random Forest model with different thresholds...\n",
      "Threshold: 0.05\n",
      "Accuracy: 0.8972\n",
      "Confusion Matrix:\n",
      " [[24148  5914]\n",
      " [  254 29684]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.80      0.89     30062\n",
      "           1       0.83      0.99      0.91     29938\n",
      "\n",
      "    accuracy                           0.90     60000\n",
      "   macro avg       0.91      0.90      0.90     60000\n",
      "weighted avg       0.91      0.90      0.90     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.1\n",
      "Accuracy: 0.942\n",
      "Confusion Matrix:\n",
      " [[26983  3079]\n",
      " [  401 29537]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94     30062\n",
      "           1       0.91      0.99      0.94     29938\n",
      "\n",
      "    accuracy                           0.94     60000\n",
      "   macro avg       0.95      0.94      0.94     60000\n",
      "weighted avg       0.95      0.94      0.94     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.15\n",
      "Accuracy: 0.9601333333333333\n",
      "Confusion Matrix:\n",
      " [[28183  1879]\n",
      " [  513 29425]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96     30062\n",
      "           1       0.94      0.98      0.96     29938\n",
      "\n",
      "    accuracy                           0.96     60000\n",
      "   macro avg       0.96      0.96      0.96     60000\n",
      "weighted avg       0.96      0.96      0.96     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.2\n",
      "Accuracy: 0.9676166666666667\n",
      "Confusion Matrix:\n",
      " [[28714  1348]\n",
      " [  595 29343]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     30062\n",
      "           1       0.96      0.98      0.97     29938\n",
      "\n",
      "    accuracy                           0.97     60000\n",
      "   macro avg       0.97      0.97      0.97     60000\n",
      "weighted avg       0.97      0.97      0.97     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.25\n",
      "Accuracy: 0.9719333333333333\n",
      "Confusion Matrix:\n",
      " [[29034  1028]\n",
      " [  656 29282]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97     30062\n",
      "           1       0.97      0.98      0.97     29938\n",
      "\n",
      "    accuracy                           0.97     60000\n",
      "   macro avg       0.97      0.97      0.97     60000\n",
      "weighted avg       0.97      0.97      0.97     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.3\n",
      "Accuracy: 0.9748333333333333\n",
      "Confusion Matrix:\n",
      " [[29276   786]\n",
      " [  724 29214]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97     30062\n",
      "           1       0.97      0.98      0.97     29938\n",
      "\n",
      "    accuracy                           0.97     60000\n",
      "   macro avg       0.97      0.97      0.97     60000\n",
      "weighted avg       0.97      0.97      0.97     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.35\n",
      "Accuracy: 0.9769166666666667\n",
      "Confusion Matrix:\n",
      " [[29464   598]\n",
      " [  787 29151]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98     30062\n",
      "           1       0.98      0.97      0.98     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.4\n",
      "Accuracy: 0.9781\n",
      "Confusion Matrix:\n",
      " [[29602   460]\n",
      " [  854 29084]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98     30062\n",
      "           1       0.98      0.97      0.98     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.45\n",
      "Accuracy: 0.9785833333333334\n",
      "Confusion Matrix:\n",
      " [[29693   369]\n",
      " [  916 29022]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     30062\n",
      "           1       0.99      0.97      0.98     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.5\n",
      "Accuracy: 0.9787333333333333\n",
      "Confusion Matrix:\n",
      " [[29773   289]\n",
      " [  987 28951]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     30062\n",
      "           1       0.99      0.97      0.98     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.55\n",
      "Accuracy: 0.9781166666666666\n",
      "Confusion Matrix:\n",
      " [[29822   240]\n",
      " [ 1073 28865]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     30062\n",
      "           1       0.99      0.96      0.98     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.6\n",
      "Accuracy: 0.9774\n",
      "Confusion Matrix:\n",
      " [[29869   193]\n",
      " [ 1163 28775]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     30062\n",
      "           1       0.99      0.96      0.98     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.65\n",
      "Accuracy: 0.9764833333333334\n",
      "Confusion Matrix:\n",
      " [[29908   154]\n",
      " [ 1257 28681]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     30062\n",
      "           1       0.99      0.96      0.98     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.7\n",
      "Accuracy: 0.9755166666666667\n",
      "Confusion Matrix:\n",
      " [[29953   109]\n",
      " [ 1360 28578]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98     30062\n",
      "           1       1.00      0.95      0.97     29938\n",
      "\n",
      "    accuracy                           0.98     60000\n",
      "   macro avg       0.98      0.98      0.98     60000\n",
      "weighted avg       0.98      0.98      0.98     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.75\n",
      "Accuracy: 0.97355\n",
      "Confusion Matrix:\n",
      " [[29990    72]\n",
      " [ 1515 28423]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97     30062\n",
      "           1       1.00      0.95      0.97     29938\n",
      "\n",
      "    accuracy                           0.97     60000\n",
      "   macro avg       0.97      0.97      0.97     60000\n",
      "weighted avg       0.97      0.97      0.97     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.8\n",
      "Accuracy: 0.9714\n",
      "Confusion Matrix:\n",
      " [[30012    50]\n",
      " [ 1666 28272]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97     30062\n",
      "           1       1.00      0.94      0.97     29938\n",
      "\n",
      "    accuracy                           0.97     60000\n",
      "   macro avg       0.97      0.97      0.97     60000\n",
      "weighted avg       0.97      0.97      0.97     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.85\n",
      "Accuracy: 0.9683333333333334\n",
      "Confusion Matrix:\n",
      " [[30029    33]\n",
      " [ 1867 28071]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97     30062\n",
      "           1       1.00      0.94      0.97     29938\n",
      "\n",
      "    accuracy                           0.97     60000\n",
      "   macro avg       0.97      0.97      0.97     60000\n",
      "weighted avg       0.97      0.97      0.97     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.9\n",
      "Accuracy: 0.9642\n",
      "Confusion Matrix:\n",
      " [[30046    16]\n",
      " [ 2132 27806]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97     30062\n",
      "           1       1.00      0.93      0.96     29938\n",
      "\n",
      "    accuracy                           0.96     60000\n",
      "   macro avg       0.97      0.96      0.96     60000\n",
      "weighted avg       0.97      0.96      0.96     60000\n",
      "\n",
      "\n",
      "\n",
      "Threshold: 0.95\n",
      "Accuracy: 0.9576166666666667\n",
      "Confusion Matrix:\n",
      " [[30052    10]\n",
      " [ 2533 27405]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     30062\n",
      "           1       1.00      0.92      0.96     29938\n",
      "\n",
      "    accuracy                           0.96     60000\n",
      "   macro avg       0.96      0.96      0.96     60000\n",
      "weighted avg       0.96      0.96      0.96     60000\n",
      "\n",
      "\n",
      "\n",
      "Maximum Accuracy: 0.9787333333333333\n",
      "Best Threshold: 0.5\n",
      "Saving the Random Forest model to disk...\n",
      "Random Forest model saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# Model Training with RandomForestClassifier\n",
    "print('Training the Random Forest model with 100 estimators and a random state of 42...')\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "print('Random Forest model training complete, making predictions...')\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "print('Testing the Random Forest model with different thresholds...')\n",
    "\n",
    "# Use threshold to improve the model. Testing all the thresholds from 0.1 to 0.9\n",
    "thresholds = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "\n",
    "max_accuracy = 0\n",
    "best_threshold = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    print('Threshold:', threshold)\n",
    "    y_pred = (rf_model.predict_proba(X_test)[:,1] >= threshold).astype(int)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print('\\n')\n",
    "\n",
    "    # Update max accuracy and best threshold if current accuracy is higher\n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        best_threshold = threshold\n",
    "\n",
    "print('Maximum Accuracy:', max_accuracy)\n",
    "print('Best Threshold:', best_threshold)\n",
    "\n",
    "print('Saving the Random Forest model to disk...')\n",
    "\n",
    "# Save the model to disk\n",
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pkl.dump(rf_model, f)\n",
    "\n",
    "print('Random Forest model saved to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584e5b1",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors model\n",
    "We will use K-nearest neighbors model to classify the URLs and evaluate the model using the test set. We'll use 490 neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd934ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors Classifier\n",
    "print('Training the K-Nearest Neighbors model with k=490...')\n",
    "knn_model = KNeighborsClassifier(n_neighbors=490)  \n",
    "knn_model.fit(X_train, y_train)\n",
    "print('K-Nearest Neighbors model training complete, making predictions...')\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "print('Saving the K-Nearest Neighbors model to disk...')\n",
    "\n",
    "# Dump the model to disk\n",
    "with open('knn_model.pkl', 'wb') as f:\n",
    "    pkl.dump(knn_model, f)\n",
    "\n",
    "print('K-Nearest Neighbors model saved to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06bc9e8",
   "metadata": {},
   "source": [
    "### Decision Tree model\n",
    "We will use Decision Tree model to classify the URLs and evaluate the model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccfbdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "print('Training the Decision Tree model with a random state of 42...')\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print('Decision Tree model training complete, making predictions...')\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "print('Saving the Decision Tree model to disk...')\n",
    "\n",
    "# Save the model to disk\n",
    "with open('decision_tree_model.pkl', 'wb') as f:\n",
    "    pkl.dump(rf_model, f)\n",
    "\n",
    "print('Decision Tree model saved to disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bcad1",
   "metadata": {},
   "source": [
    "## TensorFlow neural network, with feature extraction\n",
    "Now let's use a neural network to classify the URLs. We will use the feature extraction functions to extract features from the URLs and then use a neural network to classify the URLs. We will use the same training and test sets as before. As the same, the training set is 80% and the test set is 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a08f25",
   "metadata": {},
   "source": [
    "### Preprocessing for Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6492a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and sequence padding parameters\n",
    "max_len = 1000  # Maxiumum length of the sequence\n",
    "max_words = 240000  # 80% of the total vocabulary size (300,000)\n",
    "\n",
    "print('Tokenizing and sequence padding the URLs...')\n",
    "\n",
    "# Convert all columns to strings and concatenate them\n",
    "text_data = balanced_df.drop(['Label'], axis=1).astype(str).apply(lambda x: ' '.join(x), axis=1).values\n",
    "\n",
    "# Tokenize and sequence pad all features\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Pad the sequences\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Add TF-IDF features to the data\n",
    "#data = np.hstack((data, tfidf_dense))\n",
    "\n",
    "print('Tokenization and sequence padding complete.')\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df['Label'])\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7517f2",
   "metadata": {},
   "source": [
    "### Using RNN model with feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model definition\n",
    "print('Defining the RNN model...')\n",
    "RNN_model = tf.keras.models.Sequential()\n",
    "RNN_model.add(tf.keras.layers.Embedding(max_words, 32, input_length=max_len))\n",
    "RNN_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)))\n",
    "RNN_model.add(tf.keras.layers.Dropout(0.5))\n",
    "RNN_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)))\n",
    "RNN_model.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "RNN_model.add(tf.keras.layers.Dropout(0.5))\n",
    "RNN_model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "print('Compiling the RNN model...')\n",
    "RNN_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping, using the GPU if available (requires TensorFlow-GPU), or the CPU otherwise, for 30 epochs\n",
    "# If the model does not improve for 3 consecutive epochs, training will stop early\n",
    "print('Training the RNN model with early stopping...')\n",
    "\n",
    "# Force using CPU\n",
    "history = RNN_model.fit(X_train, y_train, epochs=30, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "print('RNN model training complete, making predictions...')\n",
    "\n",
    "# Predictions\n",
    "y_pred = RNN_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))\n",
    "\n",
    "print('Saving the RNN model to disk...')\n",
    "\n",
    "# Save the model to disk\n",
    "RNN_model.save('rnn_model.h5')\n",
    "\n",
    "print('RNN model saved to disk.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
