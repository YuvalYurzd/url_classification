{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac17c243",
   "metadata": {},
   "source": [
    "# URL Classification model using Machine Learning\n",
    "### For Computer Science B.Sc. Ariel University\n",
    "\n",
    "**By Yuval Yurzdichinsky, Matan Aviv and Roy Simanovich**\n",
    "\n",
    "## Introduction\n",
    "In this project we will build a machine learning model that will classify URLs based on if they are malicious (phishing, malware, etc.) or benign. We will use a dataset of URLs that are labeled as either malicious or benign and use it to train a model that will be able to classify new URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242fd86",
   "metadata": {},
   "source": [
    "### Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae9a164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gensim in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-whois in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: future in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from python-whois) (0.18.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tldextract in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: idna in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tldextract) (3.4)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tldextract) (2.31.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tldextract) (2.0.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from tldextract) (3.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from requests>=2.1.0->tldextract) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from requests>=2.1.0->tldextract) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (from requests>=2.1.0->tldextract) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: chardet in c:\\users\\siman\\anaconda3\\envs\\tf\\lib\\site-packages (5.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Some importent libraries installtion via pip\n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install gensim nltk\n",
    "%pip install matplotlib\n",
    "%pip install python-whois\n",
    "%pip install tldextract\n",
    "%pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4fd24ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siman\\AppData\\Local\\Temp\\ipykernel_2708\\1236473410.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "## Regular Libraries that's come with python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import socket as sock\n",
    "import math as m\n",
    "import re\n",
    "import whois as who\n",
    "import tldextract as tld\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "## URL Parsing\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "## SKLearn Libraries for Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "## SKLean metrics for model evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "## Tensorflow Libraries for Deep Learning\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83fb0b",
   "metadata": {},
   "source": [
    "### Feature extraction functions\n",
    "We will use the following feature extraction functions to extract features from the URLs:\n",
    "1. `get_url_length`: This function returns the length of the URL.\n",
    "2. `get_dot_count`: This function returns the number of dots in the URL.\n",
    "3. `get_hyphen_count_in_domain`: This function returns the number of hyphens in the domain of the URL.\n",
    "4. `contains_security_sensitive_words`: This function returns 1 if the URL contains security sensitive words (e.g., \"login\", \"signin\", \"account\", etc.) and 0 otherwise.\n",
    "5. `get_directory_length`: This function returns the length of the directory part of the URL.\n",
    "6. `get_sub_directory_count`: This function returns the number of subdirectories in the URL.\n",
    "7. `contains_ip`: This function returns 1 if the URL contains an IP address and 0 otherwise.\n",
    "8. `get_token_count_in_path`: This function returns the number of tokens in the path part of the URL.\n",
    "9. `get_largest_token_length`: This function returns the length of the largest token in the path part of the URL.\n",
    "10. `get_avarage_token_length`: This function returns the average length of the tokens in the path part of the URL.\n",
    "11. `get_file_length`: This function returns the length of the file part of the URL.\n",
    "12. `get_dot_count_in_file`: This function returns the number of dots in the file part of the URL.\n",
    "13. `get_delimiter_count_in_file`: This function returns the number of delimiters in the file part of the URL.\n",
    "14. `get_arguments_length`: This function returns the length of the arguments part of the URL.\n",
    "15. `get_number_of_arguments`: This function returns the number of arguments in the URL.\n",
    "16. `get_length_of_largest_argument_value`: This function returns the length of the largest argument value in the URL.\n",
    "17. `get_max_delimiters_in_arguments`: This function returns the maximum number of delimiters in the arguments part of the URL.\n",
    "18. `get_domain_features`: This function returns the following features of the domain part of the URL: length, number of dots, number of hyphens, number of digits, number of special characters, entropy, whether the domain is a subdomain, and whether the domain is an IP address.\n",
    "19. `get_special_character_count`: This function returns the number of special characters in the URL.\n",
    "20. `get_entropy`: This function returns the entropy of the URL.\n",
    "21. `check_url_shortened`: This function returns 1 if the URL is shortened and 0 otherwise.\n",
    "22. `get_port_number`: This function returns the port number of the URL, if it exists, and -1 otherwise.\n",
    "23. `get_subdomain_count`: This function returns the number of subdomains in the URL.\n",
    "24. `get_suspicious_tld`: This function returns 1 if the top-level domain (TLD) of the URL is suspicious (e.g., \"tk\", \"ml\", \"ga\", etc.) and 0 otherwise.\n",
    "25. `get_numeric_ratio`: This function returns the ratio of numeric characters in the URL.\n",
    "26. `get_word_count`: This function returns the number of words in the URL.\n",
    "27. `get_url_is_internationalized`: This function returns 1 if the URL is internationalized and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86207970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to ensure that the URL has a scheme (http:// or https://)\n",
    "def ensure_scheme(url):\n",
    "    if not urlparse(url).scheme:\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "# Feature extraction functions\n",
    "\n",
    "## Getting the length of the URL\n",
    "def get_url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "## Getting the dot count in the URL\n",
    "def get_dot_count(url):\n",
    "    return url.count('.')\n",
    "\n",
    "## Check if the URL contains security sensitive words\n",
    "def contains_security_sensitive_words(url):\n",
    "    security_sensitive_words = ['login', 'signin', 'auth', 'bank', 'update', 'account', 'verification', 'authenticate','authentication','verify','user']\n",
    "    return int(any(word in url for word in security_sensitive_words))\n",
    "\n",
    "## Get the directory length\n",
    "def get_directory_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return len(path)\n",
    "\n",
    "## Get the subdirectory count\n",
    "def get_sub_directory_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    return path.count('/') - 1\n",
    "\n",
    "## Get the token count in the path\n",
    "def get_token_count_in_path(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    return len(tokens) - 1\n",
    "\n",
    "## Get the length of the largest token in the path\n",
    "def get_largest_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    if tokens:\n",
    "        return max(len(token) for token in tokens)\n",
    "    return 0\n",
    "\n",
    "## Get the average token length in the path\n",
    "def get_average_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    tokens = [token for token in path.split('/') if token]\n",
    "    if tokens:\n",
    "        return np.mean([len(token) for token in tokens])\n",
    "    return 0\n",
    "\n",
    "## Get the file length\n",
    "def get_file_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return len(filename)\n",
    "\n",
    "## Get the dot count in the file\n",
    "def get_dot_count_in_file(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return filename.count('.')\n",
    "\n",
    "## Get the delimiter count in the file\n",
    "def get_delimiter_count_in_file(url):\n",
    "    path = urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    delimiters = ['.', '_', '-']\n",
    "    return sum(filename.count(delimiter) for delimiter in delimiters)\n",
    "\n",
    "## Get the arguments length\n",
    "def get_arguments_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(query)\n",
    "\n",
    "## Get the number of arguments\n",
    "def get_number_of_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    return len(parse_qs(query))\n",
    "\n",
    "## Get the length of the largest argument value\n",
    "def get_length_of_largest_argument_value(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    if params:\n",
    "        return max(len(max(values, key=len)) for values in params.values())\n",
    "    return 0\n",
    "\n",
    "## Get the average argument value length\n",
    "def get_max_delimiters_in_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlparse(url).query\n",
    "    params = parse_qs(query)\n",
    "    delimiters = ['&', '=', '-', '_']\n",
    "    if params:\n",
    "        return max(sum(value.count(delimiter) for delimiter in delimiters) for values in params.values() for value in values)\n",
    "    return 0\n",
    "\n",
    "## Get the hyphen count in the domain\n",
    "def get_hyphen_count_in_domain(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return domain.count('-')\n",
    "\n",
    "## Get the digit count in the domain\n",
    "def contains_ip(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    try:\n",
    "        sock.inet_aton(domain)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "## Get the domain length, token count, largest token length, and average token length\n",
    "def get_domain_features(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(ensure_scheme(url)).netloc\n",
    "    tokens = domain.split('.')\n",
    "    \n",
    "    # Domain Length\n",
    "    domain_length = len(domain)\n",
    "    \n",
    "    # Count of Tokens in the Domain\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Length of Largest Token in the Domain\n",
    "    largest_token_length = max(len(token) for token in tokens) if tokens else 0\n",
    "    \n",
    "    # Average Domain Token Length\n",
    "    average_token_length = sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n",
    "    \n",
    "    return domain_length, token_count, largest_token_length, average_token_length\n",
    "\n",
    "## Check if the URL contains special characters\n",
    "def get_special_character_count(url):\n",
    "    special_characters = ['@', '=', '+', '*', '?', '&', '%', '$', '#', '!']\n",
    "    return sum(url.count(char) for char in special_characters)\n",
    "\n",
    "## Get the entropy of the URL\n",
    "def get_entropy(url):\n",
    "    # Count the frequency of each character in the string\n",
    "    freq = Counter(url)\n",
    "    # Calculate the probabilities\n",
    "    probs = [count / len(url) for count in freq.values()]\n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -sum(p * m.log(p, 2) for p in probs if p > 0)\n",
    "    return entropy\n",
    "\n",
    "## Check if the URL is shortened\n",
    "def check_url_shortened(url):\n",
    "    shortened_services = ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co']\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlparse(url).netloc\n",
    "    return int(domain in shortened_services)\n",
    "\n",
    "## Get the port number, if specified\n",
    "def get_port_number(url):\n",
    "    url = ensure_scheme(url)\n",
    "    port = urlparse(url).port\n",
    "    return port if port else -1  # Return -1 if no port specified\n",
    "\n",
    "## Get the subdomain count\n",
    "def get_subdomain_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain_parts = urlparse(url).netloc.split('.')\n",
    "    # Count as subdomains any parts beyond the second-level domain and TLD\n",
    "    return max(0, len(domain_parts) - 2)\n",
    "\n",
    "## Check if the URL TLD is suspicious (e.g., .xyz, .top, .loan, .win, .club)\n",
    "def get_suspicious_tld(url):\n",
    "    suspicious_tlds = ['xyz', 'top', 'loan', 'win', 'club']\n",
    "    url = ensure_scheme(url)\n",
    "    tld = urlparse(url).netloc.split('.')[-1]\n",
    "    return int(tld in suspicious_tlds)\n",
    "\n",
    "## Get the ratio of numeric characters in the URL\n",
    "def get_numeric_ratio(url):\n",
    "    numeric_chars = sum(c.isdigit() for c in url)\n",
    "    return numeric_chars / len(url) if len(url) > 0 else 0\n",
    "\n",
    "## Get the word count in the URL\n",
    "def get_word_count(url):\n",
    "    words = re.findall(r'\\w+', url)\n",
    "    return len(words)\n",
    "\n",
    "## Check if the URL is internationalized (i.e., contains non-ASCII characters)\n",
    "def get_url_is_internationalized(url):\n",
    "    try:\n",
    "        url.encode('ascii')\n",
    "        return 0\n",
    "    except UnicodeEncodeError:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29f9c7",
   "metadata": {},
   "source": [
    "### Now load the dataset and do some preprocessing on it\n",
    "The dataset is a CSV file with two columns: `url` and `label`. The `url` column contains the URL and the `label` column contains the label of the URL. The label is 1 if the URL is malicious and 0 if the URL is benign. Since the dataset is huge (more than 1 million rows), we will only use a small subset of it for this project (150,000 rows for each class, 300,000 rows in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "009c0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('url_dataset_updated.csv')\n",
    "\n",
    "# Column name for the URL\n",
    "url_column_name = 'URL'\n",
    "\n",
    "# Remove 'http://' and 'https://' from all URLs\n",
    "df[url_column_name] = df[url_column_name].str.replace('http://', '', regex=False)\n",
    "df[url_column_name] = df[url_column_name].str.replace('https://', '', regex=False)\n",
    "\n",
    "# Separate the dataset into malicious and benign\n",
    "malicious_df = df[df['Label'] == 1]\n",
    "benign_df = df[df['Label'] == 0]\n",
    "\n",
    "# Randomly sample 150,000 entries from each\n",
    "malicious_sampled_df = resample(malicious_df, n_samples=150000, random_state=42)\n",
    "benign_sampled_df = resample(benign_df, n_samples=150000, random_state=42)\n",
    "\n",
    "# Combine the sampled data\n",
    "balanced_df = pd.concat([malicious_sampled_df, benign_sampled_df])\n",
    "\n",
    "# Shuffle the combined dataset to mix malicious and benign URLs\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Drop all duplicates from balanced_df\n",
    "balanced_df = balanced_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after dropping duplicates\n",
    "balanced_df = balanced_df.reset_index(drop=True)\n",
    "\n",
    "# Now, balanced_df contains the balanced dataset ready for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d87432",
   "metadata": {},
   "source": [
    "## Start testing with the options\n",
    "**NOTE:** We'll split the truncated dataset into a training set and a test set (80% training, 20% test) and use the training set to train the model and the test set to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ce677",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer with random forest classifier, no feature extraction\n",
    "We will start by using the TF-IDF vectorizer to convert the URLs into numerical features and then use a random forest classifier to classify the URLs. We will not use any feature extraction functions for now. This is for reference only, as we will use feature extraction functions and better methods later.\n",
    "* **Warning:** This will take a long time to run, as the dataset is huge, and TF-IDF vectorization is a slow process. Don't run this unless you have a powerful computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec8b3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we'll use TF-IDF on the URLs themselves. Advanced features can be added based on URL structure and content.\n",
    "#vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Prepare the data\n",
    "#X = balanced_df['URL']\n",
    "#y = balanced_df['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "# Create a pipeline that first transforms the data using TfidfVectorizer then applies RandomForestClassifier\n",
    "#model = make_pipeline(TfidfVectorizer(), RandomForestClassifier(n_estimators=20, random_state=42))\n",
    "\n",
    "# Train the model\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "#y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "#print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "#print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5a356",
   "metadata": {},
   "source": [
    "### Random forest classifier, with feature extraction\n",
    "Next thing we will do is use the feature extraction functions to extract features from the URLs and then use a random forest classifier to classify the URLs. We will use the same training and test sets as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06fa5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9091266578486398\n",
      "Confusion Matrix:\n",
      " [[26480  1693]\n",
      " [ 3391 24382]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91     28173\n",
      "           1       0.94      0.88      0.91     27773\n",
      "\n",
      "    accuracy                           0.91     55946\n",
      "   macro avg       0.91      0.91      0.91     55946\n",
      "weighted avg       0.91      0.91      0.91     55946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply feature extraction\n",
    "features = balanced_df['URL'].apply(lambda x: pd.Series({\n",
    "    'url_length': get_url_length(x),\n",
    "    'dot_count': get_dot_count(x),\n",
    "    'hyphen_count_domain': get_hyphen_count_in_domain(x),\n",
    "    'security_sensitive_words': contains_security_sensitive_words(x),\n",
    "    'directory_length': get_directory_length(x),\n",
    "    'sub_directory_count': get_sub_directory_count(x),\n",
    "    'token_count_path': get_token_count_in_path(x),\n",
    "    'largest_token_length': get_largest_token_length(x),\n",
    "    'average_token_length': get_average_token_length(x),\n",
    "    'file_length': get_file_length(x),\n",
    "    'contains_ip': contains_ip(x),\n",
    "    'dot_count_in_file': get_dot_count_in_file(x),\n",
    "    'delimiter_count_in_file': get_delimiter_count_in_file(x),\n",
    "    'arguments_length': get_arguments_length(x),\n",
    "    'number_of_arguments': get_number_of_arguments(x),\n",
    "    'length_of_largest_argument_value': get_length_of_largest_argument_value(x),\n",
    "    'max_delimiters_in_arguments': get_max_delimiters_in_arguments(x),\n",
    "    'special_character_count': get_special_character_count(x),\n",
    "    'entropy': get_entropy(x),\n",
    "    'url_shortened': check_url_shortened(x),\n",
    "    'port_number': get_port_number(x),\n",
    "    'subdomain_count': get_subdomain_count(x),\n",
    "    'suspicious_tld': get_suspicious_tld(x),\n",
    "    'numeric_ratio': get_numeric_ratio(x),\n",
    "    'url_is_internationalized': get_url_is_internationalized(x),\n",
    "    'domain_length': get_domain_features(x)[0],\n",
    "    'domain_token_count': get_domain_features(x)[1],\n",
    "    'largest_domain_token_length': get_domain_features(x)[2],\n",
    "    'average_domain_token_length': get_domain_features(x)[3],\n",
    "    'word_count': get_word_count(x)\n",
    "}))\n",
    "\n",
    "\n",
    "# Concatenate original DF with features\n",
    "balanced_df_with_features = pd.concat([balanced_df, features], axis=1)\n",
    "\n",
    "# Define X and y\n",
    "X = balanced_df_with_features.drop(['Label', 'URL'], axis=1)\n",
    "y = balanced_df_with_features['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "# Since your features are already numerical, directly use RandomForestClassifier without TfidfVectorizer\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bcad1",
   "metadata": {},
   "source": [
    "## TensorFlow neural network, with feature extraction\n",
    "Now let's use a neural network to classify the URLs. We will use the feature extraction functions to extract features from the URLs and then use a neural network to classify the URLs. We will use the same training and test sets as before. As the same, the training set is 80% and the test set is 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a08f25",
   "metadata": {},
   "source": [
    "### Preprocessing for Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6492a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and sequence padding parameters\n",
    "max_len = 100  # Adjust based on the length of the longest URL in your dataset\n",
    "max_words = 60000  # Adjust based on the size of your vocabulary\n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(balanced_df_with_features['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df_with_features['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df_with_features['Label'])\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7517f2",
   "metadata": {},
   "source": [
    "### Using RNN model with feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0db0b83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1399/1399 [==============================] - 74s 53ms/step - loss: 0.1934 - acc: 0.9223 - val_loss: 0.1527 - val_acc: 0.9372\n",
      "Epoch 2/10\n",
      "1399/1399 [==============================] - 78s 56ms/step - loss: 0.1471 - acc: 0.9402 - val_loss: 0.1438 - val_acc: 0.9426\n",
      "Epoch 3/10\n",
      "1399/1399 [==============================] - 78s 56ms/step - loss: 0.1273 - acc: 0.9482 - val_loss: 0.1367 - val_acc: 0.9449\n",
      "Epoch 4/10\n",
      "1399/1399 [==============================] - 78s 55ms/step - loss: 0.1217 - acc: 0.9517 - val_loss: 0.1357 - val_acc: 0.9457\n",
      "Epoch 5/10\n",
      "1399/1399 [==============================] - 80s 58ms/step - loss: 0.1262 - acc: 0.9498 - val_loss: 0.1299 - val_acc: 0.9469\n",
      "Epoch 6/10\n",
      "1399/1399 [==============================] - 78s 56ms/step - loss: 0.1094 - acc: 0.9567 - val_loss: 0.1290 - val_acc: 0.9481\n",
      "Epoch 7/10\n",
      "1399/1399 [==============================] - 78s 56ms/step - loss: 0.1080 - acc: 0.9573 - val_loss: 0.1268 - val_acc: 0.9498\n",
      "Epoch 8/10\n",
      "1399/1399 [==============================] - 82s 58ms/step - loss: 0.1034 - acc: 0.9589 - val_loss: 0.1313 - val_acc: 0.9478\n",
      "Epoch 9/10\n",
      "1399/1399 [==============================] - 81s 58ms/step - loss: 0.1003 - acc: 0.9605 - val_loss: 0.1276 - val_acc: 0.9490\n",
      "Epoch 10/10\n",
      "1399/1399 [==============================] - 81s 58ms/step - loss: 0.0990 - acc: 0.9612 - val_loss: 0.1251 - val_acc: 0.9503\n",
      "1749/1749 [==============================] - 38s 20ms/step\n",
      "Accuracy: 0.9524541522182104\n",
      "Confusion Matrix:\n",
      " [[27441   732]\n",
      " [ 1928 25845]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95     28173\n",
      "           1       0.97      0.93      0.95     27773\n",
      "\n",
      "    accuracy                           0.95     55946\n",
      "   macro avg       0.95      0.95      0.95     55946\n",
      "weighted avg       0.95      0.95      0.95     55946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RNN model definition\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_words, 32, input_length=max_len))\n",
    "model.add(tf.keras.layers.SimpleRNN(32))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping, using the CPU since the model is too simple for GPU acceleration\n",
    "with tf.device('/device:CPU:0'):\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded79e41",
   "metadata": {},
   "source": [
    "#### Optimize the TensorFlow neural network using RNN and Adam optimizer\n",
    "Now let's optimize the neural network using RNN and the Adam optimizer. We will use the feature extraction functions to extract features from the URLs and then use a neural network with RNN and the Adam optimizer to classify the URLs. We will use the same training and test sets as before. Since the model now is much more complex, we'll use TensorFlow GPU acceleration to speed up the training process, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5fdf8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1399/1399 [==============================] - 108s 72ms/step - loss: 0.2560 - acc: 0.9021 - val_loss: 0.2048 - val_acc: 0.9185\n",
      "Epoch 2/30\n",
      "1399/1399 [==============================] - 98s 70ms/step - loss: 0.1918 - acc: 0.9251 - val_loss: 0.1775 - val_acc: 0.9302\n",
      "Epoch 3/30\n",
      "1399/1399 [==============================] - 99s 71ms/step - loss: 0.1707 - acc: 0.9347 - val_loss: 0.1592 - val_acc: 0.9392\n",
      "Epoch 4/30\n",
      "1399/1399 [==============================] - 98s 70ms/step - loss: 0.1535 - acc: 0.9421 - val_loss: 0.1470 - val_acc: 0.9432\n",
      "Epoch 5/30\n",
      "1399/1399 [==============================] - 97s 69ms/step - loss: 0.1449 - acc: 0.9458 - val_loss: 0.1474 - val_acc: 0.9429\n",
      "Epoch 6/30\n",
      "1399/1399 [==============================] - 100s 72ms/step - loss: 0.1370 - acc: 0.9489 - val_loss: 0.1351 - val_acc: 0.9480\n",
      "Epoch 7/30\n",
      "1399/1399 [==============================] - 99s 71ms/step - loss: 0.1321 - acc: 0.9509 - val_loss: 0.1443 - val_acc: 0.9458\n",
      "Epoch 8/30\n",
      "1399/1399 [==============================] - 99s 71ms/step - loss: 0.1270 - acc: 0.9530 - val_loss: 0.1308 - val_acc: 0.9499\n",
      "Epoch 9/30\n",
      "1399/1399 [==============================] - 99s 71ms/step - loss: 0.1224 - acc: 0.9545 - val_loss: 0.1273 - val_acc: 0.9523\n",
      "Epoch 10/30\n",
      "1399/1399 [==============================] - 99s 71ms/step - loss: 0.1184 - acc: 0.9565 - val_loss: 0.1243 - val_acc: 0.9525\n",
      "Epoch 11/30\n",
      "1399/1399 [==============================] - 98s 70ms/step - loss: 0.1147 - acc: 0.9576 - val_loss: 0.1207 - val_acc: 0.9546\n",
      "Epoch 12/30\n",
      "1399/1399 [==============================] - 96s 69ms/step - loss: 0.1105 - acc: 0.9593 - val_loss: 0.1208 - val_acc: 0.9545\n",
      "Epoch 13/30\n",
      "1399/1399 [==============================] - 97s 69ms/step - loss: 0.1070 - acc: 0.9607 - val_loss: 0.1197 - val_acc: 0.9568\n",
      "Epoch 14/30\n",
      "1399/1399 [==============================] - 97s 70ms/step - loss: 0.1035 - acc: 0.9617 - val_loss: 0.1151 - val_acc: 0.9577\n",
      "Epoch 15/30\n",
      "1399/1399 [==============================] - 97s 69ms/step - loss: 0.1005 - acc: 0.9630 - val_loss: 0.1147 - val_acc: 0.9581\n",
      "Epoch 16/30\n",
      "1399/1399 [==============================] - 97s 69ms/step - loss: 0.0973 - acc: 0.9640 - val_loss: 0.1158 - val_acc: 0.9592\n",
      "Epoch 17/30\n",
      "1399/1399 [==============================] - 96s 68ms/step - loss: 0.0949 - acc: 0.9648 - val_loss: 0.1104 - val_acc: 0.9594\n",
      "Epoch 18/30\n",
      "1399/1399 [==============================] - 97s 69ms/step - loss: 0.0915 - acc: 0.9666 - val_loss: 0.1113 - val_acc: 0.9598\n",
      "Epoch 19/30\n",
      "1399/1399 [==============================] - 97s 69ms/step - loss: 0.0894 - acc: 0.9673 - val_loss: 0.1098 - val_acc: 0.9615\n",
      "Epoch 20/30\n",
      "1399/1399 [==============================] - 97s 69ms/step - loss: 0.0859 - acc: 0.9686 - val_loss: 0.1119 - val_acc: 0.9604\n",
      "Epoch 21/30\n",
      "1399/1399 [==============================] - 97s 70ms/step - loss: 0.0847 - acc: 0.9690 - val_loss: 0.1062 - val_acc: 0.9622\n",
      "Epoch 22/30\n",
      "1399/1399 [==============================] - 97s 70ms/step - loss: 0.0830 - acc: 0.9699 - val_loss: 0.1042 - val_acc: 0.9629\n",
      "Epoch 23/30\n",
      "1399/1399 [==============================] - 97s 69ms/step - loss: 0.0798 - acc: 0.9706 - val_loss: 0.1111 - val_acc: 0.9632\n",
      "Epoch 24/30\n",
      "1399/1399 [==============================] - 96s 69ms/step - loss: 0.0782 - acc: 0.9717 - val_loss: 0.1067 - val_acc: 0.9629\n",
      "Epoch 25/30\n",
      "1399/1399 [==============================] - 96s 69ms/step - loss: 0.0763 - acc: 0.9723 - val_loss: 0.1117 - val_acc: 0.9620\n",
      "Epoch 26/30\n",
      "1399/1399 [==============================] - 96s 69ms/step - loss: 0.0747 - acc: 0.9727 - val_loss: 0.1115 - val_acc: 0.9626\n",
      "1749/1749 [==============================] - 40s 22ms/step\n",
      "Accuracy: 0.9651985843491938\n",
      "Confusion Matrix:\n",
      " [[27699   474]\n",
      " [ 1473 26300]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97     28173\n",
      "           1       0.98      0.95      0.96     27773\n",
      "\n",
      "    accuracy                           0.97     55946\n",
      "   macro avg       0.97      0.97      0.97     55946\n",
      "weighted avg       0.97      0.97      0.97     55946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and sequence padding parameters\n",
    "max_len = 100  # Adjust based on the length of the longest URL in your dataset\n",
    "max_words = 60000  # Adjust based on the size of your vocabulary\n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words, char_level=True)\n",
    "tokenizer.fit_on_texts(balanced_df['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df['Label'])\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Model definition\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_words, 32, input_length=max_len))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))) # Add return_sequences if stacking LSTM layers\n",
    "model.add(tf.keras.layers.Dropout(0.5))  # Adjust dropout rate as needed\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)))  # Second LSTM layer, without return_sequences\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))  # Add L2 regularization\n",
    "model.add(tf.keras.layers.Dropout(0.5))  # Adjust dropout rate as needed\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "# Use GPU for training, as the model is more complex and very slow to train on CPU\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "825f16a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save(\"RNN_and_Adam_trained_model.h5\")\n",
    "\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aca729",
   "metadata": {},
   "source": [
    "#### Another optimization for the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1f07cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/30\n",
      "1399/1399 [==============================] - 2132s 2s/step - loss: 0.2637 - acc: 0.8986 - val_loss: 0.2107 - val_acc: 0.9197\n",
      "Epoch 2/30\n",
      "1399/1399 [==============================] - 2129s 2s/step - loss: 0.2096 - acc: 0.9228 - val_loss: 0.1729 - val_acc: 0.9322\n",
      "Epoch 3/30\n",
      "1399/1399 [==============================] - 2127s 2s/step - loss: 0.1939 - acc: 0.9294 - val_loss: 0.1642 - val_acc: 0.9355\n",
      "Epoch 4/30\n",
      "1399/1399 [==============================] - 2138s 2s/step - loss: 0.1819 - acc: 0.9340 - val_loss: 0.1583 - val_acc: 0.9386\n",
      "Epoch 5/30\n",
      "1399/1399 [==============================] - 2127s 2s/step - loss: 0.1737 - acc: 0.9378 - val_loss: 0.1482 - val_acc: 0.9429\n",
      "Epoch 6/30\n",
      "1399/1399 [==============================] - 2127s 2s/step - loss: 0.1688 - acc: 0.9396 - val_loss: 0.1495 - val_acc: 0.9443\n",
      "Epoch 7/30\n",
      "1399/1399 [==============================] - 2132s 2s/step - loss: 0.1639 - acc: 0.9416 - val_loss: 0.1516 - val_acc: 0.9452\n",
      "Epoch 8/30\n",
      "1399/1399 [==============================] - 2126s 2s/step - loss: 0.1603 - acc: 0.9425 - val_loss: 0.1418 - val_acc: 0.9463\n",
      "Epoch 9/30\n",
      "1399/1399 [==============================] - 2127s 2s/step - loss: 0.1565 - acc: 0.9445 - val_loss: 0.1349 - val_acc: 0.9476\n",
      "Epoch 10/30\n",
      " 341/1399 [======>.......................] - ETA: 26:20 - loss: 0.1567 - acc: 0.9435"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 46\u001b[0m\n\u001b[0;32m     42\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Use GPU for training, as the model is more complex and very slow to train on CPU\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[0;32m     49\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tokenization and sequence padding parameters\n",
    "max_len = 100  # Adjust based on the length of the longest URL in your dataset\n",
    "max_words = 60000  # Adjust based on the size of your vocabulary\n",
    "\n",
    "# Tokenize the URLs\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words, char_level=True)\n",
    "tokenizer.fit_on_texts(balanced_df['URL'])\n",
    "sequences = tokenizer.texts_to_sequences(balanced_df['URL'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "labels = np.asarray(balanced_df['Label'])\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# RNN model definition\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_words, 32, input_length=max_len)) # Embedding layer\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2))) # Bidirectional LSTM layer\n",
    "\n",
    "# Multi-layer dense network\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "# Dropout layer\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# Use Adam optimizer with a lower learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "# Use GPU for training, as the model is more complex and very slow to train on CPU\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred_classes))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f82c37b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
