{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f28742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import lit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import resample\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"Detecting-Malicious-URL App\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/home/matan/Documents/url classification/url_dataset_updated.csv')\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "url_column_name = 'URL'\n",
    "\n",
    "# Function to concatenate \"https://\" to URLs labeled with 0\n",
    "def add_https(url, label):\n",
    "    if label == 0:\n",
    "        return \"https://\" + url\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "# Apply the function to the URL column\n",
    "df[url_column_name] = df.apply(lambda row: add_https(row[url_column_name], row['label']), axis=1)\n",
    "\n",
    "# Separate the dataset into malicious and benign\n",
    "malicious_df = df[df['label'] == 1]\n",
    "benign_df = df[df['label'] == 0]\n",
    "\n",
    "# Randomly sample 150,000 entries from each\n",
    "malicious_sampled_df = resample(malicious_df, n_samples=150000, random_state=42)\n",
    "benign_sampled_df = resample(benign_df, n_samples=150000, random_state=42)\n",
    "\n",
    "# Combine the sampled data\n",
    "balanced_df = pd.concat([malicious_sampled_df, benign_sampled_df])\n",
    "\n",
    "# Shuffle the combined dataset to mix malicious and benign URLs\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "spark_df = spark.createDataFrame(balanced_df)\n",
    "\n",
    "# Tokenize the URL column\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"URL\", outputCol=\"Words\", pattern=\"\\\\W\")\n",
    "\n",
    "# CountVectorizer converts the words into feature vectors\n",
    "countVectors = CountVectorizer(inputCol=regexTokenizer.getOutputCol(), outputCol=\"rawfeatures\", vocabSize=10000, minDF=5)\n",
    "\n",
    "# TF-IDF\n",
    "idf = IDF(inputCol=countVectors.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, countVectors, idf])\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipelineFit = pipeline.fit(spark_df)\n",
    "dataset = pipelineFit.transform(spark_df)\n",
    "\n",
    "# Randomly split the dataset into training and testing (80%, 20%)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed=100)\n",
    "\n",
    "# Convert the Spark DataFrames to Pandas for scikit-learn\n",
    "train_data_pd = trainingData.select(\"features\", \"label\").toPandas()\n",
    "test_data_pd = testData.select(\"features\", \"label\").toPandas()\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = np.vstack(train_data_pd[\"features\"].apply(lambda x: x.toArray()).to_numpy())\n",
    "y_train = train_data_pd[\"label\"].to_numpy()\n",
    "\n",
    "X_test = np.vstack(test_data_pd[\"features\"].apply(lambda x: x.toArray()).to_numpy())\n",
    "y_test = test_data_pd[\"label\"].to_numpy()\n",
    "\n",
    "# Train k-NN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = knn.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "classification_rep = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499f0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
