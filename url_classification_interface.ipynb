{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bda55da",
   "metadata": {},
   "source": [
    "# URL Classification model using Machine Learning\n",
    "### For Computer Science B.Sc. Ariel University\n",
    "\n",
    "**By Yuval Yurzdichansky, Matan Aviv and Roy Simanovich**\n",
    "\n",
    "## Introduction\n",
    "In this project we will build a machine learning model that will classify URLs based on if they are malicious (phishing, malware, etc.) or benign. We will use a dataset of URLs that are labeled as either malicious or benign and use it to train a model that will be able to classify new URLs.\n",
    "\n",
    "## Description\n",
    "This is the interface of the model. You can choose one of the following models for classification:\n",
    "1. Random Forest with 100 estimators - sklearn\n",
    "2. Decision Tree - sklearn\n",
    "3. K-Nearest Neighbors (KNN) - sklearn\n",
    "4. Logistic Regression with 10,000 iterations - sklearn\n",
    "5. Gaussian Naive Bayes - sklearn\n",
    "6. Recurrent Neural Network (RNN) - tensorflow and keras\n",
    "\n",
    "After choosing a model, you can input a URL and the model will classify it as either malicious or benign."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d952d7c5",
   "metadata": {},
   "source": [
    "### Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some importent libraries installtion via pip\n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e63ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries for the project #\n",
    "\n",
    "## Regular Libraries that's come with python\n",
    "### Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "### Numerical Computing\n",
    "import numpy as np\n",
    "### Collection of Data Structures\n",
    "import collections as col\n",
    "### Regular Expressions\n",
    "import re as regex\n",
    "### URL Handling\n",
    "import urllib as urlhndl\n",
    "### Mathematical Operations\n",
    "import math\n",
    "### Socket Programming\n",
    "import socket\n",
    "\n",
    "## SKLearn Libraries for Machine Learning\n",
    "### Splitting the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "### Metrics for Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "### Preprocessing the Data\n",
    "from sklearn.utils import resample\n",
    "### TF-IDF Vectorizer for Text Data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "### Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "### Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "### Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "### K-Nearest Neighbors Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## Tensorflow Libraries for Deep Learning\n",
    "import tensorflow as tf\n",
    "\n",
    "## Pickle for saving the model to disk\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3ae76",
   "metadata": {},
   "source": [
    "### Feature extraction functions\n",
    "We will use the following feature extraction functions to extract features from the URLs:\n",
    "1. `get_url_length`: This function returns the length of the URL.\n",
    "2. `get_dot_count`: This function returns the number of dots in the URL.\n",
    "3. `get_hyphen_count_in_domain`: This function returns the number of hyphens in the domain of the URL.\n",
    "4. `contains_security_sensitive_words`: This function returns 1 if the URL contains security sensitive words (e.g., \"login\", \"signin\", \"account\", etc.) and 0 otherwise.\n",
    "5. `get_directory_length`: This function returns the length of the directory part of the URL.\n",
    "6. `get_sub_directory_count`: This function returns the number of subdirectories in the URL.\n",
    "7. `contains_ip`: This function returns 1 if the URL contains an IP address and 0 otherwise.\n",
    "8. `get_token_count_in_path`: This function returns the number of tokens in the path part of the URL.\n",
    "9. `get_largest_token_length`: This function returns the length of the largest token in the path part of the URL.\n",
    "10. `get_avarage_token_length`: This function returns the average length of the tokens in the path part of the URL.\n",
    "11. `get_file_length`: This function returns the length of the file part of the URL.\n",
    "12. `get_dot_count_in_file`: This function returns the number of dots in the file part of the URL.\n",
    "13. `get_delimiter_count_in_file`: This function returns the number of delimiters in the file part of the URL.\n",
    "14. `get_arguments_length`: This function returns the length of the arguments part of the URL.\n",
    "15. `get_number_of_arguments`: This function returns the number of arguments in the URL.\n",
    "16. `get_length_of_largest_argument_value`: This function returns the length of the largest argument value in the URL.\n",
    "17. `get_max_delimiters_in_arguments`: This function returns the maximum number of delimiters in the arguments part of the URL.\n",
    "18. `get_domain_features`: This function returns the following features of the domain part of the URL: length, number of dots, number of hyphens, number of digits, number of special characters, entropy, whether the domain is a subdomain, and whether the domain is an IP address.\n",
    "19. `get_special_character_count`: This function returns the number of special characters in the URL.\n",
    "20. `get_entropy`: This function returns the entropy of the URL.\n",
    "21. `check_url_shortened`: This function returns 1 if the URL is shortened and 0 otherwise.\n",
    "22. `get_port_number`: This function returns the port number of the URL, if it exists, and -1 otherwise.\n",
    "23. `get_subdomain_count`: This function returns the number of subdomains in the URL.\n",
    "24. `get_suspicious_tld`: This function returns 1 if the top-level domain (TLD) of the URL is suspicious (e.g., \"tk\", \"ml\", \"ga\", etc.) and 0 otherwise.\n",
    "25. `get_numeric_ratio`: This function returns the ratio of numeric characters in the URL.\n",
    "26. `get_word_count`: This function returns the number of words in the URL.\n",
    "27. `get_url_is_internationalized`: This function returns 1 if the URL is internationalized and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd12ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to concatenate \"https://\" to benign URLs \n",
    "def add_https(url, label):\n",
    "    if label == 0:\n",
    "        return \"https://\" + url\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "def ensure_scheme(url):\n",
    "    if not urlhndl.parse.urlparse(url).scheme:\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "# Feature extraction functions\n",
    "def get_url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "def get_dot_count(url):\n",
    "    return url.count('.')\n",
    "\n",
    "def contains_security_sensitive_words(url):\n",
    "    security_sensitive_words = [\n",
    "    'login', 'password', 'admin', 'root', 'secret', 'private', 'secure', 'confidential', \n",
    "    'bank', 'creditcard', 'account', 'authentication', 'authorization', 'session', 'token', \n",
    "    'apikey', 'ssl', 'https', 'secure', 'encrypted', 'auth', 'signin', 'signup', 'verification', \n",
    "    'resetpassword', 'change-password', 'forgot-password', 'otp', '2fa', 'phishing', 'malware', \n",
    "    'virus', 'trojan', 'exploit', 'hacker', 'attack', 'security', 'vulnerable', 'injection', \n",
    "    'xss', 'csrf', 'dos', 'ddos', 'bruteforce', 'firewall', 'vpn', 'proxy', 'tor', 'security-question', \n",
    "    'privacy-policy']\n",
    "    return int(any(word in url for word in security_sensitive_words))\n",
    "\n",
    "def get_directory_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    return len(path)\n",
    "\n",
    "def get_sub_directory_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    return path.count('/') - 1\n",
    "\n",
    "def get_token_count_in_path(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    return len(tokens) - 1\n",
    "\n",
    "def get_largest_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    if tokens:\n",
    "        return max(len(token) for token in tokens)\n",
    "    return 0\n",
    "\n",
    "def get_average_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    tokens = [token for token in path.split('/') if token]\n",
    "    if tokens:\n",
    "        return np.mean([len(token) for token in tokens])\n",
    "    return 0\n",
    "\n",
    "def get_file_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return len(filename)\n",
    "\n",
    "def get_dot_count_in_file(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return filename.count('.')\n",
    "\n",
    "def get_delimiter_count_in_file(url):\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    delimiters = ['.', '_', '-']\n",
    "    return sum(filename.count(delimiter) for delimiter in delimiters)\n",
    "\n",
    "def get_arguments_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    return len(query)\n",
    "\n",
    "def get_number_of_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    return len(urlhndl.parse.parse_qs(query))\n",
    "\n",
    "def get_length_of_largest_argument_value(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    params = urlhndl.parse.parse_qs(query)\n",
    "    if params:\n",
    "        return max(len(max(values, key=len)) for values in params.values())\n",
    "    return 0\n",
    "\n",
    "def get_max_delimiters_in_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    params = urlhndl.parse.parse_qs(query)\n",
    "    delimiters = ['&', '=', '-', '_']\n",
    "    if params:\n",
    "        return max(sum(value.count(delimiter) for delimiter in delimiters) for values in params.values() for value in values)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_hyphen_count_in_domain(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(url).netloc\n",
    "    return domain.count('-')\n",
    "\n",
    "def contains_ip(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(url).netloc\n",
    "    try:\n",
    "        socket.inet_aton(domain)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def get_domain_features(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(ensure_scheme(url)).netloc\n",
    "    tokens = domain.split('.')\n",
    "    \n",
    "    # Domain Length\n",
    "    domain_length = len(domain)\n",
    "    \n",
    "    # Count of Tokens in the Domain\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Length of Largest Token in the Domain\n",
    "    largest_token_length = max(len(token) for token in tokens) if tokens else 0\n",
    "    \n",
    "    # Average Domain Token Length\n",
    "    average_token_length = sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n",
    "    \n",
    "    return domain_length, token_count, largest_token_length, average_token_length\n",
    "\n",
    "# New feature extraction functions\n",
    "def get_special_character_count(url):\n",
    "    special_characters = ['@', '=', '+', '*', '?', '&', '%', '$', '#', '!']\n",
    "    return sum(url.count(char) for char in special_characters)\n",
    "\n",
    "def get_entropy(url):\n",
    "    # Count the frequency of each character in the string\n",
    "    freq = col.Counter(url)\n",
    "    # Calculate the probabilities\n",
    "    probs = [count / len(url) for count in freq.values()]\n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -sum(p * math.log(p, 2) for p in probs if p > 0)\n",
    "    return entropy\n",
    "\n",
    "def check_url_shortened(url):\n",
    "    shortened_services = ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co']\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(url).netloc\n",
    "    return int(domain in shortened_services)\n",
    "\n",
    "def get_subdomain_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain_parts = urlhndl.parse.urlparse(url).netloc.split('.')\n",
    "    # Count as subdomains any parts beyond the second-level domain and TLD\n",
    "    return max(0, len(domain_parts) - 2)\n",
    "\n",
    "def get_suspicious_tld(url):\n",
    "    suspicious_tlds = ['xyz', 'top', 'loan', 'win', 'club']\n",
    "    url = ensure_scheme(url)\n",
    "    tld = urlhndl.parse.urlparse(url).netloc.split('.')[-1]\n",
    "    return int(tld in suspicious_tlds)\n",
    "\n",
    "def get_numeric_ratio(url):\n",
    "    numeric_chars = sum(c.isdigit() for c in url)\n",
    "    return numeric_chars / len(url) if len(url) > 0 else 0\n",
    "\n",
    "def get_word_count(url):\n",
    "    words = regex.findall(r'\\w+', url)\n",
    "    return len(words)\n",
    "\n",
    "def is_https(url, timeout=0.5):\n",
    "    return int(url.startswith(\"https\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6f2221",
   "metadata": {},
   "source": [
    "### Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "choosen_model = input(\"Choose a model for classification:\\n1. Random Forest with 100 estimators - sklearn\\n2. Decision Tree - sklearn\\n3. K-Nearest Neighbors (KNN) - sklearn\\n4. Logistic Regression with 10,000 iterations - sklearn\\n5. Gaussian Naive Bayes - sklearn\\n6. Recurrent Neural Network (RNN) - tensorflow and keras\\n\")\n",
    "\n",
    "url = input(\"Enter a URL: \")\n",
    "\n",
    "# Load the chosen model from disk\n",
    "if choosen_model == \"1\":\n",
    "    model = pkl.load(open('random_forest_model.pkl', 'rb'))\n",
    "\n",
    "elif choosen_model == \"2\":\n",
    "    model = pkl.load(open('decision_tree_model.pkl', 'rb'))\n",
    "\n",
    "elif choosen_model == \"3\":\n",
    "    model = pkl.load(open('knn_model.pkl', 'rb'))\n",
    "\n",
    "elif choosen_model == \"4\":\n",
    "    model = pkl.load(open('logistic_regression_model.pkl', 'rb'))\n",
    "\n",
    "elif choosen_model == \"5\":\n",
    "    model = pkl.load(open('gaussian_nb_model.pkl', 'rb'))\n",
    "\n",
    "elif choosen_model == \"6\":\n",
    "    model = tf.keras.models.load_model('rnn_model.h5')\n",
    "\n",
    "else:\n",
    "    print(\"Invalid model choice.\")\n",
    "    exit()\n",
    "\n",
    "tf_idf_vectorizer = pkl.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "\n",
    "# Preprocess the URL\n",
    "url = ensure_scheme(url)\n",
    "\n",
    "data = {'URL': [url]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "features = df['URL'].apply(lambda x: pd.Series({\n",
    "    'url_length': get_url_length(x),\n",
    "    'dot_count': get_dot_count(x),\n",
    "    'hyphen_count_domain': get_hyphen_count_in_domain(x),\n",
    "    'security_sensitive_words': contains_security_sensitive_words(x),\n",
    "    'directory_length': get_directory_length(x),\n",
    "    'sub_directory_count': get_sub_directory_count(x),\n",
    "    'token_count_path': get_token_count_in_path(x),\n",
    "    'largest_token_length': get_largest_token_length(x),\n",
    "    'average_token_length': get_average_token_length(x),\n",
    "    'file_length': get_file_length(x),\n",
    "    'contains_ip': contains_ip(x),\n",
    "    'dot_count_in_file': get_dot_count_in_file(x),\n",
    "    'delimiter_count_in_file': get_delimiter_count_in_file(x),\n",
    "    'arguments_length': get_arguments_length(x),\n",
    "    'number_of_arguments': get_number_of_arguments(x),\n",
    "    'length_of_largest_argument_value': get_length_of_largest_argument_value(x),\n",
    "    'max_delimiters_in_arguments': get_max_delimiters_in_arguments(x),\n",
    "    'special_character_count': get_special_character_count(x),\n",
    "    'entropy': get_entropy(x),\n",
    "    'url_shortened': check_url_shortened(x),\n",
    "    'subdomain_count': get_subdomain_count(x),\n",
    "    'suspicious_tld': get_suspicious_tld(x),\n",
    "    'numeric_ratio': get_numeric_ratio(x),\n",
    "    'domain_length': get_domain_features(x)[0],\n",
    "    'domain_token_count': get_domain_features(x)[1],\n",
    "    'largest_domain_token_length': get_domain_features(x)[2],\n",
    "    'average_domain_token_length': get_domain_features(x)[3],\n",
    "    'word_count': get_word_count(x),\n",
    "    'is_https': is_https(x)\n",
    "}))\n",
    "\n",
    "# Concatenate original DF with features\n",
    "balanced_df = pd.concat([df, features], axis=1)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "data = tf_idf_vectorizer.transform(balanced_df['URL'])\n",
    "\n",
    "# Convert the sparse matrix to a dataframe\n",
    "data = pd.DataFrame(data.toarray(), columns=tf_idf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Add the features to the dataframe\n",
    "data = pd.concat([balanced_df, data], axis=1)\n",
    "\n",
    "# Drop the URL column\n",
    "data = data.drop('URL', axis=1)\n",
    "\n",
    "# Make predictions\n",
    "if choosen_model == \"6\":\n",
    "    # Reshape the data to fit the input shape of the RNN\n",
    "    data = np.reshape(data.to_numpy(), (1, 1, 30))\n",
    "    # Make predictions\n",
    "    prediction = model.predict(data)\n",
    "    # Convert the prediction to a label\n",
    "    prediction = 1 if prediction > 0.7 else 0\n",
    "\n",
    "else:\n",
    "    # Make predictions\n",
    "    prediction = model.predict(data)\n",
    "    # Convert the prediction to a label\n",
    "    prediction = 1 if prediction[0] > 0.7 else 0\n",
    "\n",
    "# Print the prediction\n",
    "if prediction == 0:\n",
    "    print(\"The model predicts that the URL is benign.\")\n",
    "\n",
    "else:\n",
    "    print(\"The model predicts that the URL is malicious.\")\n",
    "\n",
    "# Print the probability of the prediction\n",
    "print(\"The probability of the URL being malicious is\", round(model.predict_proba(data)[0][1], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24813101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
