{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bda55da",
   "metadata": {},
   "source": [
    "# URL Classification model using Machine Learning\n",
    "### For Computer Science B.Sc. Ariel University\n",
    "\n",
    "**By Yuval Yurzdichansky, Matan Aviv and Roy Simanovich**\n",
    "\n",
    "## Introduction\n",
    "In this project we will build a machine learning model that will classify URLs based on if they are malicious (phishing, malware, etc.) or benign. We will use a dataset of URLs that are labeled as either malicious or benign and use it to train a model that will be able to classify new URLs.\n",
    "\n",
    "## Description\n",
    "This is the interface of the model. You can choose one of the following models for classification:\n",
    "1. Random Forest with 100 estimators - sklearn\n",
    "2. Decision Tree - sklearn\n",
    "3. K-Nearest Neighbors (KNN) - sklearn\n",
    "4. Logistic Regression with 10,000 iterations - sklearn\n",
    "5. Gaussian Naive Bayes - sklearn\n",
    "6. Recurrent Neural Network (RNN) - tensorflow and keras\n",
    "\n",
    "After choosing a model, you can input a URL and the model will classify it as either malicious or benign."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d952d7c5",
   "metadata": {},
   "source": [
    "### Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5683ec8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (2.10.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (1.60.1)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.28.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\siman\\.conda\\envs\\tf\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Some importent libraries installtion via pip\n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31e63ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries for the project #\n",
    "\n",
    "## Regular Libraries that's come with python\n",
    "### Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "### Numerical Computing\n",
    "import numpy as np\n",
    "### Collection of Data Structures\n",
    "import collections as col\n",
    "### Regular Expressions\n",
    "import re as regex\n",
    "### URL Handling\n",
    "import urllib as urlhndl\n",
    "### Mathematical Operations\n",
    "import math\n",
    "### Socket Programming\n",
    "import socket\n",
    "\n",
    "## SKLearn Libraries for Machine Learning\n",
    "### Splitting the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "### Metrics for Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "### Preprocessing the Data\n",
    "from sklearn.utils import resample\n",
    "### TF-IDF Vectorizer for Text Data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "### Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "### Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "### Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "### K-Nearest Neighbors Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## Tensorflow Libraries for Deep Learning\n",
    "import tensorflow as tf\n",
    "\n",
    "## Pickle for saving the model to disk\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3ae76",
   "metadata": {},
   "source": [
    "### Feature extraction functions\n",
    "We will use the following feature extraction functions to extract features from the URLs:\n",
    "1. `get_url_length`: This function returns the length of the URL.\n",
    "2. `get_dot_count`: This function returns the number of dots in the URL.\n",
    "3. `get_hyphen_count_in_domain`: This function returns the number of hyphens in the domain of the URL.\n",
    "4. `contains_security_sensitive_words`: This function returns 1 if the URL contains security sensitive words (e.g., \"login\", \"signin\", \"account\", etc.) and 0 otherwise.\n",
    "5. `get_directory_length`: This function returns the length of the directory part of the URL.\n",
    "6. `get_sub_directory_count`: This function returns the number of subdirectories in the URL.\n",
    "7. `contains_ip`: This function returns 1 if the URL contains an IP address and 0 otherwise.\n",
    "8. `get_token_count_in_path`: This function returns the number of tokens in the path part of the URL.\n",
    "9. `get_largest_token_length`: This function returns the length of the largest token in the path part of the URL.\n",
    "10. `get_avarage_token_length`: This function returns the average length of the tokens in the path part of the URL.\n",
    "11. `get_file_length`: This function returns the length of the file part of the URL.\n",
    "12. `get_dot_count_in_file`: This function returns the number of dots in the file part of the URL.\n",
    "13. `get_delimiter_count_in_file`: This function returns the number of delimiters in the file part of the URL.\n",
    "14. `get_arguments_length`: This function returns the length of the arguments part of the URL.\n",
    "15. `get_number_of_arguments`: This function returns the number of arguments in the URL.\n",
    "16. `get_length_of_largest_argument_value`: This function returns the length of the largest argument value in the URL.\n",
    "17. `get_max_delimiters_in_arguments`: This function returns the maximum number of delimiters in the arguments part of the URL.\n",
    "18. `get_domain_features`: This function returns the following features of the domain part of the URL: length, number of dots, number of hyphens, number of digits, number of special characters, entropy, whether the domain is a subdomain, and whether the domain is an IP address.\n",
    "19. `get_special_character_count`: This function returns the number of special characters in the URL.\n",
    "20. `get_entropy`: This function returns the entropy of the URL.\n",
    "21. `check_url_shortened`: This function returns 1 if the URL is shortened and 0 otherwise.\n",
    "22. `get_port_number`: This function returns the port number of the URL, if it exists, and -1 otherwise.\n",
    "23. `get_subdomain_count`: This function returns the number of subdomains in the URL.\n",
    "24. `get_suspicious_tld`: This function returns 1 if the top-level domain (TLD) of the URL is suspicious (e.g., \"tk\", \"ml\", \"ga\", etc.) and 0 otherwise.\n",
    "25. `get_numeric_ratio`: This function returns the ratio of numeric characters in the URL.\n",
    "26. `get_word_count`: This function returns the number of words in the URL.\n",
    "27. `get_url_is_internationalized`: This function returns 1 if the URL is internationalized and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd12ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to concatenate \"https://\" to benign URLs \n",
    "def add_https(url, label):\n",
    "    if label == 0:\n",
    "        return \"https://\" + url\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "def ensure_scheme(url):\n",
    "    if not urlhndl.parse.urlparse(url).scheme:\n",
    "        url = 'https://' + url\n",
    "    return url\n",
    "\n",
    "# Feature extraction functions\n",
    "def get_url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "def get_dot_count(url):\n",
    "    return url.count('.')\n",
    "\n",
    "def contains_security_sensitive_words(url):\n",
    "    security_sensitive_words = [\n",
    "    'login', 'password', 'admin', 'root', 'secret', 'private', 'secure', 'confidential', \n",
    "    'bank', 'creditcard', 'account', 'authentication', 'authorization', 'session', 'token', \n",
    "    'apikey', 'ssl', 'https', 'secure', 'encrypted', 'auth', 'signin', 'signup', 'verification', \n",
    "    'resetpassword', 'change-password', 'forgot-password', 'otp', '2fa', 'phishing', 'malware', \n",
    "    'virus', 'trojan', 'exploit', 'hacker', 'attack', 'security', 'vulnerable', 'injection', \n",
    "    'xss', 'csrf', 'dos', 'ddos', 'bruteforce', 'firewall', 'vpn', 'proxy', 'tor', 'security-question', \n",
    "    'privacy-policy']\n",
    "    return int(any(word in url for word in security_sensitive_words))\n",
    "\n",
    "def get_directory_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    return len(path)\n",
    "\n",
    "def get_sub_directory_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    return path.count('/') - 1\n",
    "\n",
    "def get_token_count_in_path(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    return len(tokens) - 1\n",
    "\n",
    "def get_largest_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    tokens = path.split('/')\n",
    "    if tokens:\n",
    "        return max(len(token) for token in tokens)\n",
    "    return 0\n",
    "\n",
    "def get_average_token_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    tokens = [token for token in path.split('/') if token]\n",
    "    if tokens:\n",
    "        return np.mean([len(token) for token in tokens])\n",
    "    return 0\n",
    "\n",
    "def get_file_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return len(filename)\n",
    "\n",
    "def get_dot_count_in_file(url):\n",
    "    url = ensure_scheme(url)\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    return filename.count('.')\n",
    "\n",
    "def get_delimiter_count_in_file(url):\n",
    "    path = urlhndl.parse.urlparse(url).path\n",
    "    filename = path.split('/')[-1]\n",
    "    delimiters = ['.', '_', '-']\n",
    "    return sum(filename.count(delimiter) for delimiter in delimiters)\n",
    "\n",
    "def get_arguments_length(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    return len(query)\n",
    "\n",
    "def get_number_of_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    return len(urlhndl.parse.parse_qs(query))\n",
    "\n",
    "def get_length_of_largest_argument_value(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    params = urlhndl.parse.parse_qs(query)\n",
    "    if params:\n",
    "        return max(len(max(values, key=len)) for values in params.values())\n",
    "    return 0\n",
    "\n",
    "def get_max_delimiters_in_arguments(url):\n",
    "    url = ensure_scheme(url)\n",
    "    query = urlhndl.parse.urlparse(url).query\n",
    "    params = urlhndl.parse.parse_qs(query)\n",
    "    delimiters = ['&', '=', '-', '_']\n",
    "    if params:\n",
    "        return max(sum(value.count(delimiter) for delimiter in delimiters) for values in params.values() for value in values)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_hyphen_count_in_domain(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(url).netloc\n",
    "    return domain.count('-')\n",
    "\n",
    "def contains_ip(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(url).netloc\n",
    "    try:\n",
    "        socket.inet_aton(domain)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def get_domain_features(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(ensure_scheme(url)).netloc\n",
    "    tokens = domain.split('.')\n",
    "    \n",
    "    # Domain Length\n",
    "    domain_length = len(domain)\n",
    "    \n",
    "    # Count of Tokens in the Domain\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Length of Largest Token in the Domain\n",
    "    largest_token_length = max(len(token) for token in tokens) if tokens else 0\n",
    "    \n",
    "    # Average Domain Token Length\n",
    "    average_token_length = sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n",
    "    \n",
    "    return domain_length, token_count, largest_token_length, average_token_length\n",
    "\n",
    "# New feature extraction functions\n",
    "def get_special_character_count(url):\n",
    "    special_characters = ['@', '=', '+', '*', '?', '&', '%', '$', '#', '!']\n",
    "    return sum(url.count(char) for char in special_characters)\n",
    "\n",
    "def get_entropy(url):\n",
    "    # Count the frequency of each character in the string\n",
    "    freq = col.Counter(url)\n",
    "    # Calculate the probabilities\n",
    "    probs = [count / len(url) for count in freq.values()]\n",
    "    # Calculate the Shannon entropy\n",
    "    entropy = -sum(p * math.log(p, 2) for p in probs if p > 0)\n",
    "    return entropy\n",
    "\n",
    "def check_url_shortened(url):\n",
    "    shortened_services = ['bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co']\n",
    "    url = ensure_scheme(url)\n",
    "    domain = urlhndl.parse.urlparse(url).netloc\n",
    "    return int(domain in shortened_services)\n",
    "\n",
    "def get_subdomain_count(url):\n",
    "    url = ensure_scheme(url)\n",
    "    domain_parts = urlhndl.parse.urlparse(url).netloc.split('.')\n",
    "    # Count as subdomains any parts beyond the second-level domain and TLD\n",
    "    return max(0, len(domain_parts) - 2)\n",
    "\n",
    "def get_suspicious_tld(url):\n",
    "    suspicious_tlds = ['xyz', 'top', 'loan', 'win', 'club']\n",
    "    url = ensure_scheme(url)\n",
    "    tld = urlhndl.parse.urlparse(url).netloc.split('.')[-1]\n",
    "    return int(tld in suspicious_tlds)\n",
    "\n",
    "def get_numeric_ratio(url):\n",
    "    numeric_chars = sum(c.isdigit() for c in url)\n",
    "    return numeric_chars / len(url) if len(url) > 0 else 0\n",
    "\n",
    "def get_word_count(url):\n",
    "    words = regex.findall(r'\\w+', url)\n",
    "    return len(words)\n",
    "\n",
    "def is_https(url, timeout=0.5):\n",
    "    return int(url.startswith(\"https\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6f2221",
   "metadata": {},
   "source": [
    "### Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0b53c5",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "21e99b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models and vectorizers\n",
    "rf_model = pkl.load(open('random_forest_model.pkl', 'rb'))\n",
    "tf_idf_vectorizer = pkl.load(open('tfidf_vectorizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d8d73",
   "metadata": {},
   "source": [
    "#### Input a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6795774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request the URL from the user\n",
    "url = input(\"Enter a URL: \")\n",
    "\n",
    "# Preprocess the URL\n",
    "url = ensure_scheme(url)\n",
    "data = {'URL': [url]}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0bcdb",
   "metadata": {},
   "source": [
    "#### Prepare the URL for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d745106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features\n",
    "features = df['URL'].apply(lambda x: pd.Series({\n",
    "    'url_length': get_url_length(x),\n",
    "    'dot_count': get_dot_count(x),\n",
    "    'hyphen_count_domain': get_hyphen_count_in_domain(x),\n",
    "    'security_sensitive_words': contains_security_sensitive_words(x),\n",
    "    'directory_length': get_directory_length(x),\n",
    "    'sub_directory_count': get_sub_directory_count(x),\n",
    "    'token_count_path': get_token_count_in_path(x),\n",
    "    'largest_token_length': get_largest_token_length(x),\n",
    "    'average_token_length': get_average_token_length(x),\n",
    "    'file_length': get_file_length(x),\n",
    "    'contains_ip': contains_ip(x),\n",
    "    'dot_count_in_file': get_dot_count_in_file(x),\n",
    "    'delimiter_count_in_file': get_delimiter_count_in_file(x),\n",
    "    'arguments_length': get_arguments_length(x),\n",
    "    'number_of_arguments': get_number_of_arguments(x),\n",
    "    'length_of_largest_argument_value': get_length_of_largest_argument_value(x),\n",
    "    'max_delimiters_in_arguments': get_max_delimiters_in_arguments(x),\n",
    "    'special_character_count': get_special_character_count(x),\n",
    "    'entropy': get_entropy(x),\n",
    "    'url_shortened': check_url_shortened(x),\n",
    "    'subdomain_count': get_subdomain_count(x),\n",
    "    'suspicious_tld': get_suspicious_tld(x),\n",
    "    'numeric_ratio': get_numeric_ratio(x),\n",
    "    'domain_length': get_domain_features(x)[0],\n",
    "    'domain_token_count': get_domain_features(x)[1],\n",
    "    'largest_domain_token_length': get_domain_features(x)[2],\n",
    "    'average_domain_token_length': get_domain_features(x)[3],\n",
    "    'word_count': get_word_count(x),\n",
    "    'is_https': is_https(x)\n",
    "}))\n",
    "\n",
    "# Concatenate original DF with features\n",
    "df = pd.concat([df, features], axis=1)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "data = tf_idf_vectorizer.transform(df['URL'])\n",
    "\n",
    "# Convert the sparse matrix to a dataframe\n",
    "data = pd.DataFrame(data.toarray(), columns=tf_idf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Add the features to the dataframe\n",
    "data = pd.concat([df, data], axis=1)\n",
    "\n",
    "# Drop the URL column\n",
    "data = data.drop('URL', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a75c4a2",
   "metadata": {},
   "source": [
    "#### Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24813101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts that the URL is malicious.\n",
      "The probability of the URL being malicious is 0.87\n",
      "https://xn--procder-eya.com/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siman\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\siman\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\siman\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "prediction = rf_model.predict(data)\n",
    "\n",
    "# Use threshold to determine if the URL is malicious\n",
    "if round(rf_model.predict_proba(data)[0][1], 4) > 0.8:\n",
    "    prediction = 1\n",
    "else:\n",
    "    prediction = 0\n",
    "\n",
    "# Print the prediction\n",
    "if prediction == 0:\n",
    "    print(\"The model predicts that the URL is benign.\")\n",
    "\n",
    "else:\n",
    "    print(\"The model predicts that the URL is malicious.\")\n",
    "\n",
    "# Print the probability of the prediction\n",
    "print(\"The probability of the URL being malicious is\", round(rf_model.predict_proba(data)[0][1], 4))\n",
    "\n",
    "# Print the URL\n",
    "print(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
